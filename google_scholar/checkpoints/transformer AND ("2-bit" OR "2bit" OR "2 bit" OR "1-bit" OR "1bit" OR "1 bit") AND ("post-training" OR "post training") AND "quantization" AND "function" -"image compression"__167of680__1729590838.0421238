[141, [{"title": "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-Abstract-Conference.html", "id": "M1H30TzgocoJ", "cited_by_count": 289}, {"title": "I-vit: Integer-only quantization for efficient vision transformer inference", "title_link": "http://openaccess.thecvf.com/content/ICCV2023/html/Li_I-ViT_Integer-only_Quantization_for_Efficient_Vision_Transformer_Inference_ICCV_2023_paper.html", "id": "wA-a8sZKDhMJ", "cited_by_count": 73}, {"title": "Deep compression of pre-trained transformer models", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/5b5618e7d061748267d74478b7c5b1ab-Abstract-Conference.html", "id": "dxVXcqWwqfUJ", "cited_by_count": 19}, {"title": "Q-vit: Accurate and fully quantized low-bit vision transformer", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/deb921bff461a7b0a5c344a4871e7101-Abstract-Conference.html", "id": "pYjd1jUZ5TYJ", "cited_by_count": 73}, {"title": "Mr. biq: Post-training non-uniform quantization based on minimizing the reconstruction error", "title_link": "http://openaccess.thecvf.com/content/CVPR2022/html/Jeon_Mr.BiQ_Post-Training_Non-Uniform_Quantization_Based_on_Minimizing_the_Reconstruction_Error_CVPR_2022_paper.html", "id": "LGScuzy99a8J", "cited_by_count": 40}, {"title": "Practical edge kernels for integer-only vision transformers under post-training quantization", "title_link": "https://proceedings.mlsys.org/paper_files/paper/2023/hash/12f429641e5c63a9ca7fd1c5c4804d32-Abstract-mlsys2023.html", "id": "G0Gs0uSEz74J", "cited_by_count": 4}, {"title": "Bit-shrinking: Limiting instantaneous sharpness for improving post-training quantization", "title_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Lin_Bit-Shrinking_Limiting_Instantaneous_Sharpness_for_Improving_Post-Training_Quantization_CVPR_2023_paper.html", "id": "hZRbsLkw7BcJ", "cited_by_count": 12}, {"title": "Tsptq-vit: Two-scaled post-training quantization for vision transformer", "title_link": "https://ieeexplore.ieee.org/abstract/document/10096817/", "id": "lLtKpfBdvKkJ", "cited_by_count": 3}, {"title": "Q-detr: An efficient low-bit quantized detection transformer", "title_link": "https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Q-DETR_An_Efficient_Low-Bit_Quantized_Detection_Transformer_CVPR_2023_paper.html?ref=blog.roboflow.com", "id": "73Yoqdgfi6oJ", "cited_by_count": 17}, {"title": "Oscillation-free quantization for low-bit vision transformers", "title_link": "https://proceedings.mlr.press/v202/liu23w.html", "id": "EWi0DLp8vRUJ", "cited_by_count": 18}, {"title": "Billm: Pushing the limit of post-training quantization for llms", "title_link": "https://arxiv.org/abs/2402.04291", "id": "oTlxs9Jcl0sJ", "cited_by_count": 32}, {"title": "Comq: A backpropagation-free algorithm for post-training quantization", "title_link": "https://arxiv.org/abs/2403.07134", "id": "TmD9-8imUcIJ", "cited_by_count": 2}, {"title": "Towards efficient post-training quantization of pre-trained language models", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/096347b4efc264ae7f07742fea34af1f-Abstract-Conference.html", "id": "tN16y3kVeNYJ", "cited_by_count": 55}, {"title": "MGRQ: Post-Training Quantization For Vision Transformer With Mixed Granularity Reconstruction", "title_link": "https://arxiv.org/abs/2406.09229", "id": "5hVG1U2J-KMJ", "cited_by_count": 1}, {"title": "Quantpipe: Applying adaptive post-training quantization for distributed transformer pipelines in dynamic edge environments", "title_link": "https://ieeexplore.ieee.org/abstract/document/10096632/", "id": "Uycz-VeWEUAJ", "cited_by_count": 5}, {"title": "Vaqf: Fully automatic software-hardware co-design framework for low-bit vision transformer", "title_link": "https://arxiv.org/abs/2201.06618", "id": "Im7_w0q2SbkJ", "cited_by_count": 51}, {"title": "Efficient Adaptive Activation Rounding for Post-Training Quantization", "title_link": "https://arxiv.org/abs/2208.11945", "id": "wLlumbWxWHYJ", "cited_by_count": 8}, {"title": "[PDF][PDF] Optimization of convolutional neural networks and transformer neural networks using post-training integer quantization", "title_link": "https://opus4.kobv.de/opus4-haw/files/3660/I001377901Thesis.pdf", "id": "5McryvDFAwIJ", "cited_by_count": 2}, {"title": "FrameQuant: Flexible Low-Bit Quantization for Transformers", "title_link": "https://arxiv.org/abs/2403.06082", "id": "IL9ma09lwMIJ", "cited_by_count": 3}, {"title": "ViT-1.58 b: Mobile Vision Transformers in the 1-bit Era", "title_link": "https://arxiv.org/abs/2406.18051", "id": "C7GCXI5qwxoJ", "cited_by_count": 1}, {"title": "Packqvit: Faster sub-8-bit vision transformers via full and packed quantization on the mobile", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/1c92edb990a05f2269f0cc3afbb4c952-Abstract-Conference.html", "id": "TB8FpKrHfCMJ", "cited_by_count": 8}, {"title": "Outlier suppression: Pushing the limit of low-bit transformer language models", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/6f6db140de9c9f111b12ef8a216320a9-Abstract-Conference.html", "id": "ZtpuMfg9oo8J", "cited_by_count": 92}, {"title": "Reshape and Adapt for Output Quantization (RAOQ): Quantization-aware Training for In-memory Computing Systems", "title_link": "https://openreview.net/forum?id=fM9xTkpAdu", "id": "B2OChDoDGssJ", "cited_by_count": 4}, {"title": "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization", "title_link": "https://arxiv.org/abs/2406.05981", "id": "dV4FY5LebF0J", "cited_by_count": 1}, {"title": "Model quantization and hardware acceleration for vision transformers: A comprehensive survey", "title_link": "https://arxiv.org/abs/2405.00314", "id": "15EF6WTmIhsJ", "cited_by_count": 3}, {"title": "Auto-vit-acc: An fpga-aware automatic acceleration framework for vision transformer with mixed-scheme quantization", "title_link": "https://ieeexplore.ieee.org/abstract/document/10035108/", "id": "h1rQeGWkXbIJ", "cited_by_count": 51}, {"title": "Quantformer: Learning extremely low-precision vision transformers", "title_link": "https://ieeexplore.ieee.org/abstract/document/9992209/", "id": "1oYXrcV7pyIJ", "cited_by_count": 12}, {"title": "QuantSR: accurate low-bit quantization for efficient image super-resolution", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/b2169d573d75ff90c7b12dc3a5fc2898-Abstract-Conference.html", "id": "tVtyZmU0ptYJ", "cited_by_count": 12}, {"title": "Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization", "title_link": "https://arxiv.org/abs/2403.12422", "id": "FdmCC8U51SkJ", "cited_by_count": 5}, {"title": "Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation", "title_link": "https://arxiv.org/abs/2303.08302", "id": "vRJymvaFeV0J", "cited_by_count": 36}, {"title": "OneBit: Towards Extremely Low-bit Large Language Models", "title_link": "https://arxiv.org/abs/2402.11295", "id": "OLSU5sXFCBcJ", "cited_by_count": 16}, {"title": "Apiq: Finetuning of 2-bit quantized large language model", "title_link": "https://arxiv.org/abs/2402.05147", "id": "0qvPrUbgNA4J", "cited_by_count": 2}, {"title": "Bi-ViT: Pushing the Limit of Vision Transformer Quantization", "title_link": "https://ojs.aaai.org/index.php/AAAI/article/view/28109", "id": "XGIRK4o_lUcJ", "cited_by_count": 7}, {"title": "O-2A: Ourlier-Aware Compression for 8-bit Post-Training Quantization Model", "title_link": "https://ieeexplore.ieee.org/abstract/document/10237192/", "id": "0I49vz7zdScJ", "cited_by_count": 1}, {"title": "Qdrop: Randomly dropping quantization for extremely low-bit post-training quantization", "title_link": "https://arxiv.org/abs/2203.05740", "id": "8q2l06dbGZsJ", "cited_by_count": 118}, {"title": "Toward accurate post-training quantization for image super resolution", "title_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Tu_Toward_Accurate_Post-Training_Quantization_for_Image_Super_Resolution_CVPR_2023_paper.html", "id": "HfO4C6msHn4J", "cited_by_count": 13}, {"title": "Ant: Exploiting adaptive numerical data type for low-bit deep neural network quantization", "title_link": "https://ieeexplore.ieee.org/abstract/document/9923832/", "id": "vb97hRoe7hsJ", "cited_by_count": 32}, {"title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/c3ba4962c05c49636d4c6206a97e9c8a-Abstract-Conference.html", "id": "2NM1BuhHURsJ", "cited_by_count": 734}, {"title": "Bitnet: Scaling 1-bit transformers for large language models", "title_link": "https://arxiv.org/abs/2310.11453", "id": "rY4err_Kw_cJ", "cited_by_count": 55}, {"title": "A Simple Low-bit Quantization Framework for Video Snapshot Compressive Imaging", "title_link": "https://arxiv.org/abs/2407.21517", "id": "9KVAkrnpL8gJ", "cited_by_count": 1}, {"title": "Quip: 2-bit quantization of large language models with guarantees", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/0df38cd13520747e1e64e5b123a78ef8-Abstract-Conference.html", "id": "b0_oT2nHOMkJ", "cited_by_count": 86}, {"title": "Bipft: Binary pre-trained foundation transformer with low-rank estimation of binarization residual polynomials", "title_link": "https://ojs.aaai.org/index.php/AAAI/article/view/29542", "id": "PxUvVU6DlO4J", "cited_by_count": 1}, {"title": "Low-Rank Quantization-Aware Training for LLMs", "title_link": "https://arxiv.org/abs/2406.06385", "id": "zk9kqTJExzEJ", "cited_by_count": 4}, {"title": "Accelerating attention through gradient-based learned runtime pruning", "title_link": "https://dl.acm.org/doi/abs/10.1145/3470496.3527423", "id": "lBcEaeWHHUkJ", "cited_by_count": 34}, {"title": "Memory efficient optimizers with 4-bit states", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/3122aaa22b2fe83f9cead1a696f65ceb-Abstract-Conference.html", "id": "_TCnoz-0QP8J", "cited_by_count": 13}, {"title": "SensiMix: Sensitivity-Aware 8-bit index & 1-bit value mixed precision quantization for BERT compression", "title_link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0265621", "id": "jNelrbGxFzIJ", "cited_by_count": 13}, {"title": "Jumping through local minima: Quantization in the loss landscape of vision transformers", "title_link": "http://openaccess.thecvf.com/content/ICCV2023/html/Frumkin_Jumping_through_Local_Minima_Quantization_in_the_Loss_Landscape_of_ICCV_2023_paper.html", "id": "FNb1L-r3EXUJ", "cited_by_count": 14}, {"title": "Low-Precision Quantization Techniques for Hardware-Implementation-Friendly BERT Models", "title_link": "https://ieeexplore.ieee.org/abstract/document/9806238/", "id": "w7QsQHz7wtgJ", "cited_by_count": 4}, {"title": "PEANO-ViT: Power-Efficient Approximations of Non-Linearities in Vision Transformers", "title_link": "https://dl.acm.org/doi/abs/10.1145/3665314.3670843", "id": "aeeQA5jT-wUJ", "cited_by_count": 3}, {"title": "I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models", "title_link": "https://arxiv.org/abs/2405.17849", "id": "ptXne1kMCn4J", "cited_by_count": 1}, {"title": "Sub-8-bit quantization for on-device speech recognition: A regularization-free approach", "title_link": "https://ieeexplore.ieee.org/abstract/document/10022821/", "id": "eCmqCyt4-9IJ", "cited_by_count": 10}, {"title": "Hybrid post-training quantization for super-resolution neural network compression", "title_link": "https://ieeexplore.ieee.org/abstract/document/10093054/", "id": "sYd7aTxa3bEJ", "cited_by_count": 8}, {"title": "Norm tweaking: High-performance low-bit quantization of large language models", "title_link": "https://ojs.aaai.org/index.php/AAAI/article/view/29815", "id": "7K_-DYAGyvgJ", "cited_by_count": 24}, {"title": "Optimal brain compression: A framework for accurate post-training quantization and pruning", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/1caf09c9f4e6b0150b06a07e77f2710c-Abstract-Conference.html", "id": "szf8IM6W6R4J", "cited_by_count": 169}, {"title": "Quantized feature distillation for network quantization", "title_link": "https://ojs.aaai.org/index.php/AAAI/article/view/26354", "id": "qsL3TW820z4J", "cited_by_count": 5}, {"title": "On-device ai: Quantization-aware training of transformers in time-series", "title_link": "https://ieeexplore.ieee.org/abstract/document/10150339/", "id": "0T-sqNxTCT4J", "cited_by_count": 1}, {"title": "Flexround: Learnable rounding based on element-wise division for post-training quantization", "title_link": "https://proceedings.mlr.press/v202/lee23h.html", "id": "LiYzXZ3i0XoJ", "cited_by_count": 21}, {"title": "[PDF][PDF] KIVI: Plug-and-play 2bit KV Cache Quantization with Streaming Asymmetric Quantization", "title_link": "https://www.researchgate.net/profile/Zirui-Liu-29/publication/376831635_KIVI_Plug-and-play_2bit_KV_Cache_Quantization_with_Streaming_Asymmetric_Quantization/links/658b5d282468df72d3db3280/KIVI-Plug-and-play-2bit-KV-Cache-Quantization-with-Streaming-Asymmetric-Quantization.pdf", "id": "4rsNOUn7geEJ", "cited_by_count": 1}, {"title": "SpinQuant--LLM quantization with learned rotations", "title_link": "https://arxiv.org/abs/2405.16406", "id": "LJocxR8Bq7gJ", "cited_by_count": 7}, {"title": "Retraining-free model quantization via one-shot weight-coupling learning", "title_link": "https://openaccess.thecvf.com/content/CVPR2024/html/Tang_Retraining-Free_Model_Quantization_via_One-Shot_Weight-Coupling_Learning_CVPR_2024_paper.html", "id": "VdwiP0d20-8J", "cited_by_count": 4}, {"title": "Quarot: Outlier-free 4-bit inference in rotated llms", "title_link": "https://arxiv.org/abs/2404.00456", "id": "fgdZvS1a_DQJ", "cited_by_count": 29}, {"title": "Effective Interplay between Sparsity and Quantization: From Theory to Practice", "title_link": "https://arxiv.org/abs/2405.20935", "id": "FNmVg4oWtkIJ", "cited_by_count": 3}, {"title": "Understanding neural network binarization with forward and backward proximal quantizers", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/7f70331dbe58ad59d83941dfa7d975aa-Abstract-Conference.html", "id": "-ESX4TbxVwsJ", "cited_by_count": 1}, {"title": "Kivi: A tuning-free asymmetric 2bit quantization for kv cache", "title_link": "https://arxiv.org/abs/2402.02750", "id": "ddvel63pcjMJ", "cited_by_count": 22}, {"title": "Binaryvit: Towards efficient and accurate binary vision transformers", "title_link": "https://ieeexplore.ieee.org/abstract/document/10671591/", "id": "cbKItp-_N3AJ", "cited_by_count": 2}, {"title": "A survey on efficient vision transformers: algorithms, techniques, and performance benchmarking", "title_link": "https://ieeexplore.ieee.org/abstract/document/10508091/", "id": "-AbH6_x4TNgJ", "cited_by_count": 16}, {"title": "The case for 4-bit precision: k-bit inference scaling laws", "title_link": "https://proceedings.mlr.press/v202/dettmers23a", "id": "VhdKmOrSDjQJ", "cited_by_count": 140}, {"title": "TerViT: An efficient ternary vision transformer", "title_link": "https://arxiv.org/abs/2201.08050", "id": "PsbWMJRMcIAJ", "cited_by_count": 6}, {"title": "Bivit: Extremely compressed binary vision transformers", "title_link": "http://openaccess.thecvf.com/content/ICCV2023/html/He_BiViT_Extremely_Compressed_Binary_Vision_Transformers_ICCV_2023_paper.html", "id": "U_vHvlrWqvwJ", "cited_by_count": 19}, {"title": "Squat: Sharpness-and quantization-aware training for bert", "title_link": "https://arxiv.org/abs/2210.07171", "id": "FwJ4vQC0GYYJ", "cited_by_count": 6}, {"title": "EfficientQ: An efficient and accurate post-training neural network quantization method for medical image segmentation", "title_link": "https://www.sciencedirect.com/science/article/pii/S1361841524002020", "id": "3TyA_sEpVl4J", "cited_by_count": 1}, {"title": "Loftq: Lora-fine-tuning-aware quantization for large language models", "title_link": "https://arxiv.org/abs/2310.08659", "id": "3wQ-ey6iLsAJ", "cited_by_count": 87}, {"title": "[PDF][PDF] Hardware-friendly compression and hardware acceleration for transformer: A survey", "title_link": "https://www.aimspress.com/aimspress-data/era/2022/10/PDF/era-30-10-192.pdf", "id": "lkElcTUtwOsJ", "cited_by_count": 4}, {"title": "Qllm: Accurate and efficient low-bitwidth quantization for large language models", "title_link": "https://arxiv.org/abs/2310.08041", "id": "0Dq2wJ8OF9oJ", "cited_by_count": 34}, {"title": "Bitdistiller: Unleashing the potential of sub-4-bit llms via self-distillation", "title_link": "https://arxiv.org/abs/2402.10631", "id": "KPnT_XzTUAwJ", "cited_by_count": 8}, {"title": "Ucvit: Hardware-friendly vision transformer via unified compression", "title_link": "https://ieeexplore.ieee.org/abstract/document/9937660/", "id": "zNXzOCeR9cMJ", "cited_by_count": 4}, {"title": "Patch-wise Mixed-Precision Quantization of Vision Transformer", "title_link": "https://ieeexplore.ieee.org/abstract/document/10191205/", "id": "5kMLKPD1n0QJ", "cited_by_count": 4}, {"title": "Alphatuning: Quantization-aware parameter-efficient adaptation of large-scale pre-trained language models", "title_link": "https://arxiv.org/abs/2210.03858", "id": "9kddTDnX5gkJ", "cited_by_count": 30}, {"title": "FBPT: A Fully Binary Point Transformer", "title_link": "https://arxiv.org/abs/2403.09998", "id": "iitqf5i2GF4J", "cited_by_count": 1}, {"title": "A survey of techniques for optimizing transformer inference", "title_link": "https://www.sciencedirect.com/science/article/pii/S1383762123001698", "id": "eR3vo3-uWcgJ", "cited_by_count": 38}, {"title": "Extreme compression of large language models via additive quantization", "title_link": "https://arxiv.org/abs/2401.06118", "id": "MPm3IDJfl7wJ", "cited_by_count": 30}, {"title": "LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models", "title_link": "https://arxiv.org/abs/2405.06001", "id": "YImBWCczg3kJ", "cited_by_count": 4}, {"title": "Optimal clipping and magnitude-aware differentiation for improved quantization-aware training", "title_link": "https://proceedings.mlr.press/v162/sakr22a.html", "id": "eW3c1fI6tEYJ", "cited_by_count": 35}, {"title": "The era of 1-bit llms: All large language models are in 1.58 bits", "title_link": "https://arxiv.org/abs/2402.17764", "id": "GI2nY7Vp2eoJ", "cited_by_count": 93}, {"title": "Pruning vs quantization: which is better?", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/c48bc80aa5d3cbbdd712d1cc107b8319-Abstract-Conference.html", "id": "VkDaEawlRfwJ", "cited_by_count": 25}, {"title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models", "title_link": "https://arxiv.org/abs/2402.14866", "id": "0p4ffucAkNMJ", "cited_by_count": 4}, {"title": "A frustratingly easy post-training quantization scheme for llms", "title_link": "https://aclanthology.org/2023.emnlp-main.892/", "id": "m0-t5CFyvKsJ", "cited_by_count": 5}, {"title": "Synergistic self-supervised and quantization learning", "title_link": "https://link.springer.com/chapter/10.1007/978-3-031-20056-4_34", "id": "gGfMoSYhXTMJ", "cited_by_count": 11}, {"title": "Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs", "title_link": "https://arxiv.org/abs/2405.14428", "id": "jo7K6u6J92wJ", "cited_by_count": 1}, {"title": "A survey of FPGA and ASIC designs for transformer inference acceleration and optimization", "title_link": "https://www.sciencedirect.com/science/article/pii/S138376212400184X", "id": "OPx-S9Yq3YIJ", "cited_by_count": 1}, {"title": "Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference", "title_link": "https://arxiv.org/abs/2403.05465", "id": "_lapccbnv6gJ", "cited_by_count": 5}, {"title": "Omniquant: Omnidirectionally calibrated quantization for large language models", "title_link": "https://arxiv.org/abs/2308.13137", "id": "nPjMLgOsAfAJ", "cited_by_count": 103}, {"title": "Efficientdm: Efficient quantization-aware fine-tuning of low-bit diffusion models", "title_link": "https://arxiv.org/abs/2310.03270", "id": "PRag3r-x4eEJ", "cited_by_count": 18}, {"title": "QQQ: Quality Quattuor-Bit Quantization for Large Language Models", "title_link": "https://arxiv.org/abs/2406.09904", "id": "6Cjmy7EJ_8AJ", "cited_by_count": 1}, {"title": "Compensate Quantization Errors: Make Weights Hierarchical to Compensate Each Other", "title_link": "https://arxiv.org/abs/2406.16299", "id": "mtXU0lqyVmkJ", "cited_by_count": 1}, {"title": "Modulora: Finetuning 3-bit llms on consumer gpus by integrating with modular quantizers", "title_link": "https://arxiv.org/abs/2309.16119", "id": "-E6JIOBw4nIJ", "cited_by_count": 4}, {"title": "GSB: Group superposition binarization for vision transformer with limited training samples", "title_link": "https://www.sciencedirect.com/science/article/pii/S0893608024000492", "id": "gMDP1nKRaUcJ", "cited_by_count": 2}, {"title": "Accurate neural training with 4-bit matrix multiplications at standard formats", "title_link": "https://openreview.net/forum?id=yTbNYYcopd", "id": "DsTio9bYQs4J", "cited_by_count": 8}, {"title": "Leanquant: Accurate large language model quantization with loss-error-aware grid", "title_link": "https://arxiv.org/abs/2407.10032", "id": "I9tzGMTn07sJ", "cited_by_count": 2}, {"title": "BinaryViT: pushing binary vision transformers towards convolutional models", "title_link": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Le_BinaryViT_Pushing_Binary_Vision_Transformers_Towards_Convolutional_Models_CVPRW_2023_paper.html", "id": "ULxPe1o1EyUJ", "cited_by_count": 16}, {"title": "Affinequant: Affine transformation quantization for large language models", "title_link": "https://arxiv.org/abs/2403.12544", "id": "1OFKAyLW6-8J", "cited_by_count": 10}, {"title": "Optimize weight rounding via signed gradient descent for the quantization of llms", "title_link": "https://arxiv.org/abs/2309.05516", "id": "QwVTFKxHc5UJ", "cited_by_count": 11}, {"title": "QTIP: Quantization with Trellises and Incoherence Processing", "title_link": "https://arxiv.org/abs/2406.11235", "id": "oHpwuQjwJ6MJ", "cited_by_count": 1}, {"title": "BitCluster: Fine-grained weight quantization for load-balanced bit-serial neural network accelerators", "title_link": "https://ieeexplore.ieee.org/abstract/document/9691457/", "id": "_j8zpSgPqX0J", "cited_by_count": 6}, {"title": "Finding the task-optimal low-bit sub-distribution in deep neural networks", "title_link": "https://proceedings.mlr.press/v162/dong22a.html", "id": "XOhlwJD00GQJ", "cited_by_count": 13}, {"title": "IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers", "title_link": "https://arxiv.org/abs/2403.07339", "id": "W8FU9K4RPCIJ", "cited_by_count": 1}, {"title": "LQER: Low-Rank Quantization Error Reconstruction for LLMs", "title_link": "https://arxiv.org/abs/2402.02446", "id": "GqWbB--3GScJ", "cited_by_count": 5}, {"title": "Quantized sparse training: A unified trainable framework for joint pruning and quantization in DNNs", "title_link": "https://dl.acm.org/doi/abs/10.1145/3524066", "id": "bDpY3eKEXBYJ", "cited_by_count": 10}, {"title": "Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?", "title_link": "https://arxiv.org/abs/2310.05079", "id": "v1-mXoHU68IJ", "cited_by_count": 4}, {"title": "LCQ: Low-Rank Codebook based Quantization for Large Language Models", "title_link": "https://arxiv.org/abs/2405.20973", "id": "pRXMyxh77x0J", "cited_by_count": 1}, {"title": "Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models", "title_link": "https://arxiv.org/abs/2206.09557", "id": "zwplhYAy3psJ", "cited_by_count": 92}, {"title": "PB-LLM: Partially Binarized Large Language Models", "title_link": "https://openreview.net/forum?id=BifeBRhikU", "id": "6SwEmqzWKLwJ", "cited_by_count": 1}, {"title": "Mitigating the impact of outlier channels for language model quantization with activation regularization", "title_link": "https://arxiv.org/abs/2404.03605", "id": "kzFAGeZF30gJ", "cited_by_count": 3}, {"title": "[PDF][PDF] QuantEase: Optimization-based Quantization for Language Models-An Efficient and Intuitive Algorithm", "title_link": "https://www.researchgate.net/profile/Kayhan-Behdin/publication/373685425_QuantEase_Optimization-based_Quantization_for_Language_Models_-_An_Efficient_and_Intuitive_Algorithm/links/64f88bdff160f748d6d16c80/QuantEase-Optimization-based-Quantization-for-Language-Models-An-Efficient-and-Intuitive-Algorithm.pdf", "id": "fkDEGQyPiMoJ", "cited_by_count": 9}, {"title": "Revisiting the parameter efficiency of adapters from the perspective of precision redundancy", "title_link": "http://openaccess.thecvf.com/content/ICCV2023/html/Jie_Revisiting_the_Parameter_Efficiency_of_Adapters_from_the_Perspective_of_ICCV_2023_paper.html", "id": "Veq3mAkAmIEJ", "cited_by_count": 21}, {"title": "One-shot model for mixed-precision quantization", "title_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Koryakovskiy_One-Shot_Model_for_Mixed-Precision_Quantization_CVPR_2023_paper.html", "id": "6FkK5D9A8SsJ", "cited_by_count": 13}, {"title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "title_link": "https://arxiv.org/abs/2405.03917", "id": "KemNGS09R_gJ", "cited_by_count": 7}, {"title": "Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models", "title_link": "https://arxiv.org/abs/2406.12311", "id": "tiWugOj1anEJ", "cited_by_count": 1}, {"title": "Token-scaled logit distillation for ternary weight generative language models", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/8342218a4ec08b8c19661725e9cd6c0b-Abstract-Conference.html", "id": "Dl7p0RzIH14J", "cited_by_count": 11}, {"title": "Pb-llm: Partially binarized large language models", "title_link": "https://arxiv.org/abs/2310.00034", "id": "mZnxHQtv7VcJ", "cited_by_count": 28}, {"title": "Q-S5: Towards Quantized State Space Models", "title_link": "https://arxiv.org/abs/2406.09477", "id": "SYT78S70fHQJ", "cited_by_count": 3}, {"title": "Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression", "title_link": "https://arxiv.org/abs/2405.12591", "id": "LuON7nW_FkgJ", "cited_by_count": 1}, {"title": "Unified Scaling-Based Pure-Integer Quantization for Low-Power Accelerator of Complex CNNs", "title_link": "https://www.mdpi.com/2079-9292/12/12/2660", "id": "H7_d2tn7-8MJ", "cited_by_count": 2}, {"title": "TernaryLLM: Ternarized Large Language Model", "title_link": "https://arxiv.org/abs/2406.07177", "id": "EyAfc0nW464J", "cited_by_count": 2}, {"title": "More is Less\u2013Byte-quantized models are faster than bit-quantized models on the edge", "title_link": "https://ieeexplore.ieee.org/abstract/document/10020437/", "id": "_aJGehIJAXEJ", "cited_by_count": 1}, {"title": "Quantized distributed training of large models with convergence guarantees", "title_link": "https://proceedings.mlr.press/v202/markov23a.html", "id": "emhOjlTzW4oJ", "cited_by_count": 9}, {"title": "SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models", "title_link": "https://arxiv.org/abs/2405.14917", "id": "Sa24Yr7tHswJ", "cited_by_count": 3}, {"title": "USM-Lite: Quantization and Sparsity Aware Fine-Tuning for Speech Recognition with Universal Speech Models", "title_link": "https://ieeexplore.ieee.org/abstract/document/10448217/", "id": "cqUpAWMQO8oJ", "cited_by_count": 3}, {"title": "Accurate lora-finetuning quantization of llms via information retention", "title_link": "https://arxiv.org/abs/2402.05445", "id": "n9_1W7Mc-xAJ", "cited_by_count": 23}, {"title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification", "title_link": "https://arxiv.org/abs/2405.14256", "id": "0wKnyPeFiagJ", "cited_by_count": 5}, {"title": "Vitality: Unifying low-rank and sparse approximation for vision transformer acceleration with a linear taylor attention", "title_link": "https://ieeexplore.ieee.org/abstract/document/10071081/", "id": "e-AVqsA405sJ", "cited_by_count": 39}, {"title": "Kvquant: Towards 10 million context length llm inference with kv cache quantization", "title_link": "https://arxiv.org/abs/2401.18079", "id": "hXjqeDdnmXIJ", "cited_by_count": 16}, {"title": "Tender: Accelerating Large Language Models via Tensor Decomposition and Runtime Requantization", "title_link": "https://arxiv.org/abs/2406.12930", "id": "kNNwLR3ICIYJ", "cited_by_count": 3}, {"title": "Progressive Gradient Flow for Robust N: M Sparsity Training in Transformers", "title_link": "https://arxiv.org/abs/2402.04744", "id": "1wQ2EmsqtEUJ", "cited_by_count": 2}, {"title": "SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference", "title_link": "https://ieeexplore.ieee.org/abstract/document/10323725/", "id": "3-aPdgWUz-8J", "cited_by_count": 4}, {"title": "Toward efficient low-precision training: Data format optimization and hysteresis quantization", "title_link": "https://openreview.net/forum?id=3HJOA-1hb0e", "id": "9wYo_jBpimAJ", "cited_by_count": 9}, {"title": "An effective post-training embedding binarization approach for fast online top-k passage matching", "title_link": "https://aclanthology.org/2022.aacl-short.14/", "id": "Mf2AuKtnJykJ", "cited_by_count": 10}, {"title": "Atalanta: A Bit is Worth a \u201cThousand\u201d Tensor Values", "title_link": "https://dl.acm.org/doi/abs/10.1145/3620665.3640356", "id": "VhTPWOTN-gcJ", "cited_by_count": 2}, {"title": "Efficient quantized sparse matrix operations on tensor cores", "title_link": "https://ieeexplore.ieee.org/abstract/document/10046057/", "id": "CUlHKMkgfAUJ", "cited_by_count": 29}, {"title": "Spiking-Transformer Optimization on FPGA for Image Classification and Captioning", "title_link": "https://ieeexplore.ieee.org/abstract/document/10500284/", "id": "aBbfc2TOiJ4J", "cited_by_count": 1}, {"title": "Bibert: Accurate fully binarized bert", "title_link": "https://arxiv.org/abs/2203.06390", "id": "220arbo05FAJ", "cited_by_count": 100}, {"title": "[PDF][PDF] Llm-mq: Mixed-precision quantization for efficient llm deployment", "title_link": "https://www.researchgate.net/profile/Luning-Wang-16/publication/376624532_LLM-MQ_Mixed-precision_Quantization_for_Efficient_LLM_Deployment/links/65818dd43c472d2e8e707515/LLM-MQ-Mixed-precision-Quantization-for-Efficient-LLM-Deployment.pdf", "id": "-1xaj99XrDkJ", "cited_by_count": 8}, {"title": "Quantized Graph Neural Networks for Image Classification", "title_link": "https://www.mdpi.com/2227-7390/11/24/4927", "id": "v05wuA9l_a4J", "cited_by_count": 1}, {"title": "General Purpose Deep Learning Accelerator Based on Bit Interleaving", "title_link": "https://ieeexplore.ieee.org/abstract/document/10359130/", "id": "T7uLSSwrzhAJ", "cited_by_count": 2}, {"title": "Model compression and efficient inference for large language models: A survey", "title_link": "https://arxiv.org/abs/2402.09748", "id": "KdnkYWkKlUAJ", "cited_by_count": 16}, {"title": "Unlocking tokens as data points for generalization bounds on larger language models", "title_link": "https://arxiv.org/abs/2407.18158", "id": "dmNdzD0oOscJ", "cited_by_count": 2}, {"title": "T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge", "title_link": "https://arxiv.org/abs/2407.00088", "id": "WvHbX4NSLFkJ", "cited_by_count": 1}, {"title": "Winning both the accuracy of floating point activation and the simplicity of integer arithmetic", "title_link": "https://openreview.net/forum?id=z92lBy1ehjI", "id": "wvOjhQfD3XcJ", "cited_by_count": 3}, {"title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models", "title_link": "https://arxiv.org/abs/2405.06219", "id": "z2JYVd1WvsUJ", "cited_by_count": 6}, {"title": "Transhash: Transformer-based hamming hashing for efficient image retrieval", "title_link": "https://dl.acm.org/doi/abs/10.1145/3512527.3531405", "id": "NzLBvGCeDrYJ", "cited_by_count": 42}, {"title": "With shared microexponents, a little shifting goes a long way", "title_link": "https://dl.acm.org/doi/abs/10.1145/3579371.3589351", "id": "tB0tY_-g6qQJ", "cited_by_count": 31}, {"title": "Quantized prompt for efficient generalization of vision-language models", "title_link": "https://arxiv.org/abs/2407.10704", "id": "cc5l1AhuD_EJ", "cited_by_count": 1}, {"title": "The Role of Feature Correlation on Quantized Neural Networks", "title_link": "https://ieeexplore.ieee.org/abstract/document/10389686/", "id": "Z7fSh2vrMCYJ", "cited_by_count": 1}, {"title": "u-P: The Unit-Scaled Maximal Update Parametrization", "title_link": "https://arxiv.org/abs/2407.17465", "id": "9JFANOfcAhgJ", "cited_by_count": 2}, {"title": "Boost transformer-based language models with gpu-friendly sparsity and quantization", "title_link": "https://aclanthology.org/2023.findings-acl.15/", "id": "dq1G1fBIhJoJ", "cited_by_count": 7}, {"title": "Fast matrix multiplications for lookup table-quantized llms", "title_link": "https://arxiv.org/abs/2407.10960", "id": "Q1X8zw29RnAJ", "cited_by_count": 1}, {"title": "IVQ: In-memory acceleration of DNN inference exploiting varied quantization", "title_link": "https://ieeexplore.ieee.org/abstract/document/9724255/", "id": "W43ntm9oNvUJ", "cited_by_count": 10}, {"title": "DB-LLM: Accurate dual-binarization for efficient LLMs", "title_link": "https://arxiv.org/abs/2402.11960", "id": "QK-4qIOQh5MJ", "cited_by_count": 13}, {"title": "Compressing Large Language Models using Low Rank and Low Precision Decomposition", "title_link": "https://arxiv.org/abs/2405.18886", "id": "d31XIWaPWM4J", "cited_by_count": 3}, {"title": "Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs", "title_link": "https://arxiv.org/abs/2402.10517", "id": "nOF3gaW2hrsJ", "cited_by_count": 1}, {"title": "A Study of Quantisation-aware Training on Time Series Transformer Models for Resource-constrained FPGAs", "title_link": "https://arxiv.org/abs/2310.02654", "id": "ntBR0CgwZbkJ", "cited_by_count": 1}, {"title": "The Tiny Time-series Transformer: Low-latency High-throughput Classification of Astronomical Transients using Deep Model Compression", "title_link": "https://arxiv.org/abs/2303.08951", "id": "2twOftaLC-IJ", "cited_by_count": 2}, {"title": "An efficient segmented quantization for graph neural networks", "title_link": "https://link.springer.com/article/10.1007/s42514-022-00121-z", "id": "wfNiCS_sOTMJ", "cited_by_count": 2}, {"title": "Generating Efficient Kernels for Quantized Inference on Large Language Models", "title_link": "https://openreview.net/forum?id=jjazoNAf1S", "id": "Iry2GHEjAswJ", "cited_by_count": 1}, {"title": "Q-dm: An efficient low-bit quantized diffusion model", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/f1ee1cca0721de55bb35cf28ab95e1b4-Abstract-Conference.html", "id": "8fMIhKln3hUJ", "cited_by_count": 19}, {"title": "Mixed precision dnn quantization for overlapped speech separation and recognition", "title_link": "https://ieeexplore.ieee.org/abstract/document/9746885/", "id": "yEIi0-pYf2wJ", "cited_by_count": 10}, {"title": "Understanding the potential of fpga-based spatial acceleration for large language model inference", "title_link": "https://dl.acm.org/doi/abs/10.1145/3656177", "id": "p_-Lv1pczqIJ", "cited_by_count": 12}]]