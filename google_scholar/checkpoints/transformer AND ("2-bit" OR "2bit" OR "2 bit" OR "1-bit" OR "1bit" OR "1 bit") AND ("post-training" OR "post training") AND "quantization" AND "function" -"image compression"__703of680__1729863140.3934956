[197, [{"title": "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-Abstract-Conference.html", "id": "M1H30TzgocoJ", "cited_by_count": 289}, {"title": "I-vit: Integer-only quantization for efficient vision transformer inference", "title_link": "http://openaccess.thecvf.com/content/ICCV2023/html/Li_I-ViT_Integer-only_Quantization_for_Efficient_Vision_Transformer_Inference_ICCV_2023_paper.html", "id": "wA-a8sZKDhMJ", "cited_by_count": 73}, {"title": "Deep compression of pre-trained transformer models", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/5b5618e7d061748267d74478b7c5b1ab-Abstract-Conference.html", "id": "dxVXcqWwqfUJ", "cited_by_count": 19}, {"title": "Q-vit: Accurate and fully quantized low-bit vision transformer", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/deb921bff461a7b0a5c344a4871e7101-Abstract-Conference.html", "id": "pYjd1jUZ5TYJ", "cited_by_count": 73}, {"title": "Mr. biq: Post-training non-uniform quantization based on minimizing the reconstruction error", "title_link": "http://openaccess.thecvf.com/content/CVPR2022/html/Jeon_Mr.BiQ_Post-Training_Non-Uniform_Quantization_Based_on_Minimizing_the_Reconstruction_Error_CVPR_2022_paper.html", "id": "LGScuzy99a8J", "cited_by_count": 40}, {"title": "Practical edge kernels for integer-only vision transformers under post-training quantization", "title_link": "https://proceedings.mlsys.org/paper_files/paper/2023/hash/12f429641e5c63a9ca7fd1c5c4804d32-Abstract-mlsys2023.html", "id": "G0Gs0uSEz74J", "cited_by_count": 4}, {"title": "Bit-shrinking: Limiting instantaneous sharpness for improving post-training quantization", "title_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Lin_Bit-Shrinking_Limiting_Instantaneous_Sharpness_for_Improving_Post-Training_Quantization_CVPR_2023_paper.html", "id": "hZRbsLkw7BcJ", "cited_by_count": 12}, {"title": "Tsptq-vit: Two-scaled post-training quantization for vision transformer", "title_link": "https://ieeexplore.ieee.org/abstract/document/10096817/", "id": "lLtKpfBdvKkJ", "cited_by_count": 3}, {"title": "Q-detr: An efficient low-bit quantized detection transformer", "title_link": "https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Q-DETR_An_Efficient_Low-Bit_Quantized_Detection_Transformer_CVPR_2023_paper.html?ref=blog.roboflow.com", "id": "73Yoqdgfi6oJ", "cited_by_count": 17}, {"title": "Oscillation-free quantization for low-bit vision transformers", "title_link": "https://proceedings.mlr.press/v202/liu23w.html", "id": "EWi0DLp8vRUJ", "cited_by_count": 18}, {"title": "Billm: Pushing the limit of post-training quantization for llms", "title_link": "https://arxiv.org/abs/2402.04291", "id": "oTlxs9Jcl0sJ", "cited_by_count": 32}, {"title": "Comq: A backpropagation-free algorithm for post-training quantization", "title_link": "https://arxiv.org/abs/2403.07134", "id": "TmD9-8imUcIJ", "cited_by_count": 2}, {"title": "Towards efficient post-training quantization of pre-trained language models", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/096347b4efc264ae7f07742fea34af1f-Abstract-Conference.html", "id": "tN16y3kVeNYJ", "cited_by_count": 55}, {"title": "MGRQ: Post-Training Quantization For Vision Transformer With Mixed Granularity Reconstruction", "title_link": "https://arxiv.org/abs/2406.09229", "id": "5hVG1U2J-KMJ", "cited_by_count": 1}, {"title": "Quantpipe: Applying adaptive post-training quantization for distributed transformer pipelines in dynamic edge environments", "title_link": "https://ieeexplore.ieee.org/abstract/document/10096632/", "id": "Uycz-VeWEUAJ", "cited_by_count": 5}, {"title": "Vaqf: Fully automatic software-hardware co-design framework for low-bit vision transformer", "title_link": "https://arxiv.org/abs/2201.06618", "id": "Im7_w0q2SbkJ", "cited_by_count": 51}, {"title": "Efficient Adaptive Activation Rounding for Post-Training Quantization", "title_link": "https://arxiv.org/abs/2208.11945", "id": "wLlumbWxWHYJ", "cited_by_count": 8}, {"title": "[PDF][PDF] Optimization of convolutional neural networks and transformer neural networks using post-training integer quantization", "title_link": "https://opus4.kobv.de/opus4-haw/files/3660/I001377901Thesis.pdf", "id": "5McryvDFAwIJ", "cited_by_count": 2}, {"title": "FrameQuant: Flexible Low-Bit Quantization for Transformers", "title_link": "https://arxiv.org/abs/2403.06082", "id": "IL9ma09lwMIJ", "cited_by_count": 3}, {"title": "ViT-1.58 b: Mobile Vision Transformers in the 1-bit Era", "title_link": "https://arxiv.org/abs/2406.18051", "id": "C7GCXI5qwxoJ", "cited_by_count": 1}, {"title": "Packqvit: Faster sub-8-bit vision transformers via full and packed quantization on the mobile", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/1c92edb990a05f2269f0cc3afbb4c952-Abstract-Conference.html", "id": "TB8FpKrHfCMJ", "cited_by_count": 8}, {"title": "Outlier suppression: Pushing the limit of low-bit transformer language models", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/6f6db140de9c9f111b12ef8a216320a9-Abstract-Conference.html", "id": "ZtpuMfg9oo8J", "cited_by_count": 92}, {"title": "Reshape and Adapt for Output Quantization (RAOQ): Quantization-aware Training for In-memory Computing Systems", "title_link": "https://openreview.net/forum?id=fM9xTkpAdu", "id": "B2OChDoDGssJ", "cited_by_count": 4}, {"title": "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization", "title_link": "https://arxiv.org/abs/2406.05981", "id": "dV4FY5LebF0J", "cited_by_count": 1}, {"title": "Model quantization and hardware acceleration for vision transformers: A comprehensive survey", "title_link": "https://arxiv.org/abs/2405.00314", "id": "15EF6WTmIhsJ", "cited_by_count": 3}, {"title": "Auto-vit-acc: An fpga-aware automatic acceleration framework for vision transformer with mixed-scheme quantization", "title_link": "https://ieeexplore.ieee.org/abstract/document/10035108/", "id": "h1rQeGWkXbIJ", "cited_by_count": 51}, {"title": "Quantformer: Learning extremely low-precision vision transformers", "title_link": "https://ieeexplore.ieee.org/abstract/document/9992209/", "id": "1oYXrcV7pyIJ", "cited_by_count": 12}, {"title": "QuantSR: accurate low-bit quantization for efficient image super-resolution", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/b2169d573d75ff90c7b12dc3a5fc2898-Abstract-Conference.html", "id": "tVtyZmU0ptYJ", "cited_by_count": 12}, {"title": "Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization", "title_link": "https://arxiv.org/abs/2403.12422", "id": "FdmCC8U51SkJ", "cited_by_count": 5}, {"title": "Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation", "title_link": "https://arxiv.org/abs/2303.08302", "id": "vRJymvaFeV0J", "cited_by_count": 36}, {"title": "OneBit: Towards Extremely Low-bit Large Language Models", "title_link": "https://arxiv.org/abs/2402.11295", "id": "OLSU5sXFCBcJ", "cited_by_count": 16}, {"title": "Apiq: Finetuning of 2-bit quantized large language model", "title_link": "https://arxiv.org/abs/2402.05147", "id": "0qvPrUbgNA4J", "cited_by_count": 2}, {"title": "Bi-ViT: Pushing the Limit of Vision Transformer Quantization", "title_link": "https://ojs.aaai.org/index.php/AAAI/article/view/28109", "id": "XGIRK4o_lUcJ", "cited_by_count": 7}, {"title": "O-2A: Ourlier-Aware Compression for 8-bit Post-Training Quantization Model", "title_link": "https://ieeexplore.ieee.org/abstract/document/10237192/", "id": "0I49vz7zdScJ", "cited_by_count": 1}, {"title": "Qdrop: Randomly dropping quantization for extremely low-bit post-training quantization", "title_link": "https://arxiv.org/abs/2203.05740", "id": "8q2l06dbGZsJ", "cited_by_count": 118}, {"title": "Toward accurate post-training quantization for image super resolution", "title_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Tu_Toward_Accurate_Post-Training_Quantization_for_Image_Super_Resolution_CVPR_2023_paper.html", "id": "HfO4C6msHn4J", "cited_by_count": 13}, {"title": "Ant: Exploiting adaptive numerical data type for low-bit deep neural network quantization", "title_link": "https://ieeexplore.ieee.org/abstract/document/9923832/", "id": "vb97hRoe7hsJ", "cited_by_count": 32}, {"title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/c3ba4962c05c49636d4c6206a97e9c8a-Abstract-Conference.html", "id": "2NM1BuhHURsJ", "cited_by_count": 734}, {"title": "Bitnet: Scaling 1-bit transformers for large language models", "title_link": "https://arxiv.org/abs/2310.11453", "id": "rY4err_Kw_cJ", "cited_by_count": 55}, {"title": "A Simple Low-bit Quantization Framework for Video Snapshot Compressive Imaging", "title_link": "https://arxiv.org/abs/2407.21517", "id": "9KVAkrnpL8gJ", "cited_by_count": 1}, {"title": "Quip: 2-bit quantization of large language models with guarantees", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/0df38cd13520747e1e64e5b123a78ef8-Abstract-Conference.html", "id": "b0_oT2nHOMkJ", "cited_by_count": 86}, {"title": "Bipft: Binary pre-trained foundation transformer with low-rank estimation of binarization residual polynomials", "title_link": "https://ojs.aaai.org/index.php/AAAI/article/view/29542", "id": "PxUvVU6DlO4J", "cited_by_count": 1}, {"title": "Low-Rank Quantization-Aware Training for LLMs", "title_link": "https://arxiv.org/abs/2406.06385", "id": "zk9kqTJExzEJ", "cited_by_count": 4}, {"title": "Accelerating attention through gradient-based learned runtime pruning", "title_link": "https://dl.acm.org/doi/abs/10.1145/3470496.3527423", "id": "lBcEaeWHHUkJ", "cited_by_count": 34}, {"title": "Memory efficient optimizers with 4-bit states", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/3122aaa22b2fe83f9cead1a696f65ceb-Abstract-Conference.html", "id": "_TCnoz-0QP8J", "cited_by_count": 13}, {"title": "SensiMix: Sensitivity-Aware 8-bit index & 1-bit value mixed precision quantization for BERT compression", "title_link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0265621", "id": "jNelrbGxFzIJ", "cited_by_count": 13}, {"title": "Jumping through local minima: Quantization in the loss landscape of vision transformers", "title_link": "http://openaccess.thecvf.com/content/ICCV2023/html/Frumkin_Jumping_through_Local_Minima_Quantization_in_the_Loss_Landscape_of_ICCV_2023_paper.html", "id": "FNb1L-r3EXUJ", "cited_by_count": 14}, {"title": "Low-Precision Quantization Techniques for Hardware-Implementation-Friendly BERT Models", "title_link": "https://ieeexplore.ieee.org/abstract/document/9806238/", "id": "w7QsQHz7wtgJ", "cited_by_count": 4}, {"title": "PEANO-ViT: Power-Efficient Approximations of Non-Linearities in Vision Transformers", "title_link": "https://dl.acm.org/doi/abs/10.1145/3665314.3670843", "id": "aeeQA5jT-wUJ", "cited_by_count": 3}, {"title": "I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models", "title_link": "https://arxiv.org/abs/2405.17849", "id": "ptXne1kMCn4J", "cited_by_count": 1}, {"title": "Sub-8-bit quantization for on-device speech recognition: A regularization-free approach", "title_link": "https://ieeexplore.ieee.org/abstract/document/10022821/", "id": "eCmqCyt4-9IJ", "cited_by_count": 10}, {"title": "Hybrid post-training quantization for super-resolution neural network compression", "title_link": "https://ieeexplore.ieee.org/abstract/document/10093054/", "id": "sYd7aTxa3bEJ", "cited_by_count": 8}, {"title": "Norm tweaking: High-performance low-bit quantization of large language models", "title_link": "https://ojs.aaai.org/index.php/AAAI/article/view/29815", "id": "7K_-DYAGyvgJ", "cited_by_count": 24}, {"title": "Optimal brain compression: A framework for accurate post-training quantization and pruning", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/1caf09c9f4e6b0150b06a07e77f2710c-Abstract-Conference.html", "id": "szf8IM6W6R4J", "cited_by_count": 169}, {"title": "Quantized feature distillation for network quantization", "title_link": "https://ojs.aaai.org/index.php/AAAI/article/view/26354", "id": "qsL3TW820z4J", "cited_by_count": 5}, {"title": "On-device ai: Quantization-aware training of transformers in time-series", "title_link": "https://ieeexplore.ieee.org/abstract/document/10150339/", "id": "0T-sqNxTCT4J", "cited_by_count": 1}, {"title": "Flexround: Learnable rounding based on element-wise division for post-training quantization", "title_link": "https://proceedings.mlr.press/v202/lee23h.html", "id": "LiYzXZ3i0XoJ", "cited_by_count": 21}, {"title": "[PDF][PDF] KIVI: Plug-and-play 2bit KV Cache Quantization with Streaming Asymmetric Quantization", "title_link": "https://www.researchgate.net/profile/Zirui-Liu-29/publication/376831635_KIVI_Plug-and-play_2bit_KV_Cache_Quantization_with_Streaming_Asymmetric_Quantization/links/658b5d282468df72d3db3280/KIVI-Plug-and-play-2bit-KV-Cache-Quantization-with-Streaming-Asymmetric-Quantization.pdf", "id": "4rsNOUn7geEJ", "cited_by_count": 1}, {"title": "SpinQuant--LLM quantization with learned rotations", "title_link": "https://arxiv.org/abs/2405.16406", "id": "LJocxR8Bq7gJ", "cited_by_count": 7}, {"title": "Retraining-free model quantization via one-shot weight-coupling learning", "title_link": "https://openaccess.thecvf.com/content/CVPR2024/html/Tang_Retraining-Free_Model_Quantization_via_One-Shot_Weight-Coupling_Learning_CVPR_2024_paper.html", "id": "VdwiP0d20-8J", "cited_by_count": 4}, {"title": "Quarot: Outlier-free 4-bit inference in rotated llms", "title_link": "https://arxiv.org/abs/2404.00456", "id": "fgdZvS1a_DQJ", "cited_by_count": 29}, {"title": "Effective Interplay between Sparsity and Quantization: From Theory to Practice", "title_link": "https://arxiv.org/abs/2405.20935", "id": "FNmVg4oWtkIJ", "cited_by_count": 3}, {"title": "Understanding neural network binarization with forward and backward proximal quantizers", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/7f70331dbe58ad59d83941dfa7d975aa-Abstract-Conference.html", "id": "-ESX4TbxVwsJ", "cited_by_count": 1}, {"title": "Kivi: A tuning-free asymmetric 2bit quantization for kv cache", "title_link": "https://arxiv.org/abs/2402.02750", "id": "ddvel63pcjMJ", "cited_by_count": 22}, {"title": "Binaryvit: Towards efficient and accurate binary vision transformers", "title_link": "https://ieeexplore.ieee.org/abstract/document/10671591/", "id": "cbKItp-_N3AJ", "cited_by_count": 2}, {"title": "A survey on efficient vision transformers: algorithms, techniques, and performance benchmarking", "title_link": "https://ieeexplore.ieee.org/abstract/document/10508091/", "id": "-AbH6_x4TNgJ", "cited_by_count": 16}, {"title": "The case for 4-bit precision: k-bit inference scaling laws", "title_link": "https://proceedings.mlr.press/v202/dettmers23a", "id": "VhdKmOrSDjQJ", "cited_by_count": 140}, {"title": "TerViT: An efficient ternary vision transformer", "title_link": "https://arxiv.org/abs/2201.08050", "id": "PsbWMJRMcIAJ", "cited_by_count": 6}, {"title": "Bivit: Extremely compressed binary vision transformers", "title_link": "http://openaccess.thecvf.com/content/ICCV2023/html/He_BiViT_Extremely_Compressed_Binary_Vision_Transformers_ICCV_2023_paper.html", "id": "U_vHvlrWqvwJ", "cited_by_count": 19}, {"title": "Squat: Sharpness-and quantization-aware training for bert", "title_link": "https://arxiv.org/abs/2210.07171", "id": "FwJ4vQC0GYYJ", "cited_by_count": 6}, {"title": "EfficientQ: An efficient and accurate post-training neural network quantization method for medical image segmentation", "title_link": "https://www.sciencedirect.com/science/article/pii/S1361841524002020", "id": "3TyA_sEpVl4J", "cited_by_count": 1}, {"title": "Loftq: Lora-fine-tuning-aware quantization for large language models", "title_link": "https://arxiv.org/abs/2310.08659", "id": "3wQ-ey6iLsAJ", "cited_by_count": 87}, {"title": "[PDF][PDF] Hardware-friendly compression and hardware acceleration for transformer: A survey", "title_link": "https://www.aimspress.com/aimspress-data/era/2022/10/PDF/era-30-10-192.pdf", "id": "lkElcTUtwOsJ", "cited_by_count": 4}, {"title": "Qllm: Accurate and efficient low-bitwidth quantization for large language models", "title_link": "https://arxiv.org/abs/2310.08041", "id": "0Dq2wJ8OF9oJ", "cited_by_count": 34}, {"title": "Bitdistiller: Unleashing the potential of sub-4-bit llms via self-distillation", "title_link": "https://arxiv.org/abs/2402.10631", "id": "KPnT_XzTUAwJ", "cited_by_count": 8}, {"title": "Ucvit: Hardware-friendly vision transformer via unified compression", "title_link": "https://ieeexplore.ieee.org/abstract/document/9937660/", "id": "zNXzOCeR9cMJ", "cited_by_count": 4}, {"title": "Patch-wise Mixed-Precision Quantization of Vision Transformer", "title_link": "https://ieeexplore.ieee.org/abstract/document/10191205/", "id": "5kMLKPD1n0QJ", "cited_by_count": 4}, {"title": "Alphatuning: Quantization-aware parameter-efficient adaptation of large-scale pre-trained language models", "title_link": "https://arxiv.org/abs/2210.03858", "id": "9kddTDnX5gkJ", "cited_by_count": 30}, {"title": "FBPT: A Fully Binary Point Transformer", "title_link": "https://arxiv.org/abs/2403.09998", "id": "iitqf5i2GF4J", "cited_by_count": 1}, {"title": "A survey of techniques for optimizing transformer inference", "title_link": "https://www.sciencedirect.com/science/article/pii/S1383762123001698", "id": "eR3vo3-uWcgJ", "cited_by_count": 38}, {"title": "Extreme compression of large language models via additive quantization", "title_link": "https://arxiv.org/abs/2401.06118", "id": "MPm3IDJfl7wJ", "cited_by_count": 30}, {"title": "LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models", "title_link": "https://arxiv.org/abs/2405.06001", "id": "YImBWCczg3kJ", "cited_by_count": 4}, {"title": "Optimal clipping and magnitude-aware differentiation for improved quantization-aware training", "title_link": "https://proceedings.mlr.press/v162/sakr22a.html", "id": "eW3c1fI6tEYJ", "cited_by_count": 35}, {"title": "The era of 1-bit llms: All large language models are in 1.58 bits", "title_link": "https://arxiv.org/abs/2402.17764", "id": "GI2nY7Vp2eoJ", "cited_by_count": 93}, {"title": "Pruning vs quantization: which is better?", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/c48bc80aa5d3cbbdd712d1cc107b8319-Abstract-Conference.html", "id": "VkDaEawlRfwJ", "cited_by_count": 25}, {"title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models", "title_link": "https://arxiv.org/abs/2402.14866", "id": "0p4ffucAkNMJ", "cited_by_count": 4}, {"title": "A frustratingly easy post-training quantization scheme for llms", "title_link": "https://aclanthology.org/2023.emnlp-main.892/", "id": "m0-t5CFyvKsJ", "cited_by_count": 5}, {"title": "Synergistic self-supervised and quantization learning", "title_link": "https://link.springer.com/chapter/10.1007/978-3-031-20056-4_34", "id": "gGfMoSYhXTMJ", "cited_by_count": 11}, {"title": "Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs", "title_link": "https://arxiv.org/abs/2405.14428", "id": "jo7K6u6J92wJ", "cited_by_count": 1}, {"title": "A survey of FPGA and ASIC designs for transformer inference acceleration and optimization", "title_link": "https://www.sciencedirect.com/science/article/pii/S138376212400184X", "id": "OPx-S9Yq3YIJ", "cited_by_count": 1}, {"title": "Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference", "title_link": "https://arxiv.org/abs/2403.05465", "id": "_lapccbnv6gJ", "cited_by_count": 5}, {"title": "Omniquant: Omnidirectionally calibrated quantization for large language models", "title_link": "https://arxiv.org/abs/2308.13137", "id": "nPjMLgOsAfAJ", "cited_by_count": 103}, {"title": "Efficientdm: Efficient quantization-aware fine-tuning of low-bit diffusion models", "title_link": "https://arxiv.org/abs/2310.03270", "id": "PRag3r-x4eEJ", "cited_by_count": 18}, {"title": "QQQ: Quality Quattuor-Bit Quantization for Large Language Models", "title_link": "https://arxiv.org/abs/2406.09904", "id": "6Cjmy7EJ_8AJ", "cited_by_count": 1}, {"title": "Compensate Quantization Errors: Make Weights Hierarchical to Compensate Each Other", "title_link": "https://arxiv.org/abs/2406.16299", "id": "mtXU0lqyVmkJ", "cited_by_count": 1}, {"title": "Modulora: Finetuning 3-bit llms on consumer gpus by integrating with modular quantizers", "title_link": "https://arxiv.org/abs/2309.16119", "id": "-E6JIOBw4nIJ", "cited_by_count": 4}, {"title": "GSB: Group superposition binarization for vision transformer with limited training samples", "title_link": "https://www.sciencedirect.com/science/article/pii/S0893608024000492", "id": "gMDP1nKRaUcJ", "cited_by_count": 2}, {"title": "Accurate neural training with 4-bit matrix multiplications at standard formats", "title_link": "https://openreview.net/forum?id=yTbNYYcopd", "id": "DsTio9bYQs4J", "cited_by_count": 8}, {"title": "Leanquant: Accurate large language model quantization with loss-error-aware grid", "title_link": "https://arxiv.org/abs/2407.10032", "id": "I9tzGMTn07sJ", "cited_by_count": 2}, {"title": "BinaryViT: pushing binary vision transformers towards convolutional models", "title_link": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Le_BinaryViT_Pushing_Binary_Vision_Transformers_Towards_Convolutional_Models_CVPRW_2023_paper.html", "id": "ULxPe1o1EyUJ", "cited_by_count": 16}, {"title": "Affinequant: Affine transformation quantization for large language models", "title_link": "https://arxiv.org/abs/2403.12544", "id": "1OFKAyLW6-8J", "cited_by_count": 10}, {"title": "Optimize weight rounding via signed gradient descent for the quantization of llms", "title_link": "https://arxiv.org/abs/2309.05516", "id": "QwVTFKxHc5UJ", "cited_by_count": 11}, {"title": "QTIP: Quantization with Trellises and Incoherence Processing", "title_link": "https://arxiv.org/abs/2406.11235", "id": "oHpwuQjwJ6MJ", "cited_by_count": 1}, {"title": "BitCluster: Fine-grained weight quantization for load-balanced bit-serial neural network accelerators", "title_link": "https://ieeexplore.ieee.org/abstract/document/9691457/", "id": "_j8zpSgPqX0J", "cited_by_count": 6}, {"title": "Finding the task-optimal low-bit sub-distribution in deep neural networks", "title_link": "https://proceedings.mlr.press/v162/dong22a.html", "id": "XOhlwJD00GQJ", "cited_by_count": 13}, {"title": "IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers", "title_link": "https://arxiv.org/abs/2403.07339", "id": "W8FU9K4RPCIJ", "cited_by_count": 1}, {"title": "LQER: Low-Rank Quantization Error Reconstruction for LLMs", "title_link": "https://arxiv.org/abs/2402.02446", "id": "GqWbB--3GScJ", "cited_by_count": 5}, {"title": "Quantized sparse training: A unified trainable framework for joint pruning and quantization in DNNs", "title_link": "https://dl.acm.org/doi/abs/10.1145/3524066", "id": "bDpY3eKEXBYJ", "cited_by_count": 10}, {"title": "Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?", "title_link": "https://arxiv.org/abs/2310.05079", "id": "v1-mXoHU68IJ", "cited_by_count": 4}, {"title": "LCQ: Low-Rank Codebook based Quantization for Large Language Models", "title_link": "https://arxiv.org/abs/2405.20973", "id": "pRXMyxh77x0J", "cited_by_count": 1}, {"title": "Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models", "title_link": "https://arxiv.org/abs/2206.09557", "id": "zwplhYAy3psJ", "cited_by_count": 92}, {"title": "PB-LLM: Partially Binarized Large Language Models", "title_link": "https://openreview.net/forum?id=BifeBRhikU", "id": "6SwEmqzWKLwJ", "cited_by_count": 1}, {"title": "Mitigating the impact of outlier channels for language model quantization with activation regularization", "title_link": "https://arxiv.org/abs/2404.03605", "id": "kzFAGeZF30gJ", "cited_by_count": 3}, {"title": "[PDF][PDF] QuantEase: Optimization-based Quantization for Language Models-An Efficient and Intuitive Algorithm", "title_link": "https://www.researchgate.net/profile/Kayhan-Behdin/publication/373685425_QuantEase_Optimization-based_Quantization_for_Language_Models_-_An_Efficient_and_Intuitive_Algorithm/links/64f88bdff160f748d6d16c80/QuantEase-Optimization-based-Quantization-for-Language-Models-An-Efficient-and-Intuitive-Algorithm.pdf", "id": "fkDEGQyPiMoJ", "cited_by_count": 9}, {"title": "Revisiting the parameter efficiency of adapters from the perspective of precision redundancy", "title_link": "http://openaccess.thecvf.com/content/ICCV2023/html/Jie_Revisiting_the_Parameter_Efficiency_of_Adapters_from_the_Perspective_of_ICCV_2023_paper.html", "id": "Veq3mAkAmIEJ", "cited_by_count": 21}, {"title": "One-shot model for mixed-precision quantization", "title_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Koryakovskiy_One-Shot_Model_for_Mixed-Precision_Quantization_CVPR_2023_paper.html", "id": "6FkK5D9A8SsJ", "cited_by_count": 13}, {"title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "title_link": "https://arxiv.org/abs/2405.03917", "id": "KemNGS09R_gJ", "cited_by_count": 7}, {"title": "Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models", "title_link": "https://arxiv.org/abs/2406.12311", "id": "tiWugOj1anEJ", "cited_by_count": 1}, {"title": "Token-scaled logit distillation for ternary weight generative language models", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/8342218a4ec08b8c19661725e9cd6c0b-Abstract-Conference.html", "id": "Dl7p0RzIH14J", "cited_by_count": 11}, {"title": "Pb-llm: Partially binarized large language models", "title_link": "https://arxiv.org/abs/2310.00034", "id": "mZnxHQtv7VcJ", "cited_by_count": 28}, {"title": "Q-S5: Towards Quantized State Space Models", "title_link": "https://arxiv.org/abs/2406.09477", "id": "SYT78S70fHQJ", "cited_by_count": 3}, {"title": "Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression", "title_link": "https://arxiv.org/abs/2405.12591", "id": "LuON7nW_FkgJ", "cited_by_count": 1}, {"title": "Unified Scaling-Based Pure-Integer Quantization for Low-Power Accelerator of Complex CNNs", "title_link": "https://www.mdpi.com/2079-9292/12/12/2660", "id": "H7_d2tn7-8MJ", "cited_by_count": 2}, {"title": "TernaryLLM: Ternarized Large Language Model", "title_link": "https://arxiv.org/abs/2406.07177", "id": "EyAfc0nW464J", "cited_by_count": 2}, {"title": "More is Less\u2013Byte-quantized models are faster than bit-quantized models on the edge", "title_link": "https://ieeexplore.ieee.org/abstract/document/10020437/", "id": "_aJGehIJAXEJ", "cited_by_count": 1}, {"title": "Quantized distributed training of large models with convergence guarantees", "title_link": "https://proceedings.mlr.press/v202/markov23a.html", "id": "emhOjlTzW4oJ", "cited_by_count": 9}, {"title": "SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models", "title_link": "https://arxiv.org/abs/2405.14917", "id": "Sa24Yr7tHswJ", "cited_by_count": 3}, {"title": "USM-Lite: Quantization and Sparsity Aware Fine-Tuning for Speech Recognition with Universal Speech Models", "title_link": "https://ieeexplore.ieee.org/abstract/document/10448217/", "id": "cqUpAWMQO8oJ", "cited_by_count": 3}, {"title": "Accurate lora-finetuning quantization of llms via information retention", "title_link": "https://arxiv.org/abs/2402.05445", "id": "n9_1W7Mc-xAJ", "cited_by_count": 23}, {"title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification", "title_link": "https://arxiv.org/abs/2405.14256", "id": "0wKnyPeFiagJ", "cited_by_count": 5}, {"title": "Vitality: Unifying low-rank and sparse approximation for vision transformer acceleration with a linear taylor attention", "title_link": "https://ieeexplore.ieee.org/abstract/document/10071081/", "id": "e-AVqsA405sJ", "cited_by_count": 39}, {"title": "Kvquant: Towards 10 million context length llm inference with kv cache quantization", "title_link": "https://arxiv.org/abs/2401.18079", "id": "hXjqeDdnmXIJ", "cited_by_count": 16}, {"title": "Tender: Accelerating Large Language Models via Tensor Decomposition and Runtime Requantization", "title_link": "https://arxiv.org/abs/2406.12930", "id": "kNNwLR3ICIYJ", "cited_by_count": 3}, {"title": "Progressive Gradient Flow for Robust N: M Sparsity Training in Transformers", "title_link": "https://arxiv.org/abs/2402.04744", "id": "1wQ2EmsqtEUJ", "cited_by_count": 2}, {"title": "SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference", "title_link": "https://ieeexplore.ieee.org/abstract/document/10323725/", "id": "3-aPdgWUz-8J", "cited_by_count": 4}, {"title": "Toward efficient low-precision training: Data format optimization and hysteresis quantization", "title_link": "https://openreview.net/forum?id=3HJOA-1hb0e", "id": "9wYo_jBpimAJ", "cited_by_count": 9}, {"title": "An effective post-training embedding binarization approach for fast online top-k passage matching", "title_link": "https://aclanthology.org/2022.aacl-short.14/", "id": "Mf2AuKtnJykJ", "cited_by_count": 10}, {"title": "Atalanta: A Bit is Worth a \u201cThousand\u201d Tensor Values", "title_link": "https://dl.acm.org/doi/abs/10.1145/3620665.3640356", "id": "VhTPWOTN-gcJ", "cited_by_count": 2}, {"title": "Efficient quantized sparse matrix operations on tensor cores", "title_link": "https://ieeexplore.ieee.org/abstract/document/10046057/", "id": "CUlHKMkgfAUJ", "cited_by_count": 29}, {"title": "Spiking-Transformer Optimization on FPGA for Image Classification and Captioning", "title_link": "https://ieeexplore.ieee.org/abstract/document/10500284/", "id": "aBbfc2TOiJ4J", "cited_by_count": 1}, {"title": "Bibert: Accurate fully binarized bert", "title_link": "https://arxiv.org/abs/2203.06390", "id": "220arbo05FAJ", "cited_by_count": 100}, {"title": "[PDF][PDF] Llm-mq: Mixed-precision quantization for efficient llm deployment", "title_link": "https://www.researchgate.net/profile/Luning-Wang-16/publication/376624532_LLM-MQ_Mixed-precision_Quantization_for_Efficient_LLM_Deployment/links/65818dd43c472d2e8e707515/LLM-MQ-Mixed-precision-Quantization-for-Efficient-LLM-Deployment.pdf", "id": "-1xaj99XrDkJ", "cited_by_count": 8}, {"title": "Quantized Graph Neural Networks for Image Classification", "title_link": "https://www.mdpi.com/2227-7390/11/24/4927", "id": "v05wuA9l_a4J", "cited_by_count": 1}, {"title": "General Purpose Deep Learning Accelerator Based on Bit Interleaving", "title_link": "https://ieeexplore.ieee.org/abstract/document/10359130/", "id": "T7uLSSwrzhAJ", "cited_by_count": 2}, {"title": "Model compression and efficient inference for large language models: A survey", "title_link": "https://arxiv.org/abs/2402.09748", "id": "KdnkYWkKlUAJ", "cited_by_count": 16}, {"title": "Unlocking tokens as data points for generalization bounds on larger language models", "title_link": "https://arxiv.org/abs/2407.18158", "id": "dmNdzD0oOscJ", "cited_by_count": 2}, {"title": "T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge", "title_link": "https://arxiv.org/abs/2407.00088", "id": "WvHbX4NSLFkJ", "cited_by_count": 1}, {"title": "Winning both the accuracy of floating point activation and the simplicity of integer arithmetic", "title_link": "https://openreview.net/forum?id=z92lBy1ehjI", "id": "wvOjhQfD3XcJ", "cited_by_count": 3}, {"title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models", "title_link": "https://arxiv.org/abs/2405.06219", "id": "z2JYVd1WvsUJ", "cited_by_count": 6}, {"title": "Transhash: Transformer-based hamming hashing for efficient image retrieval", "title_link": "https://dl.acm.org/doi/abs/10.1145/3512527.3531405", "id": "NzLBvGCeDrYJ", "cited_by_count": 42}, {"title": "With shared microexponents, a little shifting goes a long way", "title_link": "https://dl.acm.org/doi/abs/10.1145/3579371.3589351", "id": "tB0tY_-g6qQJ", "cited_by_count": 31}, {"title": "Quantized prompt for efficient generalization of vision-language models", "title_link": "https://arxiv.org/abs/2407.10704", "id": "cc5l1AhuD_EJ", "cited_by_count": 1}, {"title": "The Role of Feature Correlation on Quantized Neural Networks", "title_link": "https://ieeexplore.ieee.org/abstract/document/10389686/", "id": "Z7fSh2vrMCYJ", "cited_by_count": 1}, {"title": "u-P: The Unit-Scaled Maximal Update Parametrization", "title_link": "https://arxiv.org/abs/2407.17465", "id": "9JFANOfcAhgJ", "cited_by_count": 2}, {"title": "Boost transformer-based language models with gpu-friendly sparsity and quantization", "title_link": "https://aclanthology.org/2023.findings-acl.15/", "id": "dq1G1fBIhJoJ", "cited_by_count": 7}, {"title": "Fast matrix multiplications for lookup table-quantized llms", "title_link": "https://arxiv.org/abs/2407.10960", "id": "Q1X8zw29RnAJ", "cited_by_count": 1}, {"title": "IVQ: In-memory acceleration of DNN inference exploiting varied quantization", "title_link": "https://ieeexplore.ieee.org/abstract/document/9724255/", "id": "W43ntm9oNvUJ", "cited_by_count": 10}, {"title": "DB-LLM: Accurate dual-binarization for efficient LLMs", "title_link": "https://arxiv.org/abs/2402.11960", "id": "QK-4qIOQh5MJ", "cited_by_count": 13}, {"title": "Compressing Large Language Models using Low Rank and Low Precision Decomposition", "title_link": "https://arxiv.org/abs/2405.18886", "id": "d31XIWaPWM4J", "cited_by_count": 3}, {"title": "Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs", "title_link": "https://arxiv.org/abs/2402.10517", "id": "nOF3gaW2hrsJ", "cited_by_count": 1}, {"title": "A Study of Quantisation-aware Training on Time Series Transformer Models for Resource-constrained FPGAs", "title_link": "https://arxiv.org/abs/2310.02654", "id": "ntBR0CgwZbkJ", "cited_by_count": 1}, {"title": "The Tiny Time-series Transformer: Low-latency High-throughput Classification of Astronomical Transients using Deep Model Compression", "title_link": "https://arxiv.org/abs/2303.08951", "id": "2twOftaLC-IJ", "cited_by_count": 2}, {"title": "An efficient segmented quantization for graph neural networks", "title_link": "https://link.springer.com/article/10.1007/s42514-022-00121-z", "id": "wfNiCS_sOTMJ", "cited_by_count": 2}, {"title": "Generating Efficient Kernels for Quantized Inference on Large Language Models", "title_link": "https://openreview.net/forum?id=jjazoNAf1S", "id": "Iry2GHEjAswJ", "cited_by_count": 1}, {"title": "Q-dm: An efficient low-bit quantized diffusion model", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/f1ee1cca0721de55bb35cf28ab95e1b4-Abstract-Conference.html", "id": "8fMIhKln3hUJ", "cited_by_count": 19}, {"title": "Mixed precision dnn quantization for overlapped speech separation and recognition", "title_link": "https://ieeexplore.ieee.org/abstract/document/9746885/", "id": "yEIi0-pYf2wJ", "cited_by_count": 10}, {"title": "Understanding the potential of fpga-based spatial acceleration for large language model inference", "title_link": "https://dl.acm.org/doi/abs/10.1145/3656177", "id": "p_-Lv1pczqIJ", "cited_by_count": 12}, {"title": "Revisit and Benchmarking of Automated Quantization Towards Fair Comparison", "title_link": "https://ieeexplore.ieee.org/abstract/document/10252023/", "id": "6xUaCz6M4yMJ", "cited_by_count": 0}, {"title": "STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs", "title_link": "https://arxiv.org/abs/2408.01803", "id": "I-GrDevJ2WMJ", "cited_by_count": 0}, {"title": "IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers", "title_link": "https://arxiv.org/abs/2403.07339", "id": "W8FU9K4RPCIJ", "cited_by_count": 1}, {"title": "LQER: Low-Rank Quantization Error Reconstruction for LLMs", "title_link": "https://arxiv.org/abs/2402.02446", "id": "GqWbB--3GScJ", "cited_by_count": 5}, {"title": "Quantized sparse training: A unified trainable framework for joint pruning and quantization in DNNs", "title_link": "https://dl.acm.org/doi/abs/10.1145/3524066", "id": "bDpY3eKEXBYJ", "cited_by_count": 10}, {"title": "Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?", "title_link": "https://arxiv.org/abs/2310.05079", "id": "v1-mXoHU68IJ", "cited_by_count": 4}, {"title": "LCQ: Low-Rank Codebook based Quantization for Large Language Models", "title_link": "https://arxiv.org/abs/2405.20973", "id": "pRXMyxh77x0J", "cited_by_count": 1}, {"title": "AdaNF: Quantization Group Adaptive NormalFloat for Low Bit Fine-tuning of LLMs", "title_link": "https://openreview.net/forum?id=nH7k4BkmcX", "id": "-uIOtpNyJE0J", "cited_by_count": 0}, {"title": "FlexRound: Learnable Rounding by Element-wise Division for Post-Training Quantization", "title_link": "https://openreview.net/forum?id=-tYCaP0phY_", "id": "FfcFToOW764J", "cited_by_count": 0}, {"title": "Differentiable Soft Min-Max Loss to Restrict Weight Range for Model Quantization", "title_link": "https://openreview.net/forum?id=ytPyPIypM6", "id": "3awie4MBYIAJ", "cited_by_count": 0}, {"title": "Forget and Rewire: Enhancing the Resilience of Transformer-based Models against {Bit-Flip} Attacks", "title_link": "https://www.usenix.org/conference/usenixsecurity24/presentation/nazari", "id": "xVAM9PVaF1kJ", "cited_by_count": 0}, {"title": "Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models", "title_link": "https://arxiv.org/abs/2206.09557", "id": "zwplhYAy3psJ", "cited_by_count": 96}, {"title": "PB-LLM: Partially Binarized Large Language Models", "title_link": "https://openreview.net/forum?id=BifeBRhikU", "id": "6SwEmqzWKLwJ", "cited_by_count": 1}, {"title": "Mitigating the impact of outlier channels for language model quantization with activation regularization", "title_link": "https://arxiv.org/abs/2404.03605", "id": "kzFAGeZF30gJ", "cited_by_count": 3}, {"title": "[PDF][PDF] QuantEase: Optimization-based Quantization for Language Models-An Efficient and Intuitive Algorithm", "title_link": "https://www.researchgate.net/profile/Kayhan-Behdin/publication/373685425_QuantEase_Optimization-based_Quantization_for_Language_Models_-_An_Efficient_and_Intuitive_Algorithm/links/64f88bdff160f748d6d16c80/QuantEase-Optimization-based-Quantization-for-Language-Models-An-Efficient-and-Intuitive-Algorithm.pdf", "id": "fkDEGQyPiMoJ", "cited_by_count": 9}, {"title": "Low-Bitwidth Floating Point Quantization for Efficient High-Quality Diffusion Models", "title_link": "https://arxiv.org/abs/2408.06995", "id": "nhtWSXtfxYwJ", "cited_by_count": 0}, {"title": "Revisiting the parameter efficiency of adapters from the perspective of precision redundancy", "title_link": "http://openaccess.thecvf.com/content/ICCV2023/html/Jie_Revisiting_the_Parameter_Efficiency_of_Adapters_from_the_Perspective_of_ICCV_2023_paper.html", "id": "Veq3mAkAmIEJ", "cited_by_count": 22}, {"title": "BitNet B1. 58 Reloaded: State-of-the-Art Performance Also on Smaller Networks", "title_link": "https://link.springer.com/chapter/10.1007/978-3-031-66705-3_20", "id": "BsMWBLoru-wJ", "cited_by_count": 0}, {"title": "One-shot model for mixed-precision quantization", "title_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Koryakovskiy_One-Shot_Model_for_Mixed-Precision_Quantization_CVPR_2023_paper.html", "id": "6FkK5D9A8SsJ", "cited_by_count": 13}, {"title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", "title_link": "https://arxiv.org/abs/2405.03917", "id": "KemNGS09R_gJ", "cited_by_count": 9}, {"title": "ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models", "title_link": "https://arxiv.org/abs/2408.08554", "id": "DUBj_it78BIJ", "cited_by_count": 0}, {"title": "BinaryFormer: A Hierarchical-Adaptive Binary Vision Transformer (ViT) for Efficient Computing", "title_link": "https://ieeexplore.ieee.org/abstract/document/10531134/", "id": "pxpDrw6f7QEJ", "cited_by_count": 0}, {"title": "Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models", "title_link": "https://arxiv.org/abs/2406.12311", "id": "tiWugOj1anEJ", "cited_by_count": 1}, {"title": "Token-scaled logit distillation for ternary weight generative language models", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/8342218a4ec08b8c19661725e9cd6c0b-Abstract-Conference.html", "id": "Dl7p0RzIH14J", "cited_by_count": 11}, {"title": "Pb-llm: Partially binarized large language models", "title_link": "https://arxiv.org/abs/2310.00034", "id": "mZnxHQtv7VcJ", "cited_by_count": 29}, {"title": "MICSim: A Modular Simulator for Mixed-signal Compute-in-Memory based AI Accelerator", "title_link": "https://arxiv.org/abs/2409.14838", "id": "YTH8jx3iTOAJ", "cited_by_count": 0}, {"title": "Q-S5: Towards Quantized State Space Models", "title_link": "https://arxiv.org/abs/2406.09477", "id": "SYT78S70fHQJ", "cited_by_count": 3}, {"title": "BiViT: Exploring Binary Vision Transformers", "title_link": "https://openreview.net/forum?id=lXBzOtKn20t", "id": "oDq8d1j3RygJ", "cited_by_count": 0}, {"title": "Fisher-aware Quantization for DETR Detectors with Critical-category Objectives", "title_link": "https://arxiv.org/abs/2407.03442", "id": "5zA5k8TYbMkJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Accelerated Segmentation with Mixed-Precision Quantization of EfficientViT-SAM", "title_link": "https://lup.lub.lu.se/luur/download?func=downloadFile&recordOId=9174462&fileOId=9174463", "id": "lo3IewWTW8sJ", "cited_by_count": 0}, {"title": "Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression", "title_link": "https://arxiv.org/abs/2405.12591", "id": "LuON7nW_FkgJ", "cited_by_count": 2}, {"title": "Compressing large neural networks: Algorithms, systems and scaling laws", "title_link": "https://research-explorer.ista.ac.at/record/17485", "id": "so3tKkmM_kkJ", "cited_by_count": 0}, {"title": "Unified Scaling-Based Pure-Integer Quantization for Low-Power Accelerator of Complex CNNs", "title_link": "https://www.mdpi.com/2079-9292/12/12/2660", "id": "H7_d2tn7-8MJ", "cited_by_count": 2}, {"title": "TernaryLLM: Ternarized Large Language Model", "title_link": "https://arxiv.org/abs/2406.07177", "id": "EyAfc0nW464J", "cited_by_count": 2}, {"title": "More is Less\u2013Byte-quantized models are faster than bit-quantized models on the edge", "title_link": "https://ieeexplore.ieee.org/abstract/document/10020437/", "id": "_aJGehIJAXEJ", "cited_by_count": 1}, {"title": "Pyramid Vector Quantization for LLMs", "title_link": "https://arxiv.org/abs/2410.16926", "id": "bnIutiywAEgJ", "cited_by_count": 0}, {"title": "TerDiT: Ternary Diffusion Models with Transformers", "title_link": "https://arxiv.org/abs/2405.14854", "id": "aP8Lyo95OwkJ", "cited_by_count": 0}, {"title": "Channel-Wise Mixed-Precision Quantization for Large Language Models", "title_link": "https://arxiv.org/abs/2410.13056", "id": "v7dTTvVrVCwJ", "cited_by_count": 0}, {"title": "SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models", "title_link": "https://arxiv.org/abs/2405.14917", "id": "Sa24Yr7tHswJ", "cited_by_count": 3}, {"title": "USM-Lite: Quantization and Sparsity Aware Fine-Tuning for Speech Recognition with Universal Speech Models", "title_link": "https://ieeexplore.ieee.org/abstract/document/10448217/", "id": "cqUpAWMQO8oJ", "cited_by_count": 3}, {"title": "Accurate lora-finetuning quantization of llms via information retention", "title_link": "https://arxiv.org/abs/2402.05445", "id": "n9_1W7Mc-xAJ", "cited_by_count": 25}, {"title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification", "title_link": "https://arxiv.org/abs/2405.14256", "id": "0wKnyPeFiagJ", "cited_by_count": 6}, {"title": "Searching Optimal Floating-Point Format for Sub-8-Bit Large Language Model Inference", "title_link": "https://ieeexplore.ieee.org/abstract/document/10457111/", "id": "x9H4G_cr8PUJ", "cited_by_count": 0}, {"title": "Vitality: Unifying low-rank and sparse approximation for vision transformer acceleration with a linear taylor attention", "title_link": "https://ieeexplore.ieee.org/abstract/document/10071081/", "id": "e-AVqsA405sJ", "cited_by_count": 39}, {"title": "Kvquant: Towards 10 million context length llm inference with kv cache quantization", "title_link": "https://arxiv.org/abs/2401.18079", "id": "hXjqeDdnmXIJ", "cited_by_count": 16}, {"title": "Tender: Accelerating Large Language Models via Tensor Decomposition and Runtime Requantization", "title_link": "https://arxiv.org/abs/2406.12930", "id": "kNNwLR3ICIYJ", "cited_by_count": 3}, {"title": "Progressive Gradient Flow for Robust N: M Sparsity Training in Transformers", "title_link": "https://arxiv.org/abs/2402.04744", "id": "1wQ2EmsqtEUJ", "cited_by_count": 3}, {"title": "INT-FlashAttention: Enabling Flash Attention for INT8 Quantization", "title_link": "https://arxiv.org/abs/2409.16997", "id": "VzYlG6OoG5YJ", "cited_by_count": 0}, {"title": "SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference", "title_link": "https://ieeexplore.ieee.org/abstract/document/10323725/", "id": "3-aPdgWUz-8J", "cited_by_count": 4}, {"title": "Toward efficient low-precision training: Data format optimization and hysteresis quantization", "title_link": "https://openreview.net/forum?id=3HJOA-1hb0e", "id": "9wYo_jBpimAJ", "cited_by_count": 9}, {"title": "[PDF][PDF] CodeGPT on XTC", "title_link": "https://repository.tudelft.nl/file/File_47facba4-c909-47e7-bf8a-52cafa7f74ac", "id": "M4K6ZUd4tWkJ", "cited_by_count": 0}, {"title": "Trainable pruned ternary quantization for medical signal classification models", "title_link": "https://www.sciencedirect.com/science/article/pii/S0925231224009871", "id": "L74dymtEyksJ", "cited_by_count": 0}, {"title": "Hardware-Friendly Post-Training Quantization: Input-and Output-Channelwise Scale and Offset", "title_link": "https://openreview.net/forum?id=itJj6p7ssr", "id": "PVosMXZECeoJ", "cited_by_count": 0}, {"title": "An effective post-training embedding binarization approach for fast online top-k passage matching", "title_link": "https://aclanthology.org/2022.aacl-short.14/", "id": "Mf2AuKtnJykJ", "cited_by_count": 10}, {"title": "Fast and Energy-Efficient Inference for Attention-Based Natural Language Processing Models", "title_link": "https://tspace.library.utoronto.ca/handle/1807/128003", "id": "NAlfU77HwuMJ", "cited_by_count": 0}, {"title": "[PDF][PDF] POCA: Post-training Quantization with Temporal Alignment for Codec Avatars", "title_link": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05670.pdf", "id": "UWcliiw94jIJ", "cited_by_count": 0}, {"title": "Atalanta: A Bit is Worth a \u201cThousand\u201d Tensor Values", "title_link": "https://dl.acm.org/doi/abs/10.1145/3620665.3640356", "id": "VhTPWOTN-gcJ", "cited_by_count": 2}, {"title": "Efficient quantized sparse matrix operations on tensor cores", "title_link": "https://ieeexplore.ieee.org/abstract/document/10046057/", "id": "CUlHKMkgfAUJ", "cited_by_count": 29}, {"title": "Spiking-Transformer Optimization on FPGA for Image Classification and Captioning", "title_link": "https://ieeexplore.ieee.org/abstract/document/10500284/", "id": "aBbfc2TOiJ4J", "cited_by_count": 1}, {"title": "Resource-Efficient Speech Quality Prediction through Quantization Aware Training and Binary Activation Maps", "title_link": "https://arxiv.org/abs/2407.04578", "id": "SdbZN5zP2oUJ", "cited_by_count": 0}, {"title": "Tapered-Precision Numerical Formats for Deep Learning Inference and Training", "title_link": "https://repository.rit.edu/theses/11621/", "id": "iSQOv3SLta4J", "cited_by_count": 0}, {"title": "Bibert: Accurate fully binarized bert", "title_link": "https://arxiv.org/abs/2203.06390", "id": "220arbo05FAJ", "cited_by_count": 102}, {"title": "Robust and Efficient Quantization-aware Training via Coreset Selection", "title_link": "https://openreview.net/forum?id=4c2pZzG94y", "id": "aV6gW0y5_bEJ", "cited_by_count": 0}, {"title": "Quantization and adversarial robustness of embedded deep neural networks", "title_link": "https://theses.hal.science/tel-04136202/", "id": "ERtRUdrxZlcJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Llm-mq: Mixed-precision quantization for efficient llm deployment", "title_link": "https://www.researchgate.net/profile/Luning-Wang-16/publication/376624532_LLM-MQ_Mixed-precision_Quantization_for_Efficient_LLM_Deployment/links/65818dd43c472d2e8e707515/LLM-MQ-Mixed-precision-Quantization-for-Efficient-LLM-Deployment.pdf", "id": "-1xaj99XrDkJ", "cited_by_count": 8}, {"title": "Quantized Graph Neural Networks for Image Classification", "title_link": "https://www.mdpi.com/2227-7390/11/24/4927", "id": "v05wuA9l_a4J", "cited_by_count": 1}, {"title": "BERT model optimization methods for inference: a comparative study of five alternative BERT-model implementations", "title_link": "https://lutpub.lut.fi/handle/10024/165034", "id": "LawjeET3gJ8J", "cited_by_count": 0}, {"title": "BiSup: Bidirectional Quantization Error Suppression for Large Language Models", "title_link": "https://arxiv.org/abs/2405.15346", "id": "4ZD8EwVbMysJ", "cited_by_count": 0}, {"title": "General Purpose Deep Learning Accelerator Based on Bit Interleaving", "title_link": "https://ieeexplore.ieee.org/abstract/document/10359130/", "id": "T7uLSSwrzhAJ", "cited_by_count": 3}, {"title": "Foundations of Large Language Model Compression--Part 1: Weight Quantization", "title_link": "https://arxiv.org/abs/2409.02026", "id": "9xo8uMUZym4J", "cited_by_count": 0}, {"title": "Model compression and efficient inference for large language models: A survey", "title_link": "https://arxiv.org/abs/2402.09748", "id": "KdnkYWkKlUAJ", "cited_by_count": 16}, {"title": "Evaluating Quantized Large Language Models for Code Generation on Low-Resource Language Benchmarks", "title_link": "https://arxiv.org/abs/2410.14766", "id": "Bc2yRufxiI4J", "cited_by_count": 0}, {"title": "T-mac: Cpu renaissance via table lookup for low-bit llm deployment on edge", "title_link": "https://arxiv.org/abs/2407.00088", "id": "WvHbX4NSLFkJ", "cited_by_count": 2}, {"title": "Unlocking tokens as data points for generalization bounds on larger language models", "title_link": "https://arxiv.org/abs/2407.18158", "id": "dmNdzD0oOscJ", "cited_by_count": 2}, {"title": "Winning both the accuracy of floating point activation and the simplicity of integer arithmetic", "title_link": "https://openreview.net/forum?id=z92lBy1ehjI", "id": "wvOjhQfD3XcJ", "cited_by_count": 3}, {"title": "DUAROT: DUAL ROTATION FOR ADVANCED OUT-LIER MITIGATION IN ROTATED LLMS", "title_link": "https://openreview.net/forum?id=oHBS7R6JcP", "id": "aNjhRSpfTvcJ", "cited_by_count": 0}, {"title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models", "title_link": "https://arxiv.org/abs/2405.06219", "id": "z2JYVd1WvsUJ", "cited_by_count": 6}, {"title": "CL-Calib: Enhancing Post-training Quantization Calibration through Contrastive Learning", "title_link": "https://openreview.net/forum?id=Wxyyc2vvGd", "id": "uf4SYNJsRT8J", "cited_by_count": 0}, {"title": "Transhash: Transformer-based hamming hashing for efficient image retrieval", "title_link": "https://dl.acm.org/doi/abs/10.1145/3512527.3531405", "id": "NzLBvGCeDrYJ", "cited_by_count": 42}, {"title": "With shared microexponents, a little shifting goes a long way", "title_link": "https://dl.acm.org/doi/abs/10.1145/3579371.3589351", "id": "tB0tY_-g6qQJ", "cited_by_count": 31}, {"title": "Quantized prompt for efficient generalization of vision-language models", "title_link": "https://arxiv.org/abs/2407.10704", "id": "cc5l1AhuD_EJ", "cited_by_count": 1}, {"title": "AutoMPQ: Automatic Mixed-Precision Neural Network Search via Few-Shot Quantization Adapter", "title_link": "https://ieeexplore.ieee.org/abstract/document/10523945/", "id": "haKPRtTpQ1EJ", "cited_by_count": 0}, {"title": "ABS: Accumulation Bit-Width Scaling Method for Designing Low-Precision Tensor Core", "title_link": "https://ieeexplore.ieee.org/abstract/document/10571370/", "id": "ctISmzXYr6oJ", "cited_by_count": 0}, {"title": "ARB-LLM: Alternating Refined Binarizations for Large Language Models", "title_link": "https://arxiv.org/abs/2410.03129", "id": "6e7dBeWYk8oJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Distributional Quantization of Large Language Models", "title_link": "https://www.cee.org/sites/default/files/rsi/Papers/Cholakov_Radostin.pdf", "id": "cmVXGXhkNhEJ", "cited_by_count": 0}, {"title": "The Role of Feature Correlation on Quantized Neural Networks", "title_link": "https://ieeexplore.ieee.org/abstract/document/10389686/", "id": "Z7fSh2vrMCYJ", "cited_by_count": 1}, {"title": "QuantEase: Optimization-based Quantization for Large Language Models", "title_link": "https://openreview.net/forum?id=I07KLz6Em1", "id": "HPFU8CLJdZ0J", "cited_by_count": 0}, {"title": "u-P: The Unit-Scaled Maximal Update Parametrization", "title_link": "https://arxiv.org/abs/2407.17465", "id": "9JFANOfcAhgJ", "cited_by_count": 2}, {"title": "EXAQ: Exponent Aware Quantization For LLMs Acceleration", "title_link": "https://arxiv.org/abs/2410.03185", "id": "ITBKKpSLUyAJ", "cited_by_count": 0}, {"title": "Boost transformer-based language models with gpu-friendly sparsity and quantization", "title_link": "https://aclanthology.org/2023.findings-acl.15/", "id": "dq1G1fBIhJoJ", "cited_by_count": 7}, {"title": "[BUCH][B] Accelerating Attention Models on Hardware", "title_link": "https://escholarship.org/content/qt6d62c22g/qt6d62c22g.pdf", "id": "WtwirfqJrxUJ", "cited_by_count": 0}, {"title": "Accelerating Large Scale Generative AI: A Comprehensive Study", "title_link": "https://search.proquest.com/openview/64bf5127bcbaf0db17974c1b475c9234/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "DHHNShUFL3kJ", "cited_by_count": 0}, {"title": "Fast matrix multiplications for lookup table-quantized llms", "title_link": "https://arxiv.org/abs/2407.10960", "id": "Q1X8zw29RnAJ", "cited_by_count": 1}, {"title": "Uncovering the Hidden Cost of Model Compression", "title_link": "https://openaccess.thecvf.com/content/CVPR2024W/PV/html/Misra_Uncovering_the_Hidden_Cost_of_Model_Compression_CVPRW_2024_paper.html", "id": "5TYlj3t3y0wJ", "cited_by_count": 0}, {"title": "ELSA: Exploiting Layer-wise N: M Sparsity for Vision Transformer Acceleration", "title_link": "https://openaccess.thecvf.com/content/CVPR2024W/ECV24/html/Huang_ELSA_Exploiting_Layer-wise_NM_Sparsity_for_Vision_Transformer_Acceleration_CVPRW_2024_paper.html", "id": "vIoHTRksySMJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Towards Fair and Efficient Distributed Intelligence", "title_link": "https://api.mountainscholar.org/server/api/core/bitstreams/89ba41b0-b714-4214-8133-60e7868827dd/content", "id": "DgHM2GbAHa4J", "cited_by_count": 0}, {"title": "IVQ: In-memory acceleration of DNN inference exploiting varied quantization", "title_link": "https://ieeexplore.ieee.org/abstract/document/9724255/", "id": "W43ntm9oNvUJ", "cited_by_count": 10}, {"title": "DB-LLM: Accurate dual-binarization for efficient LLMs", "title_link": "https://arxiv.org/abs/2402.11960", "id": "QK-4qIOQh5MJ", "cited_by_count": 13}, {"title": "LoQT: Low Rank Adapters for Quantized Training", "title_link": "https://arxiv.org/abs/2405.16528", "id": "D14vvgce__gJ", "cited_by_count": 0}, {"title": "Compressing Large Language Models using Low Rank and Low Precision Decomposition", "title_link": "https://arxiv.org/abs/2405.18886", "id": "d31XIWaPWM4J", "cited_by_count": 3}, {"title": "Torch2Chip: An End-to-end Customizable Deep Neural Network Compression and Deployment Toolkit for Prototype Hardware Accelerator Design", "title_link": "https://proceedings.mlsys.org/paper_files/paper/2024/hash/b8bf2c0dd0b48511889b7d3b2c5fc8f5-Abstract-Conference.html", "id": "4NmTFihevSAJ", "cited_by_count": 0}, {"title": "On-Chip Learning via Transformer In-Context Learning", "title_link": "https://arxiv.org/abs/2410.08711", "id": "7TWm2JfKFicJ", "cited_by_count": 0}, {"title": "OutEffHop: A Principled Outlier-Efficient Attention Layer from Dense Associative Memory Models", "title_link": "https://openreview.net/forum?id=ZCrRCICOkr", "id": "LfdM8DZFLkMJ", "cited_by_count": 0}, {"title": "Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs", "title_link": "https://arxiv.org/abs/2402.10517", "id": "nOF3gaW2hrsJ", "cited_by_count": 4}, {"title": "Bedot: Bit Efficient Dot Product for Deep Generative Models", "title_link": "https://link.springer.com/chapter/10.1007/978-3-031-32180-1_2", "id": "_KJfj4jIzKEJ", "cited_by_count": 0}, {"title": "A Study of Quantisation-aware Training on Time Series Transformer Models for Resource-constrained FPGAs", "title_link": "https://arxiv.org/abs/2310.02654", "id": "ntBR0CgwZbkJ", "cited_by_count": 1}, {"title": "The Tiny Time-series Transformer: Low-latency High-throughput Classification of Astronomical Transients using Deep Model Compression", "title_link": "https://arxiv.org/abs/2303.08951", "id": "2twOftaLC-IJ", "cited_by_count": 2}, {"title": "Efficient signal acquisition and deep learning model compression", "title_link": "https://repositories.lib.utexas.edu/items/d274a838-8cf7-4868-bbbc-9c9036741a41", "id": "z9zCTLV_G5AJ", "cited_by_count": 0}, {"title": "An efficient segmented quantization for graph neural networks", "title_link": "https://link.springer.com/article/10.1007/s42514-022-00121-z", "id": "wfNiCS_sOTMJ", "cited_by_count": 2}, {"title": "OPAL: Outlier-Preserved Microscaling Quantization A ccelerator for Generative Large Language Models", "title_link": "https://arxiv.org/abs/2409.05902", "id": "_mX5Pys2GOMJ", "cited_by_count": 0}, {"title": "Generating Efficient Kernels for Quantized Inference on Large Language Models", "title_link": "https://openreview.net/forum?id=jjazoNAf1S", "id": "Iry2GHEjAswJ", "cited_by_count": 1}, {"title": "QERA: an Analytical Framework for Quantization Error Reconstruction", "title_link": "https://arxiv.org/abs/2410.06040", "id": "yYM2GxIPCtgJ", "cited_by_count": 0}, {"title": "Compensate Quantization Errors+: Quantized Models Are Inquisitive Learners", "title_link": "https://arxiv.org/abs/2407.15508", "id": "RgmbSA4U-q4J", "cited_by_count": 0}, {"title": "Q-dm: An efficient low-bit quantized diffusion model", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/f1ee1cca0721de55bb35cf28ab95e1b4-Abstract-Conference.html", "id": "8fMIhKln3hUJ", "cited_by_count": 19}, {"title": "Mixed precision dnn quantization for overlapped speech separation and recognition", "title_link": "https://ieeexplore.ieee.org/abstract/document/9746885/", "id": "yEIi0-pYf2wJ", "cited_by_count": 10}, {"title": "[PDF][PDF] BFP-CIM: Runtime Energy-Accuracy Scalable Computing-in-Memory-Based DNN Accelerator Using Dynamic Block-Floating-Point Arithmetic", "title_link": "https://access.ee.ntu.edu.tw/files/journals/(2024)BFP-CIM_Runtime_Energy-Accuracy_Scalable_Computing-in-Memory-Based_DNN_Accelerator_Using_Dynamic_Block-Floating-Point_Arithmetic.pdf", "id": "JJQISiO3o6sJ", "cited_by_count": 0}, {"title": "Low Resolution Neural Networks", "title_link": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4975898", "id": "Ei8fq8iTcgsJ", "cited_by_count": 0}, {"title": "QT-DoG: Quantization-aware Training for Domain Generalization", "title_link": "https://arxiv.org/abs/2410.06020", "id": "dRDEHE4EPb0J", "cited_by_count": 0}, {"title": "Rotation Invariant Quantization for Model Compression", "title_link": "https://arxiv.org/abs/2303.03106", "id": "3G2bVvm_gYYJ", "cited_by_count": 0}, {"title": "Adaptive Global Power-of-Two Ternary Quantization Algorithm Based on Unfixed Boundary Thresholds", "title_link": "https://www.mdpi.com/1424-8220/24/1/181", "id": "diyqAnCIFJcJ", "cited_by_count": 0}, {"title": "Optimizing neural networks for TinyML: a study on quantization schemes", "title_link": "https://www.politesi.polimi.it/handle/10589/222539", "id": "zwmm1VRe0x8J", "cited_by_count": 0}, {"title": "Adaptive Quantization Error Reconstruction for LLMs with Mixed Precision", "title_link": "https://openreview.net/forum?id=T5pGDydMkS", "id": "HEsuwGHUhS0J", "cited_by_count": 0}, {"title": "DECOUPLE QUANTIZATION STEP AND OUTLIER-MIGRATED RECONSTRUCTION FOR PTQ", "title_link": "https://openreview.net/forum?id=ZtlcdjE1K3", "id": "eoVnvb_O1gAJ", "cited_by_count": 0}, {"title": "Comparative Analysis of Quantization Frameworks: A Deep Learning Perspective: What are Quantization Frameworks in Deep Learning, their Techniques\u00a0\u2026", "title_link": "https://www.diva-portal.org/smash/record.jsf?pid=diva2:1887665", "id": "1HBQ6unKpGQJ", "cited_by_count": 0}, {"title": "Edge-MPQ: Layer-Wise Mixed-Precision Quantization with Tightly Integrated Versatile Inference Units for Edge Computing", "title_link": "https://ieeexplore.ieee.org/abstract/document/10633877/", "id": "FgwbEnMQf8MJ", "cited_by_count": 0}, {"title": "Understanding the potential of fpga-based spatial acceleration for large language model inference", "title_link": "https://dl.acm.org/doi/abs/10.1145/3656177", "id": "p_-Lv1pczqIJ", "cited_by_count": 13}, {"title": "Frame Quantization of Neural Networks", "title_link": "https://arxiv.org/abs/2404.08131", "id": "sGQ3Ucp7DMwJ", "cited_by_count": 0}, {"title": "Investigating the Impact of Quantization on Adversarial Robustness", "title_link": "https://arxiv.org/abs/2404.05639", "id": "E5beiT5-b1YJ", "cited_by_count": 0}, {"title": "Keep the Cost Down: A Review on Methods to Optimize LLM's KV-Cache Consumption", "title_link": "https://arxiv.org/abs/2407.18003", "id": "M3bey3meVIMJ", "cited_by_count": 1}, {"title": "Push Quantization-Aware Training Toward Full Precision Performances via Consistency Regularization", "title_link": "https://arxiv.org/abs/2402.13497", "id": "R0cHqXIVkwsJ", "cited_by_count": 0}, {"title": "big. LITTLE Vision Transformer for Efficient Visual Recognition", "title_link": "https://arxiv.org/abs/2410.10267", "id": "_NNoO4b-g8EJ", "cited_by_count": 0}, {"title": "QCore: Data-efficient, on-device continual calibration for quantized models", "title_link": "https://dl.acm.org/doi/abs/10.14778/3681954.3681957", "id": "KRIxdojFX4oJ", "cited_by_count": 1}, {"title": "ReALLM: A general framework for LLM compression and fine-tuning", "title_link": "https://arxiv.org/abs/2405.13155", "id": "RcsS8PQNqYAJ", "cited_by_count": 0}, {"title": "Certified quantization strategy synthesis for neural networks", "title_link": "https://link.springer.com/chapter/10.1007/978-3-031-71162-6_18", "id": "yRaEFhq0X48J", "cited_by_count": 0}, {"title": "Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective", "title_link": "https://arxiv.org/abs/2410.04466", "id": "QzZvQwY6MrsJ", "cited_by_count": 0}, {"title": "Communication-efficient distributed training of deep neural networks: An algorithms and systems perspective", "title_link": "https://research-explorer.ista.ac.at/record/17490", "id": "3fPrlCN-XfEJ", "cited_by_count": 0}, {"title": "LUTein: Dense-Sparse Bit-Slice Architecture With Radix-4 LUT-Based Slice-Tensor Processing Units", "title_link": "https://ieeexplore.ieee.org/abstract/document/10476468/", "id": "SkaxZxUmkX0J", "cited_by_count": 1}, {"title": "Preliminary: Theories and Algorithms", "title_link": "https://www.taylorfrancis.com/chapters/edit/10.1201/9781003340225-3/preliminary-theories-algorithms-qihua-zhou-peiran-dong", "id": "TwVZMuzVjFMJ", "cited_by_count": 0}, {"title": "MELTing point: Mobile Evaluation of Language Transformers", "title_link": "https://arxiv.org/abs/2403.12844", "id": "zv8OF_NqzXsJ", "cited_by_count": 10}, {"title": "Post-training quantization with low-precision minifloats and integers on FPGAs", "title_link": "https://arxiv.org/abs/2311.12359", "id": "pS_NiFpkx2IJ", "cited_by_count": 2}, {"title": "VitBit: Enhancing Embedded GPU Performance for AI Workloads through Register Operand Packing", "title_link": "https://dl.acm.org/doi/abs/10.1145/3673038.3673045", "id": "0gfu2z4_FfoJ", "cited_by_count": 0}, {"title": "Data-free quantization via mixed-precision compensation without fine-tuning", "title_link": "https://www.sciencedirect.com/science/article/pii/S0031320323004788", "id": "g_0eg-E1-iwJ", "cited_by_count": 15}, {"title": "Edge inference with fully differentiable quantized mixed precision neural networks", "title_link": "https://openaccess.thecvf.com/content/WACV2024/html/Schaefer_Edge_Inference_With_Fully_Differentiable_Quantized_Mixed_Precision_Neural_Networks_WACV_2024_paper.html", "id": "_fihLTuBInAJ", "cited_by_count": 8}, {"title": "HLSTransform: Energy-Efficient Llama 2 Inference on FPGAs Via High Level Synthesis", "title_link": "https://arxiv.org/abs/2405.00738", "id": "kYziL9_7D6gJ", "cited_by_count": 2}, {"title": "Winner-take-all column row sampling for memory efficient adaptation of language model", "title_link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/0a6059857ae5c82ea9726ee9282a7145-Abstract-Conference.html", "id": "qtd4LmR4r3oJ", "cited_by_count": 14}, {"title": "BFP-CIM: Runtime Energy-Accuracy Scalable Computing-in-Memory-Based DNN Accelerator Using Dynamic Block-Floating-Point Arithmetic", "title_link": "https://ieeexplore.ieee.org/abstract/document/10348028/", "id": "Lp5kNB3JzZ0J", "cited_by_count": 0}, {"title": "LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization", "title_link": "https://arxiv.org/abs/2403.01136", "id": "ta-tzvrLbhIJ", "cited_by_count": 6}, {"title": "A comprehensive survey of compression algorithms for language models", "title_link": "https://arxiv.org/abs/2401.15347", "id": "PaxiKGAAjb0J", "cited_by_count": 9}, {"title": "ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters", "title_link": "https://arxiv.org/abs/2402.10930", "id": "yomC4WK5a2wJ", "cited_by_count": 1}, {"title": "HTQ: Exploring the High-Dimensional Trade-Off of mixed-precision quantization", "title_link": "https://www.sciencedirect.com/science/article/pii/S0031320324005399", "id": "2FCHQX3j3GUJ", "cited_by_count": 1}, {"title": "2-bit conformer quantization for automatic speech recognition", "title_link": "https://arxiv.org/abs/2305.16619", "id": "oO_V9qewNEMJ", "cited_by_count": 8}, {"title": "Accelerator-aware training for transducer-based speech recognition", "title_link": "https://ieeexplore.ieee.org/abstract/document/10022592/", "id": "ORVW1Rwpj-sJ", "cited_by_count": 1}, {"title": "[PDF][PDF] Techniques and Optimization Strategies for Efficient Hardware Acceleration of Neural Networks: Tap-Wisely-Quantized Winograd Algorithm and Capsule\u00a0\u2026", "title_link": "https://tesidottorato.depositolegale.it/bitstream/20.500.14242/69724/1/conv_latex_thesis.pdf", "id": "OTiHXujVBzoJ", "cited_by_count": 0}, {"title": "QCore: Data-Efficient, On-Device Continual Calibration for Quantized Models--Extended Version", "title_link": "https://arxiv.org/abs/2404.13990", "id": "4E3UBih479cJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Deep Learning Acceleration on Edge Devices with Algorithm/Hardware Co-Design", "title_link": "https://repository.library.northeastern.edu/files/neu:4f186q00m/fulltext.pdf", "id": "b0LFVtI0wsoJ", "cited_by_count": 0}, {"title": "Scalable MatMul-free Language Modeling", "title_link": "https://arxiv.org/abs/2406.02528", "id": "0lryrpFq4N4J", "cited_by_count": 5}, {"title": "Are Conventional SNNs Really Efficient? A Perspective from Network Quantization", "title_link": "http://openaccess.thecvf.com/content/CVPR2024/html/Shen_Are_Conventional_SNNs_Really_Efficient_A_Perspective_from_Network_Quantization_CVPR_2024_paper.html", "id": "KfJoDgEAacEJ", "cited_by_count": 2}, {"title": "Efficient hardware acceleration of deep neural networks via arithmetic complexity reduction", "title_link": "https://upcommons.upc.edu/handle/2117/404668", "id": "9XYBsWMEzm4J", "cited_by_count": 0}, {"title": "[PDF][PDF] Towards Green AI: Assessing the Robustness of Conformer and Transformer Models under Compression", "title_link": "https://eurasip.org/Proceedings/Eusipco/Eusipco2024/pdfs/0000336.pdf", "id": "6mDjWGPQt5UJ", "cited_by_count": 0}, {"title": "ETA: An efficient training accelerator for DNNs based on hardware-algorithm co-optimization", "title_link": "https://ieeexplore.ieee.org/abstract/document/9707608/", "id": "oiPfI4c79fQJ", "cited_by_count": 19}, {"title": "Inferflow: an Efficient and Highly Configurable Inference Engine for Large Language Models", "title_link": "https://arxiv.org/abs/2401.08294", "id": "D6tZfQ_RXW8J", "cited_by_count": 2}, {"title": "FastQuery: Communication-efficient Embedding Table Query for Private LLM Inference", "title_link": "https://arxiv.org/abs/2405.16241", "id": "3Z4NxWd3qPcJ", "cited_by_count": 0}, {"title": "Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache", "title_link": "https://proceedings.mlsys.org/paper_files/paper/2024/hash/bbb7506579431a85861a05fff048d3e1-Abstract-Conference.html", "id": "Mdiu4RigREkJ", "cited_by_count": 8}, {"title": "PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression", "title_link": "https://arxiv.org/abs/2405.14852", "id": "C4ASQ9GupTgJ", "cited_by_count": 3}, {"title": "Language Models as Zero-shot Lossless Gradient Compressors: Towards General Neural Parameter Prior Models", "title_link": "https://arxiv.org/abs/2409.17836", "id": "DQyZfbpb8CUJ", "cited_by_count": 0}, {"title": "A Review of Posit Arithmetic for Energy-Efficient Computation: Methodologies, Applications, and Challenges", "title_link": "https://link.springer.com/chapter/10.1007/978-3-031-42478-6_24", "id": "VKYa23iVpmAJ", "cited_by_count": 0}, {"title": "BFD: Binarized Frequency-enhanced Distillation for Vision Transformer", "title_link": "https://ieeexplore.ieee.org/abstract/document/10688360/", "id": "OZZEokmjb3EJ", "cited_by_count": 0}, {"title": "FAT: Frequency-aware transformation for bridging full-precision and low-precision deep representations", "title_link": "https://ieeexplore.ieee.org/abstract/document/9837828/", "id": "QO4W70jtVOoJ", "cited_by_count": 3}, {"title": "A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models", "title_link": "https://arxiv.org/abs/2410.13841", "id": "_c7dHdcT-1cJ", "cited_by_count": 0}, {"title": "Lossless KV Cache Compression to 2%", "title_link": "https://arxiv.org/abs/2410.15252", "id": "M0fOS2iVJNgJ", "cited_by_count": 0}, {"title": "Transformer-based models and hardware acceleration analysis in autonomous driving: A survey", "title_link": "https://arxiv.org/abs/2304.10891", "id": "7SVjf3mCqGEJ", "cited_by_count": 12}, {"title": "MC-MoE: Mixture Compressor for Mixture-of-Experts LLMs Gains More", "title_link": "https://arxiv.org/abs/2410.06270", "id": "xhBN3AhZjpoJ", "cited_by_count": 0}, {"title": "A Survey: Collaborative Hardware and Software Design in the Era of Large Language Models", "title_link": "https://arxiv.org/abs/2410.07265", "id": "ARTFYEP9PBQJ", "cited_by_count": 0}, {"title": "N3h-core: Neuron-designed neural network accelerator via fpga-based heterogeneous computing cores", "title_link": "https://dl.acm.org/doi/abs/10.1145/3490422.3502367", "id": "immwtER3HqAJ", "cited_by_count": 14}, {"title": "[HTML][HTML] QuATON: Quantization Aware Training of Optical Neurons", "title_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10996779/", "id": "07FN8MFC0qMJ", "cited_by_count": 0}, {"title": "Ladder: Enabling Efficient {Low-Precision} Deep Learning Computing through Hardware-aware Tensor Transformation", "title_link": "https://www.usenix.org/conference/osdi24/presentation/wang-lei", "id": "weaVkde-KgAJ", "cited_by_count": 3}, {"title": "Improved model design and training techniques for efficient DNN inference", "title_link": "https://repositories.lib.utexas.edu/items/7b77b6bd-021f-4fcb-9d3e-ffbabd34cce0", "id": "_2yZEyd2jqAJ", "cited_by_count": 0}, {"title": "LightToken: A task and model-agnostic lightweight token embedding framework for pre-trained language models", "title_link": "https://dl.acm.org/doi/abs/10.1145/3580305.3599416", "id": "utCVQ-NOKA0J", "cited_by_count": 4}, {"title": "Parameter-efficient fine-tuning for large models: A comprehensive survey", "title_link": "https://arxiv.org/abs/2403.14608", "id": "-Y8JuxIyxIQJ", "cited_by_count": 119}, {"title": "[PDF][PDF] Assessing Task-Specific Performance Gains from Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models", "title_link": "https://thesis.eur.nl/pub/73000/FV_Thesis_477745pv_MSc.pdf", "id": "i5ZvpuyNk74J", "cited_by_count": 0}, {"title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference", "title_link": "https://arxiv.org/abs/2408.04107", "id": "qWms09jKZb4J", "cited_by_count": 0}, {"title": "MPQ-YOLO: Ultra low mixed-precision quantization of YOLO for edge devices deployment", "title_link": "https://www.sciencedirect.com/science/article/pii/S0925231223013334", "id": "5WiRkBRsaysJ", "cited_by_count": 7}, {"title": "Spike Motion: A Spiking Neural Network Framework for Resource Aware Implementation of Spiking Transformer Network on FPGA", "title_link": "https://search.proquest.com/openview/2fcd00f34176b5d739636d70fee2c75f/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "b5b-lqTHq38J", "cited_by_count": 0}, {"title": "Quantized Embedding Vectors for Controllable Diffusion Language Models", "title_link": "https://arxiv.org/abs/2402.10107", "id": "3cZsCA7jhV4J", "cited_by_count": 0}, {"title": "SI-BiViT: Binarizing Vision Transformers with Spatial Interaction", "title_link": "https://openreview.net/forum?id=HacSqd6Yw6", "id": "W0D_s-ju2lAJ", "cited_by_count": 0}, {"title": "Single-path bit sharing for automatic loss-aware model compression", "title_link": "https://ieeexplore.ieee.org/abstract/document/10122994/", "id": "OIwkPrehseoJ", "cited_by_count": 6}, {"title": "Compression and Analysis of Pre-trained Language Model using Neural Slimming", "title_link": "https://uwspace.uwaterloo.ca/handle/10012/18586", "id": "G3aPn7FYdEQJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Semantics-Augmented Quantization-Aware Training for Point Cloud Classification", "title_link": "https://diglib.eg.org/bitstreams/94aeb24c-1cf4-4edc-85f0-98797cb77c81/download", "id": "SvBxX49fLq0J", "cited_by_count": 0}, {"title": "2Bits of Protein: Efficient Protein Language Models at the Scale of 2-bits", "title_link": "https://openreview.net/forum?id=bVQjzz3ABw", "id": "QKAj-ekNkUQJ", "cited_by_count": 0}, {"title": "ME-Switch: A Memory-Efficient Expert Switching Framework for Large Language Models", "title_link": "https://arxiv.org/abs/2406.09041", "id": "TQMC3amakb0J", "cited_by_count": 0}, {"title": "[BUCH][B] Binary Neural Networks: Algorithms, Architectures, and Applications", "title_link": "https://books.google.com/books?hl=de&lr=&id=fqQIEQAAQBAJ&oi=fnd&pg=PP1&dq=transformer+AND+(%222-bit%22+OR+%222bit%22+OR+%222+bit%22+OR+%221-bit%22+OR+%221bit%22+OR+%221+bit%22)+AND+(%22post-training%22+OR+%22post+training%22)+AND+%22quantization%22+AND+%22function%22+-%22image+compression%22&ots=97Vl65JMOz&sig=7_CzzpYZANhPodMwJwMww8u_xVk", "id": "U_JzY1feVZAJ", "cited_by_count": 1}, {"title": "Accessible Foundation Models: Systems, Algorithms, and Science", "title_link": "https://search.proquest.com/openview/6c8dd68b06a47574c371e521806504a7/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "hsZpaXAMD6MJ", "cited_by_count": 0}, {"title": "Dual-discriminator adversarial framework for data-free quantization", "title_link": "https://www.sciencedirect.com/science/article/pii/S0925231222011420", "id": "HHJRfPwonToJ", "cited_by_count": 6}, {"title": "Endor: Hardware-Friendly Sparse Format for Offloaded LLM Inference", "title_link": "https://arxiv.org/abs/2406.11674", "id": "gpRwzM-3vI0J", "cited_by_count": 0}, {"title": "Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm", "title_link": "https://arxiv.org/abs/2403.05527", "id": "h48eVYF__7wJ", "cited_by_count": 33}, {"title": "Structured term pruning for computational efficient neural networks inference", "title_link": "https://ieeexplore.ieee.org/abstract/document/9759473/", "id": "nxSLY9oCLO0J", "cited_by_count": 5}, {"title": "Fast State Restoration in LLM Serving with HCache", "title_link": "https://arxiv.org/abs/2410.05004", "id": "vo-MdRvPuOoJ", "cited_by_count": 1}, {"title": "FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation", "title_link": "https://arxiv.org/abs/2407.07093", "id": "kgmbXuMZJQsJ", "cited_by_count": 0}, {"title": "Reducing the Side-Effects of Oscillations in Training of Quantized YOLO Networks", "title_link": "https://openaccess.thecvf.com/content/WACV2024/html/Gupta_Reducing_the_Side-Effects_of_Oscillations_in_Training_of_Quantized_YOLO_WACV_2024_paper.html", "id": "WD_LSpHMD6sJ", "cited_by_count": 2}, {"title": "SPARK: Scalable and Precision-Aware Acceleration of Neural Networks via Efficient Encoding", "title_link": "https://ieeexplore.ieee.org/abstract/document/10476472/", "id": "UiSB028IBLoJ", "cited_by_count": 6}, {"title": "A low-cost fully integer-based cnn accelerator on fpga for real-time traffic sign recognition", "title_link": "https://ieeexplore.ieee.org/abstract/document/9853508/", "id": "FJK-3HYytaUJ", "cited_by_count": 11}, {"title": "Sharpness-Aware Data Generation for Zero-shot Quantization", "title_link": "https://openreview.net/forum?id=8mKXMnhnFW", "id": "Q5CzJc69AX8J", "cited_by_count": 0}, {"title": "Fast: Dnn training under variable precision block floating point with stochastic rounding", "title_link": "https://ieeexplore.ieee.org/abstract/document/9773221/", "id": "MJZ7IjIZM3oJ", "cited_by_count": 56}, {"title": "Think: Thinner key cache by query-driven pruning", "title_link": "https://arxiv.org/abs/2407.21018", "id": "GFtTSyXTwwwJ", "cited_by_count": 2}, {"title": "Training-Free Activation Sparsity in Large Language Models", "title_link": "https://arxiv.org/abs/2408.14690", "id": "ZtWRVIIuw60J", "cited_by_count": 0}, {"title": "QEBVerif: Quantization error bound verification of neural networks", "title_link": "https://link.springer.com/chapter/10.1007/978-3-031-37703-7_20", "id": "Hla00VOEBfgJ", "cited_by_count": 9}, {"title": "ESPACE: Dimensionality Reduction of Activations for Model Compression", "title_link": "https://arxiv.org/abs/2410.05437", "id": "gjtzFKz1WYcJ", "cited_by_count": 0}, {"title": "Succinct Compression: Lossless Compression for Fast and Memory-Efficient Deep Neural Network Inference", "title_link": "https://openreview.net/forum?id=VNzq9PBFta", "id": "dY7H2pxqePIJ", "cited_by_count": 0}, {"title": "PSQ: An Automatic Search Framework for Data-Free Quantization on PIM-based Architecture", "title_link": "https://ieeexplore.ieee.org/abstract/document/10361000/", "id": "yVJXewpTnRUJ", "cited_by_count": 1}, {"title": "Inference Optimization of Foundation Models on AI Accelerators", "title_link": "https://dl.acm.org/doi/abs/10.1145/3637528.3671465", "id": "QN6H78rWQjwJ", "cited_by_count": 0}, {"title": "Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs", "title_link": "https://arxiv.org/abs/2405.20835", "id": "_r0GcG7HThsJ", "cited_by_count": 2}, {"title": "BOLD: Boolean Logic Deep Learning", "title_link": "https://arxiv.org/abs/2405.16339", "id": "Dvy2lOOr8NUJ", "cited_by_count": 0}, {"title": "Self-distilled quantization: Achieving high compression rates in transformer-based language models", "title_link": "https://arxiv.org/abs/2307.05972", "id": "f9wtJqzpgSsJ", "cited_by_count": 2}, {"title": "Pruning large language models with semi-structural adaptive sparse training", "title_link": "https://arxiv.org/abs/2407.20584", "id": "DsaR8xKxEmsJ", "cited_by_count": 1}, {"title": "[HTML][HTML] Accelerating and Compressing Transformer-Based PLMs for Enhanced Comprehension of Computer Terminology", "title_link": "https://www.mdpi.com/1999-5903/16/11/385", "id": "S0_Ys2oxNb0J", "cited_by_count": 0}, {"title": "Efficient Processing of Convolutional Neural Networks on the Edge: A Hybrid Approach Using Hardware Acceleration and Dual-Teacher Compression", "title_link": "https://stars.library.ucf.edu/etd2023/321/", "id": "64RsPfLheRYJ", "cited_by_count": 0}, {"title": "Smart-DNN+: A Memory-efficient Neural Networks Compression Framework for the Model Inference", "title_link": "https://dl.acm.org/doi/abs/10.1145/3617688", "id": "WjGHVusCDx8J", "cited_by_count": 1}, {"title": "Joint Pruning and Channel-wise Mixed-Precision Quantization for Efficient Deep Neural Networks", "title_link": "https://ieeexplore.ieee.org/abstract/document/10644100/", "id": "Y1gVaiagBGoJ", "cited_by_count": 0}, {"title": "LLM-Codebook for Extreme Compression of Large Language Models", "title_link": "https://openreview.net/forum?id=nMbWsXPUVL", "id": "ELO24GAK0GMJ", "cited_by_count": 0}, {"title": "Foldgpt: Simple and effective large language model compression scheme", "title_link": "https://arxiv.org/abs/2407.00928", "id": "uXo_PnuyVaIJ", "cited_by_count": 1}, {"title": "Swift: a scalable lightweight infrastructure for fine-tuning", "title_link": "https://arxiv.org/abs/2408.05517", "id": "qI3F_km6biQJ", "cited_by_count": 2}, {"title": "Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge", "title_link": "https://arxiv.org/abs/2410.16454", "id": "hmvlD7GUQMEJ", "cited_by_count": 0}, {"title": "Variation-aware vision transformer quantization", "title_link": "https://arxiv.org/abs/2307.00331", "id": "BIjxgeV3y7kJ", "cited_by_count": 11}, {"title": "Hardware-Efficient Quantization for Green Custom Foundation Models", "title_link": "https://openreview.net/forum?id=NODACmWFNJ", "id": "F_4wTncq5J0J", "cited_by_count": 0}, {"title": "A Comprehensive Survey on Recent Model Compression and Acceleration Approaches for Deep Neural Networks and Transformers", "title_link": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4893335", "id": "0ZzpDGZAPwUJ", "cited_by_count": 0}, {"title": "MoDeGPT: Modular Decomposition for Large Language Model Compression", "title_link": "https://arxiv.org/abs/2408.09632", "id": "YS7l0sbVsf8J", "cited_by_count": 0}, {"title": "Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix", "title_link": "https://arxiv.org/abs/2410.11261", "id": "ocbdG0Sx63wJ", "cited_by_count": 0}, {"title": "SoBS-X: Squeeze-out bit sparsity for ReRAM-crossbar-based neural network accelerator", "title_link": "https://ieeexplore.ieee.org/abstract/document/9769275/", "id": "FFRkQOy1myQJ", "cited_by_count": 6}, {"title": "Table-Lookup MAC: Scalable Processing of Quantised Neural Networks in FPGA Soft Logic", "title_link": "https://dl.acm.org/doi/abs/10.1145/3626202.3637576", "id": "F_mrCbxU9bYJ", "cited_by_count": 0}, {"title": "Exploiting Neural-Network Statistics for Low-Power DNN Inference", "title_link": "https://ieeexplore.ieee.org/abstract/document/10498075/", "id": "_h_DYXYaxfgJ", "cited_by_count": 0}, {"title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context Scenarios", "title_link": "https://arxiv.org/abs/2409.10593", "id": "0vJ9rMLBZcYJ", "cited_by_count": 0}, {"title": "Applying maximum entropy principle on quantized neural networks correlates with high accuracy", "title_link": "https://hal.science/hal-04409740/", "id": "LDiSIdxbyS4J", "cited_by_count": 0}, {"title": "Training of Physical Neural Networks", "title_link": "https://arxiv.org/abs/2406.03372", "id": "eVK7HX_ajiEJ", "cited_by_count": 2}, {"title": "[PDF][PDF] End-to-End Neural Network Compression via \u21131", "title_link": "https://openaccess.thecvf.com/content/CVPR2024W/MAI/supplemental/Nasery_End-to-End_Neural_Network_CVPRW_2024_supplemental.pdf", "id": "OfPMzBl2B14J", "cited_by_count": 0}, {"title": "INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers", "title_link": "https://arxiv.org/abs/2307.03712", "id": "DbTjMQbP7pUJ", "cited_by_count": 1}, {"title": "Sparse finetuning for inference acceleration of large language models", "title_link": "https://arxiv.org/abs/2310.06927", "id": "aZjhI5RBJBIJ", "cited_by_count": 12}, {"title": "Rand: Robustness aware norm decay for quantized seq2seq models", "title_link": "https://arxiv.org/abs/2305.15536", "id": "10NEyXXlT_EJ", "cited_by_count": 4}, {"title": "Efficient speech representation learning with low-bit quantization", "title_link": "https://arxiv.org/abs/2301.00652", "id": "N_phVgfQRtEJ", "cited_by_count": 6}, {"title": "Improving post-training quantization on object detection with task loss-guided lp metric", "title_link": "https://arxiv.org/abs/2304.09785", "id": "O9Znp7vidR8J", "cited_by_count": 1}, {"title": "[PDF][PDF] A Comparative Study of PEFT Approaches for Language Models of Code", "title_link": "https://helda.helsinki.fi/bitstreams/75b4256c-009a-4cf2-bf84-798a6127dfaf/download", "id": "H20nOP6zooMJ", "cited_by_count": 0}, {"title": "Computational complexity optimization of neural network-based equalizers in digital signal processing: a comprehensive approach", "title_link": "https://ieeexplore.ieee.org/abstract/document/10496171/", "id": "yiLIEAapB9QJ", "cited_by_count": 14}, {"title": "Flexible Residual Binarization for Image Super-Resolution", "title_link": "https://openreview.net/forum?id=zji9DLksTz", "id": "ukxNarN_px4J", "cited_by_count": 2}, {"title": "Binarydm: Towards accurate binarization of diffusion model", "title_link": "https://arxiv.org/abs/2404.05662", "id": "bTWx9cuX9a8J", "cited_by_count": 4}, {"title": "A Comprehensive Approach Towards Wheat Leaf Disease Identification Leveraging Transformer Models and Federated Learning", "title_link": "https://ieeexplore.ieee.org/abstract/document/10623136/", "id": "fpsB3flQgBsJ", "cited_by_count": 0}, {"title": "OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning", "title_link": "https://arxiv.org/abs/2405.05957", "id": "RLa1KmA4T-UJ", "cited_by_count": 1}, {"title": "FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models", "title_link": "https://arxiv.org/abs/2405.18218", "id": "2lH8fSS9cZAJ", "cited_by_count": 2}, {"title": "Cbq: Cross-block quantization for large language models", "title_link": "https://arxiv.org/abs/2312.07950", "id": "SYs6WXFmxT0J", "cited_by_count": 6}, {"title": "A survey of quantization methods for efficient neural network inference", "title_link": "https://www.taylorfrancis.com/chapters/edit/10.1201/9781003162810-13/survey-quantization-methods-efficient-neural-network-inference-amir-gholami-sehoon-kim-zhen-dong-zhewei-yao-michael-mahoney-kurt-keutzer", "id": "Qqkb0h3nbzYJ", "cited_by_count": 1169}, {"title": "Zeroquant (4+ 2): Redefining llms quantization with a new fp6-centric strategy for diverse generative tasks", "title_link": "https://arxiv.org/abs/2312.08583", "id": "Crhvmi5S_jYJ", "cited_by_count": 5}, {"title": "Survey on Computer Vision Techniques for Internet-of-Things Devices", "title_link": "https://ieeexplore.ieee.org/abstract/document/10205899/", "id": "Vq-zJD9dqesJ", "cited_by_count": 2}, {"title": "MultiQuant: A Novel Multi-Branch Topology Method for Arbitrary Bit-width Network Quantization", "title_link": "https://arxiv.org/abs/2305.08117", "id": "3MnH99Uc0iYJ", "cited_by_count": 2}, {"title": "Understanding and improving knowledge distillation for quantization-aware training of large transformer encoders", "title_link": "https://arxiv.org/abs/2211.11014", "id": "zeh2Hqs3FugJ", "cited_by_count": 8}, {"title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models", "title_link": "https://arxiv.org/abs/2405.14366", "id": "yt6lkpf21PoJ", "cited_by_count": 12}, {"title": "Timestep-Aware Correction for Quantized Diffusion Models", "title_link": "https://arxiv.org/abs/2407.03917", "id": "EF2CboHURTQJ", "cited_by_count": 0}, {"title": "Enhancing computation efficiency in large language models through weight and activation quantization", "title_link": "https://arxiv.org/abs/2311.05161", "id": "omL03houD9YJ", "cited_by_count": 7}, {"title": "Progressive Mixed-Precision Decoding for Efficient LLM Inference", "title_link": "https://arxiv.org/abs/2410.13461", "id": "tUWP0rhRyzAJ", "cited_by_count": 0}, {"title": "GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs", "title_link": "https://arxiv.org/abs/2408.15300", "id": "Xm7eRRv19VEJ", "cited_by_count": 1}, {"title": "BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models", "title_link": "https://arxiv.org/abs/2310.01329", "id": "k3gjiKwsdcQJ", "cited_by_count": 2}, {"title": "Fast inference of mixture-of-experts language models with offloading", "title_link": "https://arxiv.org/abs/2312.17238", "id": "vw4Of67TQm0J", "cited_by_count": 12}, {"title": "Boosting Binary Neural Networks via Dynamic Thresholds Learning", "title_link": "https://arxiv.org/abs/2211.02292", "id": "4kzYfhp5M8QJ", "cited_by_count": 0}, {"title": "Logic and arithmetic computation-in-memory accelerators: Based on memristor devices", "title_link": "https://research.tudelft.nl/en/publications/logic-and-arithmetic-computation-in-memory-accelerators-based-on-", "id": "SjDrlrVdFIUJ", "cited_by_count": 0}, {"title": "[HTML][HTML] Computer vision model compression techniques for embedded systems: A survey", "title_link": "https://www.sciencedirect.com/science/article/pii/S009784932400150X", "id": "THaJ1DHUhgsJ", "cited_by_count": 0}, {"title": "Error-aware Quantization through Noise Tempering", "title_link": "https://arxiv.org/abs/2212.05603", "id": "_xoxnmEHaoQJ", "cited_by_count": 2}, {"title": "PalmBench: A Comprehensive Benchmark of Compressed Large Language Models on Mobile Platforms", "title_link": "https://arxiv.org/abs/2410.05315", "id": "4kpjXUf7jGsJ", "cited_by_count": 0}, {"title": "Efficient quantization-aware training with adaptive coreset selection", "title_link": "https://arxiv.org/abs/2306.07215", "id": "3clBJGFJ98gJ", "cited_by_count": 11}, {"title": "Quantization of Neural Networks", "title_link": "https://link.springer.com/chapter/10.1007/978-981-99-5068-3_4", "id": "0pZz4qVJiCMJ", "cited_by_count": 0}, {"title": "Interpolating Video-LLMs: Toward Longer-sequence LMMs in a Training-free Manner", "title_link": "https://arxiv.org/abs/2409.12963", "id": "XvCn6Edkw_sJ", "cited_by_count": 0}, {"title": "[PDF][PDF] SpQuant-SNN: ultra-low precision membrane potential with sparse activations unlock the potential of on-device spiking neural networks applications", "title_link": "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1440000/pdf?isPublishedV2=true", "id": "zPyiO9dFp8EJ", "cited_by_count": 0}, {"title": "Mobile edge intelligence for large language models: A contemporary survey", "title_link": "https://arxiv.org/abs/2407.18921", "id": "1EpY2uQEhdEJ", "cited_by_count": 7}, {"title": "A survey on efficient inference for large language models", "title_link": "https://arxiv.org/abs/2404.14294", "id": "duKqwDtsssQJ", "cited_by_count": 35}, {"title": "Embedding Compression in Recommender Systems: A Survey", "title_link": "https://dl.acm.org/doi/abs/10.1145/3637841", "id": "03GY_2749LoJ", "cited_by_count": 10}, {"title": "Binary and ternary natural language generation", "title_link": "https://arxiv.org/abs/2306.01841", "id": "dZNx6aOYwGIJ", "cited_by_count": 7}, {"title": "Hardware Support for Trustworthy Machine Learning: A Survey", "title_link": "https://ieeexplore.ieee.org/abstract/document/10528373/", "id": "zI5VAY1_FJgJ", "cited_by_count": 0}, {"title": "Deeploy: Enabling Energy-Efficient Deployment of Small Language Models On Heterogeneous Microcontrollers", "title_link": "https://arxiv.org/abs/2408.04413", "id": "riYJuv6tSr8J", "cited_by_count": 1}, {"title": "[HTML][HTML] LSTM Gate Disclosure as an Embedded AI Methodology for Wearable Fall-Detection Sensors", "title_link": "https://www.mdpi.com/2073-8994/16/10/1296", "id": "WvGjMk-9MnEJ", "cited_by_count": 0}, {"title": "Device-edge digital semantic communication with trained non-linear quantization", "title_link": "https://ieeexplore.ieee.org/abstract/document/10200355/", "id": "mrIk6HW9xTwJ", "cited_by_count": 7}, {"title": "Mixed-Precision Quantization with Cross-Layer Dependencies", "title_link": "https://arxiv.org/abs/2307.05657", "id": "441YCDntx8QJ", "cited_by_count": 2}, {"title": "Lq-lora: Low-rank plus quantized matrix decomposition for efficient language model finetuning", "title_link": "https://arxiv.org/abs/2311.12023", "id": "UCbBoC51mxkJ", "cited_by_count": 30}, {"title": "Efficient and effective methods for mixed precision neural network quantization for faster, energy-efficient inference", "title_link": "https://arxiv.org/abs/2301.13330", "id": "x66_pmVo_fMJ", "cited_by_count": 8}, {"title": "Llm inference unveiled: Survey and roofline model insights", "title_link": "https://arxiv.org/abs/2402.16363", "id": "u4RfTVhKko0J", "cited_by_count": 30}, {"title": "Overviewing AI-Dedicated Hardware for On-Device AI in Smartphones", "title_link": "https://link.springer.com/chapter/10.1007/978-3-031-22170-5_4", "id": "PU0Dib320zYJ", "cited_by_count": 3}, {"title": "Comparative Study: Standalone IEEE 16-bit Floating-Point for Image Classification", "title_link": "https://arxiv.org/abs/2305.10947", "id": "_3SBanAY-b4J", "cited_by_count": 1}, {"title": "Approximate computing and the efficient machine learning expedition", "title_link": "https://dl.acm.org/doi/abs/10.1145/3508352.3561105", "id": "iGUlUFY8hVAJ", "cited_by_count": 13}, {"title": "Neural Network Compression Using Binarization and Few Full-Precision Weights", "title_link": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4927691", "id": "lHv2EoN3woUJ", "cited_by_count": 0}, {"title": "Auto Tuning Quantized Graph Neural Networks on GPU Tensor Core via TVM", "title_link": "https://ieeexplore.ieee.org/abstract/document/10456077/", "id": "ijscYxxH3UEJ", "cited_by_count": 0}, {"title": "A Review of State-of-the-Art Mixed-Precision Neural Network Frameworks", "title_link": "https://ieeexplore.ieee.org/abstract/document/10509805/", "id": "ZnTVbznLZ9MJ", "cited_by_count": 3}, {"title": "A survey of computationally efficient graph neural networks for reconfigurable systems", "title_link": "https://www.mdpi.com/2078-2489/15/7/377", "id": "sXxnhonaGMYJ", "cited_by_count": 1}, {"title": "[BUCH][B] RRAM Compute-in-Memory Hardware for Efficient, Versatile, and Accurate ai Inference", "title_link": "https://search.proquest.com/openview/4b2d442d248410e80abe6f44e4952273/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "nPd0v6zhT0UJ", "cited_by_count": 0}, {"title": "A Survey of Model Compression and Its Feedback Mechanism in Federated Learning", "title_link": "https://dl.acm.org/doi/abs/10.1145/3643488.3660293", "id": "ymslhLU2XQsJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Training High-performance Spiking Neural Networks Through Reducing Quantization Error", "title_link": "https://practical-dl.github.io/2022/short_paper/19.pdf", "id": "EdbMKBEdYYQJ", "cited_by_count": 0}, {"title": "Efficient methods for deep learning", "title_link": "https://www.sciencedirect.com/science/article/pii/B9780128221099000138", "id": "p0E7bYf3gwkJ", "cited_by_count": 9}, {"title": "Enable deep learning on mobile devices: Methods, systems, and applications", "title_link": "https://dl.acm.org/doi/abs/10.1145/3486618", "id": "B5HXaxbAB_cJ", "cited_by_count": 103}, {"title": "Odg-q: Robust quantization via online domain generalization", "title_link": "https://ieeexplore.ieee.org/abstract/document/9956164/", "id": "BAcFD7nixkMJ", "cited_by_count": 1}, {"title": "[PDF][PDF] Energy-Efficient AI on EDGE", "title_link": "https://www.researchgate.net/profile/Prasham-Sheth/publication/370471083_Energy-Efficient_AI_on_EDGE/links/6451f90a97449a0e1a741b3c/Energy-Efficient-AI-on-EDGE.pdf", "id": "Mbhr1998BEIJ", "cited_by_count": 0}, {"title": "Software Optimization and Design Methodology for Low Power Computer Vision Systems", "title_link": "https://dl.acm.org/doi/abs/10.1145/3687310", "id": "Dml28GAMpoIJ", "cited_by_count": 0}, {"title": "VcLLM: Video Codecs are Secretly Tensor Codecs", "title_link": "https://arxiv.org/abs/2407.00467", "id": "qf2xcgkTxecJ", "cited_by_count": 0}, {"title": "EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search", "title_link": "https://arxiv.org/abs/2410.14649", "id": "vyV_REOckVwJ", "cited_by_count": 0}, {"title": "Towards Light-Weight and High Performance Speech Enhancement and Recognition Using Mixed Precision Neural Network Quantization", "title_link": "https://search.proquest.com/openview/ba60e85ee8e3ece25e8f18db2822dbcb/1?pq-origsite=gscholar&cbl=2026366&diss=y", "id": "-4zGJtKB1B4J", "cited_by_count": 0}, {"title": "Bringing AI to edge: From deep learning's perspective", "title_link": "https://www.sciencedirect.com/science/article/pii/S0925231221016428", "id": "xG3kTPM2dMkJ", "cited_by_count": 130}, {"title": "Fast and Low-Cost Approximate Multiplier for FPGAs using Dynamic Reconfiguration", "title_link": "https://arxiv.org/abs/2310.10053", "id": "oDElByoWeHMJ", "cited_by_count": 0}, {"title": "RBNN: memory-efficient reconfigurable deep binary neural network with IP protection for Internet of things", "title_link": "https://ieeexplore.ieee.org/abstract/document/9852785/", "id": "Ce0lEe-ODmgJ", "cited_by_count": 8}, {"title": "Deploying deep learning networks based advanced techniques for image processing on FPGA platform", "title_link": "https://link.springer.com/article/10.1007/s00521-023-08718-3", "id": "LafzqVmPE9oJ", "cited_by_count": 7}, {"title": "A survey of resource-efficient llm and multimodal foundation models", "title_link": "https://arxiv.org/abs/2401.08092", "id": "5sm8TgDYkeIJ", "cited_by_count": 57}, {"title": "SpaLLM: Unified Compressive Adaptation of Large Language Models with Sketching", "title_link": "https://arxiv.org/abs/2410.06364", "id": "9hVx8UGEtzQJ", "cited_by_count": 0}, {"title": "DyRecMul: Fast and Low-Cost Approximate Multiplier for FPGAs using Dynamic Reconfiguration", "title_link": "https://dl.acm.org/doi/abs/10.1145/3663480", "id": "3y6zI-NVGVwJ", "cited_by_count": 1}, {"title": "QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models", "title_link": "https://arxiv.org/abs/2307.03738", "id": "238E_n0elMoJ", "cited_by_count": 3}, {"title": "On-device language models: A comprehensive review", "title_link": "https://arxiv.org/abs/2409.00088", "id": "eItQJPAiF9QJ", "cited_by_count": 3}, {"title": "A survey of model compression strategies for object detection", "title_link": "https://link.springer.com/article/10.1007/s11042-023-17192-x", "id": "9VzRJdEYraAJ", "cited_by_count": 17}, {"title": "Efficient large language models: A survey", "title_link": "https://arxiv.org/abs/2312.03863", "id": "ok-1VecpDZMJ", "cited_by_count": 83}, {"title": "Join the high accuracy club on imagenet with a binary neural network ticket", "title_link": "https://arxiv.org/abs/2211.12933", "id": "M_mFdJIlJQIJ", "cited_by_count": 20}, {"title": "Highly accurate and fast YOLOv4-Based polyp detection", "title_link": "https://www.sciencedirect.com/science/article/pii/S0957417423013362", "id": "OB9Fq2rCOQMJ", "cited_by_count": 17}, {"title": "Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping", "title_link": "https://arxiv.org/abs/2308.07641", "id": "7okIU4C6HZ8J", "cited_by_count": 2}, {"title": "Light-weight visualvoice: Neural network quantization on audio visual speech separation", "title_link": "https://ieeexplore.ieee.org/abstract/document/10193263/", "id": "redKcvL8HYkJ", "cited_by_count": 5}, {"title": "Efficient Deep Learning Models for Edge IOT Devices-A Review", "title_link": "https://www.techrxiv.org/doi/full/10.36227/techrxiv.172254372.21002541", "id": "y0ILMKpaP8kJ", "cited_by_count": 0}, {"title": "INTERPOLATING VIDEO-LLMS: TOWARD LONGER", "title_link": "https://openreview.net/forum?id=QrTvFCa4nX", "id": "VdKNPbLKSboJ", "cited_by_count": 0}, {"title": "Apple intelligence foundation language models", "title_link": "https://arxiv.org/abs/2407.21075", "id": "xXeIKWO4ddAJ", "cited_by_count": 10}, {"title": "CRPIM: An efficient compute-reuse scheme for ReRAM-based Processing-in-Memory DNN accelerators", "title_link": "https://www.sciencedirect.com/science/article/pii/S1383762124001292", "id": "TlxXeden_sgJ", "cited_by_count": 0}, {"title": "SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms", "title_link": "https://arxiv.org/abs/2406.03287", "id": "rZqdPQm32QAJ", "cited_by_count": 4}, {"title": "MobileNMT: Enabling Translation in 15MB and 30ms", "title_link": "https://arxiv.org/abs/2306.04235", "id": "X-1ino6pF30J", "cited_by_count": 1}, {"title": "Hardware approximate techniques for deep neural network accelerators: A survey", "title_link": "https://dl.acm.org/doi/abs/10.1145/3527156", "id": "X7-Fu3D9-GQJ", "cited_by_count": 97}, {"title": "Block-Wise Mixed-Precision Quantization: Enabling High Efficiency for Practical ReRAM-based DNN Accelerators", "title_link": "https://arxiv.org/abs/2310.12182", "id": "5GX_HsuXvxAJ", "cited_by_count": 1}, {"title": "Llm as a system service on mobile devices", "title_link": "https://arxiv.org/abs/2403.11805", "id": "n2yLM788Q2YJ", "cited_by_count": 15}, {"title": "LLM Adaptation and Utilization", "title_link": "https://link.springer.com/content/pdf/10.1007/978-3-031-65647-7_4.pdf", "id": "U1_LLsCav0kJ", "cited_by_count": 0}, {"title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads", "title_link": "https://arxiv.org/abs/2410.01805", "id": "NHhqj4VLjmcJ", "cited_by_count": 0}, {"title": "Towards accurate binary neural networks via modeling contextual dependencies", "title_link": "https://link.springer.com/chapter/10.1007/978-3-031-20083-0_32", "id": "A51nMypeclgJ", "cited_by_count": 9}, {"title": "Seed-TTS: A Family of High-Quality Versatile Speech Generation Models", "title_link": "https://arxiv.org/abs/2406.02430", "id": "zmNRS52SFCEJ", "cited_by_count": 23}, {"title": "Edgemoe: Fast on-device inference of moe-based large language models", "title_link": "https://arxiv.org/abs/2308.14352", "id": "EfiwOPmNFeUJ", "cited_by_count": 39}, {"title": "Achieving Peak Performance for Large Language Models: A Systematic Review", "title_link": "https://ieeexplore.ieee.org/abstract/document/10589417/", "id": "9JggUy0XtccJ", "cited_by_count": 0}, {"title": "Structured pruning for efficient generative pre-trained language models", "title_link": "https://aclanthology.org/2023.findings-acl.692/", "id": "gPQjRmtOVH0J", "cited_by_count": 23}, {"title": "Fpga-based deep learning inference accelerators: Where are we standing?", "title_link": "https://dl.acm.org/doi/abs/10.1145/3613963", "id": "sTFKipU3-gwJ", "cited_by_count": 12}, {"title": "Sparsification of deep neural networks via ternary quantization", "title_link": "https://webthesis.biblio.polito.it/29424/", "id": "pIJB6hMaTKIJ", "cited_by_count": 0}, {"title": "Utilization of local large language models for business applications", "title_link": "https://aaltodoc.aalto.fi/items/01e2845d-07fc-4748-8258-2e526592b202", "id": "yaq8_ncYtysJ", "cited_by_count": 3}, {"title": "[PDF][PDF] Efficient Classification of Aircraft Marshalling Signs with FMCW Radar", "title_link": "https://research.tue.nl/files/333629567/Vermunt_J.pdf", "id": "-em6-JBFgh8J", "cited_by_count": 0}, {"title": "[HTML][HTML] 7 \u03bcJ/inference end-to-end gesture recognition from dynamic vision sensor data using ternarized hybrid convolutional neural networks", "title_link": "https://www.sciencedirect.com/science/article/pii/S0167739X23002704", "id": "20t2HbW_AnwJ", "cited_by_count": 6}, {"title": "Model compression methods for YOLOv5: A review", "title_link": "https://arxiv.org/abs/2307.11904", "id": "axQvxDEz7CoJ", "cited_by_count": 14}, {"title": "Plinio: a user-friendly library of gradient-based methods for complexity-aware DNN optimization", "title_link": "https://ieeexplore.ieee.org/abstract/document/10272045/", "id": "47IxJ2lugFEJ", "cited_by_count": 10}, {"title": "Pruning vs XNOR-Net: A comprehensive study of deep learning for audio classification on edge-devices", "title_link": "https://ieeexplore.ieee.org/abstract/document/9672158/", "id": "Q6kOy9yc65QJ", "cited_by_count": 21}, {"title": "Edge enhanced network monitoring using TinyML", "title_link": "https://oulurepo.oulu.fi/handle/10024/51084", "id": "dJxm8UQJidgJ", "cited_by_count": 0}, {"title": "Constrained deep learning for MEMS sensors-based applications", "title_link": "https://theses.hal.science/tel-04363136/", "id": "l5Vm4MlaYX4J", "cited_by_count": 0}, {"title": "Towards Optimization-Friendly Binary Neural Network", "title_link": "https://openreview.net/forum?id=4Hq816XDDG", "id": "se-0_sUnjOQJ", "cited_by_count": 0}, {"title": "Palu: Compressing KV-Cache with Low-Rank Projection", "title_link": "https://arxiv.org/abs/2407.21118", "id": "uNZd63PX67kJ", "cited_by_count": 0}, {"title": "Efficient Implementation of Activation Function on FPGA for Accelerating Neural Networks", "title_link": "https://ieeexplore.ieee.org/abstract/document/10181406/", "id": "jjWrQdSpKcAJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Performance Analysis and Comparison of Quantized Language Models on Resource Constraints-The Case of Supply Chain Risk Data.", "title_link": "https://www.researchgate.net/profile/Chuka-Uzo/publication/384144893_Performance_Analysis_and_Comparison_of_Quantized_Language_Models_on_Resource_Constraints_-The_Case_of_Supply_Chain_Risk_Data/links/66f16e1efc6cc46489705e53/Performance-Analysis-and-Comparison-of-Quantized-Language-Models-on-Resource-Constraints-The-Case-of-Supply-Chain-Risk-Data.pdf", "id": "oGOJFfoZNBQJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Speeding up Semantic History Compression in Reinforcement Learning", "title_link": "https://epub.jku.at/obvulihs/content/titleinfo/10242755/full.pdf", "id": "CI4BQRNUZH4J", "cited_by_count": 0}, {"title": "Efficient Artificial Intelligence with Novel Matrix Transformations and Homomorphic Encryption", "title_link": "https://ieeexplore.ieee.org/abstract/document/10689665/", "id": "dBZVj7U39jwJ", "cited_by_count": 0}, {"title": "[HTML][HTML] Hardware Acceleration and Approximation of CNN Computations: Case Study on an Integer Version of LeNet", "title_link": "https://www.mdpi.com/2079-9292/13/14/2709", "id": "1D9Wkw3wZiMJ", "cited_by_count": 0}, {"title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices", "title_link": "https://arxiv.org/abs/2406.02532", "id": "8vE7-caqvp0J", "cited_by_count": 2}, {"title": "Cambricon-LLM: A Chiplet-Based Hybrid Architecture for On-Device Inference of 70B LLM", "title_link": "https://arxiv.org/abs/2409.15654", "id": "XHKD0m_mKKAJ", "cited_by_count": 1}, {"title": "Ultra-low Precision Multiplication-free Training for Deep Neural Networks", "title_link": "https://arxiv.org/abs/2302.14458", "id": "NJ3HHvB9ihkJ", "cited_by_count": 1}, {"title": "Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity", "title_link": "https://arxiv.org/abs/2406.02913", "id": "XZCwz7ctHIMJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Low-Cost Language Models: Survey and Performance Evaluation on Python Code Generation Jessica L\u00f3pez Espejel, Mahaman Sanoussi Yahaya Alassan\u00a0\u2026", "title_link": "https://www.researchgate.net/profile/Jessica-Lopez-Espejel/publication/380074496_Low-Cost_Language_Models_Survey_and_Performance_Evaluation_on_Python_Code_Generation/links/66d5c723fa5e11512c47d2c0/Low-Cost-Language-Models-Survey-and-Performance-Evaluation-on-Python-Code-Generation.pdf", "id": "SeBppPJQjPkJ", "cited_by_count": 0}, {"title": "Robust and Energy Efficient Deep Learning Systems", "title_link": "https://repositum.tuwien.at/handle/20.500.12708/197559", "id": "5YJrGH37V18J", "cited_by_count": 0}, {"title": "Improving and Automating Machine Learning Model Compression", "title_link": "https://keep.lib.asu.edu/items/193384", "id": "OMSAQjb_IsEJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Pruning vs XNOR-Net: A Comprehensive Study on Deep Learning for Audio Classification in Microcontrollers", "title_link": "https://www.academia.edu/download/85868787/2108.06128v1.pdf", "id": "X9dwE730TEkJ", "cited_by_count": 0}, {"title": "Review of lightweight deep convolutional neural networks", "title_link": "https://link.springer.com/article/10.1007/s11831-023-10032-z", "id": "gshYhLSWDz8J", "cited_by_count": 13}, {"title": "Fast matrix multiplication for binary and ternary CNNs on ARM CPU", "title_link": "https://ieeexplore.ieee.org/abstract/document/9956533/", "id": "4XezA_jsc4oJ", "cited_by_count": 4}, {"title": "[PDF][PDF] The efficiency spectrum of large language models: An algorithmic survey", "title_link": "https://www.researchgate.net/profile/Tianyi-Chen-36/publication/376139731_The_Efficiency_Spectrum_of_Large_Language_Models_An_Algorithmic_Survey/links/656b657cb86a1d521b28b422/The-Efficiency-Spectrum-of-Large-Language-Models-An-Algorithmic-Survey.pdf", "id": "04TyF9K2ZDoJ", "cited_by_count": 12}, {"title": "Advancements in accelerating deep neural network inference on aiot devices: A survey", "title_link": "https://ieeexplore.ieee.org/abstract/document/10398463/", "id": "WLq0DccPoXEJ", "cited_by_count": 17}, {"title": "Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions", "title_link": "https://arxiv.org/abs/2409.02111", "id": "U2bhIX01DZoJ", "cited_by_count": 0}, {"title": "Neuromorphic Vision Chip", "title_link": "https://link.springer.com/chapter/10.1007/978-3-031-11506-6_5", "id": "wxvl34pDkbIJ", "cited_by_count": 0}, {"title": "[HTML][HTML] Adaptive approximate computing in edge AI and IoT applications: A review", "title_link": "https://www.sciencedirect.com/science/article/pii/S1383762124000511", "id": "FxjQT7sdwlsJ", "cited_by_count": 7}, {"title": "Sparsity-Aware Protocol for ZK-friendly ML Models: Shedding Lights on Practical ZKML", "title_link": "https://eprint.iacr.org/2024/1018", "id": "3i-mNW4y8ccJ", "cited_by_count": 0}, {"title": "Code Generation on a Diet: A Comparative Evaluation of Low-Parameter Large Language Models", "title_link": "https://studenttheses.uu.nl/handle/20.500.12932/46906", "id": "769Nr1WlnMkJ", "cited_by_count": 0}, {"title": "[BUCH][B] Generative AI on AWS: Building context-aware multimodal reasoning applications", "title_link": "https://books.google.com/books?hl=de&lr=&id=MOTiEAAAQBAJ&oi=fnd&pg=PT7&dq=transformer+AND+(%222-bit%22+OR+%222bit%22+OR+%222+bit%22+OR+%221-bit%22+OR+%221bit%22+OR+%221+bit%22)+AND+(%22post-training%22+OR+%22post+training%22)+AND+%22quantization%22+AND+%22function%22+-%22image+compression%22&ots=k3qZtl1bxB&sig=DztwF6Edb7Nd8GUyLX4i-VzxVhY", "id": "x6ioE1G0o04J", "cited_by_count": 3}, {"title": "Expert Router: Orchestrating Efficient Language Model Inference through Prompt Classification", "title_link": "https://arxiv.org/abs/2404.15153", "id": "NQcJQxNrhxEJ", "cited_by_count": 0}, {"title": "Fast and Energy-Efficient Inference for Attention-Based Natural Language Processing Models", "title_link": "https://search.proquest.com/openview/8ef0b6a759aff7cf22bf26f40affd1bd/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "s5M7ZBF-Zk4J", "cited_by_count": 0}, {"title": "Layer Ensemble Averaging for Improving Memristor-Based Artificial Neural Network Performance", "title_link": "https://arxiv.org/abs/2404.15621", "id": "-D52YhD7bgYJ", "cited_by_count": 0}, {"title": "Digital-SC: Digital Semantic Communication with Adaptive Network Split and Learned Non-Linear Quantization", "title_link": "https://arxiv.org/abs/2305.13553", "id": "AHE7hdbgvsQJ", "cited_by_count": 3}, {"title": "Acceleration algorithms in gnns: A survey", "title_link": "https://arxiv.org/abs/2405.04114", "id": "lbMoRGvJxXsJ", "cited_by_count": 2}, {"title": "Development and Optimization of a 1-Dimensional Convolutional Neural Network-Based Keyword Spotting Model for FPGA Acceleration", "title_link": "https://scholarworks.gvsu.edu/theses/1129/", "id": "PcZa4FW9XK4J", "cited_by_count": 0}, {"title": "Resource-Efficient Neural Networks for Embedded Systems", "title_link": "https://www.jmlr.org/papers/v25/18-566.html", "id": "Q8trvuThNvkJ", "cited_by_count": 10}, {"title": "Memory-based computing for energy-efficient ai: Grand challenges", "title_link": "https://ieeexplore.ieee.org/abstract/document/10321880/", "id": "7qZd7ilNi7UJ", "cited_by_count": 2}, {"title": "Optimal fine-grained n: M sparsity for activations and neural gradients", "title_link": "https://arxiv.org/abs/2203.10991", "id": "4FisYL7eY_EJ", "cited_by_count": 12}, {"title": "Lightweight Deep Learning for Resource-Constrained Environments: A Survey", "title_link": "https://dl.acm.org/doi/abs/10.1145/3657282", "id": "0U_Cz5RXTUUJ", "cited_by_count": 6}, {"title": "SpikingRx: From Neural to Spiking Receiver", "title_link": "https://arxiv.org/abs/2409.05610", "id": "cfFM8s-Vh6cJ", "cited_by_count": 0}, {"title": "Model compression of deep neural network architectures for visual pattern recognition: Current status and future directions", "title_link": "https://www.sciencedirect.com/science/article/pii/S0045790624001083", "id": "APNWpmW9frgJ", "cited_by_count": 1}, {"title": "A high-performance accelerator for super-resolution processing on embedded GPU", "title_link": "https://ieeexplore.ieee.org/abstract/document/10041020/", "id": "lMza3IxHFZsJ", "cited_by_count": 6}, {"title": "Efficient Multitask Dense Predictor via Binarization", "title_link": "https://openaccess.thecvf.com/content/CVPR2024/html/Shang_Efficient_Multitask_Dense_Predictor_via_Binarization_CVPR_2024_paper.html", "id": "3NyMD9cTUDgJ", "cited_by_count": 0}, {"title": "CNN hardware accelerator for real-time bearing fault diagnosis", "title_link": "https://www.mdpi.com/1424-8220/23/13/5897", "id": "mue5FCGQGI4J", "cited_by_count": 3}, {"title": "SpaceEvo: Searching Hardware-Friendly Search Space for Efficient Int8 Inference", "title_link": "https://openreview.net/forum?id=RsSJ2_M2Nk4", "id": "VWNzWAkRzi8J", "cited_by_count": 0}, {"title": "Marlin: Mixed-precision auto-regressive parallel inference on large language models", "title_link": "https://arxiv.org/abs/2408.11743", "id": "gMoAsB-KgyUJ", "cited_by_count": 2}, {"title": "Software-Hardware Co-Design: Towards Ultimate Efficiency in Deep Learning Acceleration", "title_link": "https://search.proquest.com/openview/913cca707a0bf3e5557756f302e70a02/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "VmLuStfgxocJ", "cited_by_count": 0}, {"title": "A Review of AIoT-based Edge Devices and Lightweight Deployment", "title_link": "https://www.techrxiv.org/doi/full/10.36227/techrxiv.21687248.v2", "id": "qbi28j-fyRsJ", "cited_by_count": 4}, {"title": "Learning Generalizable Mixed-Precision Quantization via Attribution Imitation", "title_link": "https://link.springer.com/article/10.1007/s11263-024-02130-7", "id": "K89njZANH-gJ", "cited_by_count": 1}, {"title": "Energy-efficient time series analysis with machine learning and deep learning on embedded computing platforms", "title_link": "http://amsdottorato.unibo.it/11234/", "id": "HiEDpn9v00kJ", "cited_by_count": 0}, {"title": "Be Like Water: Adaptive Floating Point for Machine Learning", "title_link": "https://proceedings.mlr.press/v162/yeh22a.html", "id": "3JwGSG8b940J", "cited_by_count": 6}, {"title": "SAC: An ultra-efficient spin-based architecture for compressed DNNs", "title_link": "https://dl.acm.org/doi/abs/10.1145/3632957", "id": "11KTQbm6UUoJ", "cited_by_count": 2}, {"title": "Compeft: Compression for communicating parameter efficient updates via sparsification and quantization", "title_link": "https://arxiv.org/abs/2311.13171", "id": "gOvgdcyK40wJ", "cited_by_count": 7}, {"title": "CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification", "title_link": "https://arxiv.org/abs/2409.01366", "id": "TiOT6o7V2V0J", "cited_by_count": 0}, {"title": "Computational complexity evaluation of neural network applications in signal processing", "title_link": "https://arxiv.org/abs/2206.12191", "id": "nnhhRThRJ-4J", "cited_by_count": 50}, {"title": "[PDF][PDF] Effective multi-hot encoding and classifier for lightweight scene text recognition with a large character set", "title_link": "https://www.researchgate.net/profile/Chun-Guang-Li-2/publication/358098099_Effective_Multi-Hot_Encoding_and_Classifier_for_Lightweight_Scene_Text_Recognition_with_a_Large_Character_Set/links/637ee0b22f4bca7fd088052d/Effective-Multi-Hot-Encoding-and-Classifier-for-Lightweight-Scene-Text-Recognition-with-a-Large-Character-Set.pdf", "id": "QuzVQeVK4p4J", "cited_by_count": 2}, {"title": "[BUCH][B] Tapered-Precision Numerical Formats for Deep Learning Inference and Training", "title_link": "https://search.proquest.com/openview/a88513887d40ec2e6744025447d7d948/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "BKaDncYIgGMJ", "cited_by_count": 0}, {"title": "Mixed-TD: Efficient Neural Network Accelerator with Layer-Specific Tensor Decomposition", "title_link": "https://ieeexplore.ieee.org/abstract/document/10296400/", "id": "Sz5pnkm1DbQJ", "cited_by_count": 0}, {"title": "A methodological framework for optimizing the energy consumption of deep neural networks: a case study of a cyber threat detector", "title_link": "https://link.springer.com/article/10.1007/s00521-024-09588-z", "id": "YI7d-_Hm0gQJ", "cited_by_count": 1}, {"title": "Relu strikes back: Exploiting activation sparsity in large language models", "title_link": "https://arxiv.org/abs/2310.04564", "id": "2sM30rDNoU0J", "cited_by_count": 41}, {"title": "[BUCH][B] Hardware-Aware Efficient Deep Learning", "title_link": "https://search.proquest.com/openview/2b862ba3e326d5c53a77ec5e67adf2b4/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "2pRDLSVvIoUJ", "cited_by_count": 1}, {"title": "Lowering PyTorch's Memory Consumption for Selective Differentiation", "title_link": "https://arxiv.org/abs/2404.12406", "id": "Ns-JZroNhAcJ", "cited_by_count": 0}, {"title": "State of the Art of Machine Learning", "title_link": "https://link.springer.com/chapter/10.1007/978-3-031-46990-9_7", "id": "bMGFNKf4uTIJ", "cited_by_count": 0}, {"title": "Approximate Fault-Tolerant Neural Network Systems", "title_link": "https://ieeexplore.ieee.org/abstract/document/10567290/", "id": "gG8sbseFkoEJ", "cited_by_count": 0}, {"title": "Identification of heart arrhythmias by utilizing a deep learning approach of the ECG signals on edge devices", "title_link": "https://www.mdpi.com/2073-431X/11/12/176", "id": "xeea0y8HatsJ", "cited_by_count": 6}, {"title": "[HTML][HTML] Reinforcement learning-based computation offloading in edge computing: Principles, methods, challenges", "title_link": "https://www.sciencedirect.com/science/article/pii/S1110016824007798", "id": "xNpJR97cnBQJ", "cited_by_count": 0}, {"title": "HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models", "title_link": "https://arxiv.org/abs/2405.10299", "id": "PdIEInYRx6wJ", "cited_by_count": 2}, {"title": "A Review on the emerging technology of TinyML", "title_link": "https://dl.acm.org/doi/abs/10.1145/3661820", "id": "qU2RBl_lnDUJ", "cited_by_count": 2}, {"title": "[BUCH][B] Efficient Hardware Implementation of Deep Learning Networks Based on the Convolutional Neural Network", "title_link": "https://search.proquest.com/openview/88f7fb34f45dd4ff487a123855233d7b/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "3R-XnYuZriAJ", "cited_by_count": 0}, {"title": "Efficient neural networks for tiny machine learning: A comprehensive review", "title_link": "https://arxiv.org/abs/2311.11883", "id": "6lnLfZS4cNAJ", "cited_by_count": 8}, {"title": "[PDF][PDF] In-network ML-based Anomaly Detection", "title_link": "https://davidpissarra.com/thesis.pdf", "id": "OmEk1yAnQekJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Benchmarking public large language model", "title_link": "https://opus4.kobv.de/opus4-haw/files/4593/I001854150Thesis.pdf", "id": "AzH0O1IK4pEJ", "cited_by_count": 20}, {"title": "Enabling Large-Scale Privacy-Preserving Recurrent Neural Networks with Fully Homomorphic Encryption", "title_link": "https://search.proquest.com/openview/9eda6083ce3041bf81a50e4859d57a5e/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "vrjNrnHS4XAJ", "cited_by_count": 0}, {"title": "Evaluation of strategies for the adaptation of large neural models to the task of machine translation in constrained scenarios", "title_link": "https://riunet.upv.es/handle/10251/198517", "id": "ODnomcBEpXcJ", "cited_by_count": 0}, {"title": "Binarized Diffusion Model for Image Super-Resolution", "title_link": "https://arxiv.org/abs/2406.05723", "id": "v6aaO0uUV_oJ", "cited_by_count": 0}, {"title": "Composable interventions for language models", "title_link": "https://arxiv.org/abs/2407.06483", "id": "iLN9v4IQgykJ", "cited_by_count": 2}, {"title": "ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking", "title_link": "https://arxiv.org/abs/2406.11257", "id": "8k3WOTzwlUcJ", "cited_by_count": 0}, {"title": "[HTML][HTML] Autocorrelation Matrix Knowledge Distillation: A Task-Specific Distillation Method for BERT Models", "title_link": "https://www.mdpi.com/2076-3417/14/20/9180", "id": "oCivobVYbooJ", "cited_by_count": 0}, {"title": "Deltazip: Multi-tenant language model serving via delta compression", "title_link": "https://arxiv.org/abs/2312.05215", "id": "Qza0g6lGJFwJ", "cited_by_count": 9}, {"title": "LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ", "title_link": "https://arxiv.org/abs/2409.16779", "id": "GH2ol57x2d8J", "cited_by_count": 0}, {"title": "Design automation for fast, lightweight, and effective deep learning models: A survey", "title_link": "https://arxiv.org/abs/2208.10498", "id": "IfReUUxZVDMJ", "cited_by_count": 3}, {"title": "CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer Learning", "title_link": "https://arxiv.org/abs/2402.12736", "id": "TjzcxOcb440J", "cited_by_count": 0}, {"title": "Approximate Computing and In-Memory Computing: The Best of the Two Worlds!", "title_link": "https://search.proquest.com/openview/3be5d44c649e4c25afb829812c3a56b9/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "HypDB7-4M0YJ", "cited_by_count": 0}, {"title": "[BUCH][B] Compressed Training for Uncertainty-Aware Compact Neural Networks", "title_link": "https://search.proquest.com/openview/a8695767d4b6dc7d7429c3ab2eb7866e/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "OwxnOmaZ34wJ", "cited_by_count": 0}, {"title": "MSD: Mixing Signed Digit Representations for Hardware-efficient DNN Acceleration on FPGA with Heterogeneous Resources", "title_link": "https://ieeexplore.ieee.org/abstract/document/10171562/", "id": "Fpzb2lZOY6AJ", "cited_by_count": 5}, {"title": "A survey on lora of large language models", "title_link": "https://arxiv.org/abs/2407.11046", "id": "k4B--BPLRFwJ", "cited_by_count": 5}, {"title": "Architecting High Performance Silicon Systems for Accurate and Efficient On-Chip Deep Learning", "title_link": "https://search.proquest.com/openview/19ab6db3db1a1658316f5eb66aa4bc51/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "gLv2PW04PXQJ", "cited_by_count": 1}, {"title": "SM2: Towards Similarity-Guided Model Sharding and Merging for Distributed Video Analytics", "title_link": "https://search.proquest.com/openview/e61a81c539faee4affd38c25e32480bf/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "jNUj0mU9nkMJ", "cited_by_count": 0}, {"title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models", "title_link": "https://arxiv.org/abs/2403.08739", "id": "rEslO_DsbEsJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Streamlining TinyML Lifecycle with Large Language Models: A Framework for Automation", "title_link": "https://helda.helsinki.fi/bitstreams/d5538b30-169b-4561-a6e8-31d41052d049/download", "id": "lSDC_DUKhZAJ", "cited_by_count": 0}, {"title": "A Scalable, Interpretable, Verifiable & Differentiable Logic Gate Convolutional Neural Network Architecture From Truth Tables", "title_link": "https://arxiv.org/abs/2208.08609", "id": "h4JKNWjggSgJ", "cited_by_count": 0}, {"title": "Multiple Disease Detection using Machine Learning Techniques.", "title_link": "https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=26268493&AN=172014673&h=dd82NBfNXrJxJYz6agZW5RytIL8ya%2FvVlG60horcMS6veX9MswvzE0li2%2BVj0H0cEMttPeRVOSzdXMIRoyz%2FqA%3D%3D&crl=c", "id": "deHgtLwfqz0J", "cited_by_count": 1}, {"title": "Adaptive Mixed-Precision Networks", "title_link": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4274178", "id": "61uuV-Pv6YwJ", "cited_by_count": 0}, {"title": "Minimum variance unbiased n: M sparsity for the neural gradients", "title_link": "https://openreview.net/forum?id=vuD2xEtxZcj", "id": "4AAmp9SG0c0J", "cited_by_count": 5}, {"title": "Accelerating deep neural networks via semi-structured activation sparsity", "title_link": "https://openaccess.thecvf.com/content/ICCV2023W/RCV/html/Grimaldi_Accelerating_Deep_Neural_Networks_via_Semi-Structured_Activation_Sparsity_ICCVW_2023_paper.html", "id": "FWaTFIhsZ_AJ", "cited_by_count": 5}, {"title": "[BUCH][B] Quantization for High-dimensional Data and Neural Networks: Theory and Algorithms", "title_link": "https://search.proquest.com/openview/f2f3b3aa2341a9227a2b1bab77b1cb91/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "K7uHRYgF7LMJ", "cited_by_count": 1}, {"title": "[BUCH][B] Learned Approximate Computing for Machine Learning", "title_link": "https://search.proquest.com/openview/98a47be3c04fde1b385fe67b0547ea46/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "vTABYZKuOfYJ", "cited_by_count": 0}, {"title": "An 8-bit single perceptron processing unit for tiny machine learning applications", "title_link": "https://ieeexplore.ieee.org/abstract/document/10295439/", "id": "vQ3bAy7qK5oJ", "cited_by_count": 8}, {"title": "Algorithms and Frameworks for Generating Neural Network Models Addressing Energy-Efficiency, Robustness, and Privacy", "title_link": "https://search.proquest.com/openview/1d684fba4cb114b9ab512ebb71adef58/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "tBASfFccUUkJ", "cited_by_count": 0}, {"title": "Boolean Variation and Boolean Logic BackPropagation", "title_link": "https://arxiv.org/abs/2311.07427", "id": "slz6vpmg0ksJ", "cited_by_count": 2}, {"title": "Llm-based edge intelligence: A comprehensive survey on architectures, applications, security and trustworthiness", "title_link": "https://ieeexplore.ieee.org/abstract/document/10669603/", "id": "-RnOV_eFq5YJ", "cited_by_count": 3}, {"title": "T3DNet: Compressing Point Cloud Models for Lightweight 3D Recognition", "title_link": "https://arxiv.org/abs/2402.19264", "id": "YI1LylYDnC4J", "cited_by_count": 0}, {"title": "Dynamic Fog Computing for Enhanced LLM Execution in Medical Applications", "title_link": "https://arxiv.org/abs/2408.04680", "id": "M7IdzhIMpE0J", "cited_by_count": 2}, {"title": "An Optimized Hardware Inference of SABiNN: Shift-Accumulate Binarized Neural Network for Sleep Apnea Detection", "title_link": "https://ieeexplore.ieee.org/abstract/document/10136211/", "id": "1wz3Iz47jd8J", "cited_by_count": 3}, {"title": "Embedded Neural Networks in Resource-Constrained Hearing Instruments", "title_link": "https://orbit.dtu.dk/en/publications/embedded-neural-networks-in-resource-constrained-hearing-instrume", "id": "Q3vsiQwMDv0J", "cited_by_count": 0}, {"title": "Training Binary Neural Networks in a Binary Weight Space", "title_link": "https://openreview.net/forum?id=Dm4qrBuFKH", "id": "L4dHzMXAnSgJ", "cited_by_count": 0}, {"title": "Lightweight pixel difference networks for efficient visual representation learning", "title_link": "https://ieeexplore.ieee.org/abstract/document/10198340/", "id": "kmCaZHwDbcYJ", "cited_by_count": 14}, {"title": "zkLLM: Zero Knowledge Proofs for Large Language Models", "title_link": "https://arxiv.org/abs/2404.16109", "id": "rbDw3N-H7BoJ", "cited_by_count": 6}, {"title": "Self-knowledge distillation enhanced binary neural networks derived from underutilized information", "title_link": "https://link.springer.com/article/10.1007/s10489-024-05444-8", "id": "68OWxaE_TQ0J", "cited_by_count": 0}, {"title": "Hardware-Software Co-Design of Deep Neural Networks: From Handcrafted to Automated Design and Deployment", "title_link": "https://mediatum.ub.tum.de/1656745", "id": "vB2yKRuZvXkJ", "cited_by_count": 0}, {"title": "Arithmetic for Deep Learning", "title_link": "https://link.springer.com/chapter/10.1007/978-3-031-42808-1_24", "id": "0RNYdn6Qt2QJ", "cited_by_count": 0}, {"title": "Neural inference at the frontier of energy, space, and time", "title_link": "https://www.science.org/doi/abs/10.1126/science.adh1174", "id": "pW1OlVeWCcEJ", "cited_by_count": 47}, {"title": "Superbnn: Randomized binary neural network using adiabatic superconductor josephson devices", "title_link": "https://dl.acm.org/doi/abs/10.1145/3613424.3623771", "id": "Zj1WrXQbDTQJ", "cited_by_count": 1}, {"title": "Green edge AI: A contemporary survey", "title_link": "https://ieeexplore.ieee.org/abstract/document/10637271/", "id": "hY3VgyrYOokJ", "cited_by_count": 7}, {"title": "Slice-Level Scheduling for High Throughput and Load Balanced LLM Serving", "title_link": "https://arxiv.org/abs/2406.13511", "id": "iZLBB3bCSNwJ", "cited_by_count": 1}, {"title": "Photonic-Electronic Integrated Circuits for High-Performance Computing and AI Accelerators", "title_link": "https://ieeexplore.ieee.org/abstract/document/10598302/", "id": "2nuTkKQebyEJ", "cited_by_count": 2}, {"title": "Deployment of Artificial Intelligence Models on Edge Devices: A Tutorial Brief", "title_link": "https://ieeexplore.ieee.org/abstract/document/10328759/", "id": "sLjXiTKlQs8J", "cited_by_count": 9}, {"title": "Learning verifiable representations", "title_link": "https://research-explorer.ista.ac.at/record/11362", "id": "Y08vhSmhnA0J", "cited_by_count": 0}, {"title": "SiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts Models", "title_link": "https://proceedings.mlsys.org/paper_files/paper/2024/hash/698cfaf72a208aef2e78bcac55b74328-Abstract-Conference.html", "id": "gAKEIunMGvkJ", "cited_by_count": 3}, {"title": "Optimizing DNN Inference on Multi-Accelerator SoCs at Training-time", "title_link": "https://arxiv.org/abs/2409.18566", "id": "h8TBDobPGlYJ", "cited_by_count": 0}, {"title": "LLMServingSim: A HW/SW Co-Simulation Infrastructure for LLM Inference Serving at Scale", "title_link": "https://arxiv.org/abs/2408.05499", "id": "gZlkP1tk6IEJ", "cited_by_count": 1}, {"title": "Systems and Algorithms for Efficient, Secure and Private Machine Learning Inference", "title_link": "https://search.proquest.com/openview/59d7316ee4a5765ce16c455e34a813f6/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "kCpt1j3cqdgJ", "cited_by_count": 0}, {"title": "Low-Cost Language Models: Survey and Performance Evaluation on Python Code Generation", "title_link": "https://arxiv.org/abs/2404.11160", "id": "s2J_xGEXdJEJ", "cited_by_count": 1}, {"title": "NAS-BNN: Neural Architecture Search for Binary Neural Networks", "title_link": "https://arxiv.org/abs/2408.15484", "id": "2Xwf5g9_OAEJ", "cited_by_count": 0}, {"title": "FPGA acceleration of automated ship detection and CNN-based sip/iceberg discriminator in SAR imagery", "title_link": "https://summit.sfu.ca/item/36202", "id": "L3m1Sk7DjJgJ", "cited_by_count": 0}, {"title": "DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video Analytics", "title_link": "https://arxiv.org/abs/2403.14353", "id": "CYREJWhH5VkJ", "cited_by_count": 2}, {"title": "[PDF][PDF] Mapping Efficient Convolutional Neural Networks to Resource-Limited System-on-Chip Field-Programmable Gate Arrays for Mobile Deep Learning Application\u00a0\u2026", "title_link": "https://ikee.lib.auth.gr/record/354668/files/GRI-2024-43162.pdf", "id": "1KD003nslWwJ", "cited_by_count": 0}, {"title": "Approximations in deep learning", "title_link": "https://link.springer.com/chapter/10.1007/978-3-030-94705-7_15", "id": "bdmgHEhInRUJ", "cited_by_count": 6}, {"title": "Efficient and effective text encoding for chinese llama and alpaca", "title_link": "https://arxiv.org/abs/2304.08177", "id": "MvhRbsqnhGgJ", "cited_by_count": 214}, {"title": "GOENet: Group Operations Enhanced Binary Neural Network for Efficient Image Classification", "title_link": "https://ieeexplore.ieee.org/abstract/document/10487018/", "id": "iP9KZJIVFiUJ", "cited_by_count": 0}, {"title": "Scgnet: Shifting and cascaded group network", "title_link": "https://ieeexplore.ieee.org/abstract/document/10049473/", "id": "PRq7GpdLndYJ", "cited_by_count": 8}, {"title": "Boolean Logic for Low-Energy Deep Learning", "title_link": "https://openreview.net/forum?id=YyVJctb2v4", "id": "BKJ4EUM6P34J", "cited_by_count": 0}, {"title": "[BUCH][B] Applying Deep Compression to Enable Spoken Language Edge Inference", "title_link": "https://search.proquest.com/openview/61f095f4db462b9a1cde6ff7b1d266de/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "cH7ucZ4F2ukJ", "cited_by_count": 0}, {"title": "[HTML][HTML] Deep neural networks compression: A comparative survey and choice recommendations", "title_link": "https://www.sciencedirect.com/science/article/pii/S0925231222014643", "id": "-7ukhpAauHgJ", "cited_by_count": 59}, {"title": "Deployment of Artificial Intelligence Models into Edge Devices: A Tutorial Brief", "title_link": "https://www.techrxiv.org/doi/full/10.36227/techrxiv.24072675.v1", "id": "7PolkyHnOxAJ", "cited_by_count": 0}, {"title": "A tutorial on open-source large language models for behavioral science", "title_link": "https://link.springer.com/article/10.3758/s13428-024-02455-8", "id": "G0yByGfqwU8J", "cited_by_count": 17}, {"title": "Training with Mixed-Precision Floating-Point Assignments", "title_link": "https://arxiv.org/abs/2301.13464", "id": "uPAAE9UaYvcJ", "cited_by_count": 2}, {"title": "Point cloud based semantic segmentation for catenary systems using deep learning: Compressibility of a PointNet++ network", "title_link": "http://essay.utwente.nl/92901/", "id": "nQx0Vr6qTlQJ", "cited_by_count": 1}, {"title": "Towards Efficient Edge Intelligence with In-Sensor and Neuromorphic Computing: Algorithm-Hardware Co-Design", "title_link": "https://search.proquest.com/openview/e79564c23f94637851dc65eb792decec/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "PMKzo3ZsnhsJ", "cited_by_count": 0}, {"title": "CaM: Cache Merging for Memory-efficient LLMs Inference", "title_link": "https://openreview.net/forum?id=LCTmppB165", "id": "LAedpErOmMUJ", "cited_by_count": 0}, {"title": "Amphista: Accelerate LLM Inference with Bi-directional Multiple Drafting Heads in a Non-autoregressive Style", "title_link": "https://arxiv.org/abs/2406.13170", "id": "xWcavRF_HX0J", "cited_by_count": 3}, {"title": "Structured binary neural networks for image recognition", "title_link": "https://link.springer.com/article/10.1007/s11263-022-01638-0", "id": "ARmuOep3jWsJ", "cited_by_count": 23}, {"title": "[PDF][PDF] DACAPO: Accelerating Continuous Learning in Autonomous Systems for Video Analytics", "title_link": "https://jongse-park.github.io/files/paper/2024-isca-dacapo.pdf", "id": "AxLgURgU-fwJ", "cited_by_count": 0}, {"title": "End-to-End Neural Network Compression via  Regularized Latency Surrogates", "title_link": "https://arxiv.org/abs/2306.05785", "id": "xjYvuSMlufIJ", "cited_by_count": 0}, {"title": "Compiler-centric across-stack deep learning acceleration", "title_link": "https://theses.gla.ac.uk/83959/", "id": "7DWvWuzNqdYJ", "cited_by_count": 1}, {"title": "Training with Mixed-Precision Floating-Point Assignments", "title_link": "https://arxiv.org/abs/2301.13464", "id": "uPAAE9UaYvcJ", "cited_by_count": 2}, {"title": "Point cloud based semantic segmentation for catenary systems using deep learning: Compressibility of a PointNet++ network", "title_link": "http://essay.utwente.nl/92901/", "id": "nQx0Vr6qTlQJ", "cited_by_count": 1}, {"title": "Towards Efficient Edge Intelligence with In-Sensor and Neuromorphic Computing: Algorithm-Hardware Co-Design", "title_link": "https://search.proquest.com/openview/e79564c23f94637851dc65eb792decec/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "PMKzo3ZsnhsJ", "cited_by_count": 0}, {"title": "CaM: Cache Merging for Memory-efficient LLMs Inference", "title_link": "https://openreview.net/forum?id=LCTmppB165", "id": "LAedpErOmMUJ", "cited_by_count": 0}, {"title": "Amphista: Accelerate LLM Inference with Bi-directional Multiple Drafting Heads in a Non-autoregressive Style", "title_link": "https://arxiv.org/abs/2406.13170", "id": "xWcavRF_HX0J", "cited_by_count": 3}, {"title": "Structured binary neural networks for image recognition", "title_link": "https://link.springer.com/article/10.1007/s11263-022-01638-0", "id": "ARmuOep3jWsJ", "cited_by_count": 23}, {"title": "[PDF][PDF] DACAPO: Accelerating Continuous Learning in Autonomous Systems for Video Analytics", "title_link": "https://jongse-park.github.io/files/paper/2024-isca-dacapo.pdf", "id": "AxLgURgU-fwJ", "cited_by_count": 0}, {"title": "End-to-End Neural Network Compression via  Regularized Latency Surrogates", "title_link": "https://arxiv.org/abs/2306.05785", "id": "xjYvuSMlufIJ", "cited_by_count": 0}, {"title": "Compiler-centric across-stack deep learning acceleration", "title_link": "https://theses.gla.ac.uk/83959/", "id": "7DWvWuzNqdYJ", "cited_by_count": 1}, {"title": "[PDF][PDF] NEURAL APPROACHES TO THEOREM SEARCH & PROOF REPAIR", "title_link": "https://tompreichel.com/thesis.pdf", "id": "LSeVrl9G33oJ", "cited_by_count": 0}, {"title": "CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information", "title_link": "https://arxiv.org/abs/2409.13199", "id": "uaHy3r0GVwEJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Implementation of Machine Learning Algorithms on Ultra-Low-Power Hardware for In-Sensor Inference", "title_link": "https://tesidottorato.depositolegale.it/bitstream/20.500.14242/156910/1/conv_tesi-final.pdf", "id": "dMmPaT08ClQJ", "cited_by_count": 0}, {"title": "MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router", "title_link": "https://arxiv.org/abs/2410.12013", "id": "Jvsm4FODeBMJ", "cited_by_count": 0}, {"title": "Knowledge-preserving Pruning for Pre-trained Language Models without Retraining", "title_link": "https://arxiv.org/abs/2308.03449", "id": "AK8FIQjyHxoJ", "cited_by_count": 2}, {"title": "Binary Dense Predictors for Human Pose Estimation Based on Dynamic Thresholds and Filtering", "title_link": "https://ieeexplore.ieee.org/abstract/document/9746998/", "id": "DiyHgJSSpnQJ", "cited_by_count": 2}, {"title": "[PDF][PDF] Hardware-Software Co-Design for Energy-Efficient Neural Network Inference at the Extreme Edge", "title_link": "https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/698281/1/Doctoral_Thesis-5.pdf", "id": "qMCjXd6An6AJ", "cited_by_count": 0}, {"title": "Smoothing Disruption Across the Stack: Tales of Memory, Heterogeneity, & Compilers", "title_link": "https://ieeexplore.ieee.org/abstract/document/10546772/", "id": "-FTlsI6yDD8J", "cited_by_count": 0}, {"title": "[BUCH][B] Model Compression for Efficient Machine Learning Inference", "title_link": "https://search.proquest.com/openview/fb05ec5a8feb8ad931b6f422904ca14a/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "5s2fOyC_X94J", "cited_by_count": 0}, {"title": "Near-Memory Processing for Low-precision Deep Neural Networks", "title_link": "https://www.repository.cam.ac.uk/items/cf64e959-1ec5-453e-900a-e16f1f5eefc1", "id": "5pMN3EuODN8J", "cited_by_count": 0}, {"title": "Update compression for deep neural networks on the edge", "title_link": "https://openaccess.thecvf.com/content/CVPR2022W/MobileAI/html/Chen_Update_Compression_for_Deep_Neural_Networks_on_the_Edge_CVPRW_2022_paper.html", "id": "hEaGT1012BYJ", "cited_by_count": 16}, {"title": "[PDF][PDF] Evaluation of the DL accelerator designs", "title_link": "https://vedliot.eu/wp-content/uploads/2024/05/VEDLIoT_Deliverable_D3.3_v1.1_submitted.pdf", "id": "kqqREk2VTqYJ", "cited_by_count": 0}, {"title": "HadSkip: Homotopic and Adaptive Layer Skipping of Pre-trained Language Models for Efficient Inference", "title_link": "https://aclanthology.org/2023.findings-emnlp.283/", "id": "gY0EyTHvxdwJ", "cited_by_count": 2}, {"title": "Weightless neural networks for fast, low-energy inference", "title_link": "https://repositories.lib.utexas.edu/items/724dfac2-7346-4f1a-867b-511848686d80", "id": "3ruS8Gy8i8oJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Improving image sensor performance by developing on-chip machine learning algorithms", "title_link": "https://www.duo.uio.no/bitstream/handle/10852/106555/1/Thesis.pdf", "id": "s7f0aD8K7j8J", "cited_by_count": 0}, {"title": "Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models", "title_link": "https://openreview.net/forum?id=s2NjWfaYdZ", "id": "lueEDZs9qDYJ", "cited_by_count": 2}, {"title": "Zero redundancy distributed learning with differential privacy", "title_link": "https://arxiv.org/abs/2311.11822", "id": "tq6QVGexp3IJ", "cited_by_count": 3}, {"title": "Inca: Input-stationary dataflow at outside-the-box thinking about deep learning accelerators", "title_link": "https://ieeexplore.ieee.org/abstract/document/10070992/", "id": "Mq5Os6FmTHAJ", "cited_by_count": 8}, {"title": "Robust Implementation of Retrieval-Augmented Generation on Edge-based Computing-in-Memory Architectures", "title_link": "https://arxiv.org/abs/2405.04700", "id": "bpI_3TWycbwJ", "cited_by_count": 2}, {"title": "A transistor operations model for deep learning energy consumption scaling law", "title_link": "https://ieeexplore.ieee.org/abstract/document/9984954/", "id": "3EGSLPkGGLcJ", "cited_by_count": 1}, {"title": "Boosted dynamic neural networks", "title_link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "id": "mOUt69Abtu0J", "cited_by_count": 13}, {"title": "The Evolution of Mixture of Experts: A Survey from Basics to Breakthroughs", "title_link": "https://www.preprints.org/manuscript/202408.0583", "id": "JHBZ0S25LvsJ", "cited_by_count": 0}, {"title": "Investigations into Ultra-Low-Power Underwater Imaging", "title_link": "https://dspace.mit.edu/handle/1721.1/152645", "id": "AYwujT4-pNgJ", "cited_by_count": 0}, {"title": "On-Chip DNN Training for Direct Feedback Alignment in FeFET", "title_link": "https://link.springer.com/chapter/10.1007/978-3-031-19568-6_11", "id": "zQX1LuHGM5gJ", "cited_by_count": 0}, {"title": "Spiking Diffusion Models", "title_link": "https://ieeexplore.ieee.org/abstract/document/10665907/", "id": "JAeWI-5VIP8J", "cited_by_count": 0}, {"title": "Improving the Efficiency and Robustness of In-Memory Computing in Emerging Technologies", "title_link": "https://search.proquest.com/openview/3aef46e2dd34370a9ece49bca2ea21d8/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "8odoBkUqkbEJ", "cited_by_count": 0}, {"title": "[PDF][PDF] The Evolution of MoE: A Survey from Basics to Breakthroughs", "title_link": "https://www.researchgate.net/profile/Arpita-Vats/publication/382916607_THE_EVOLUTION_OF_MIXTURE_OF_EXPERTS_A_SURVEY_FROM_BASICS_TO_BREAKTHROUGHS/links/66e7627cdde50b3258771e3a/THE-EVOLUTION-OF-MIXTURE-OF-EXPERTS-A-SURVEY-FROM-BASICS-TO-BREAKTHROUGHS.pdf", "id": "rN4lcZ5oWSUJ", "cited_by_count": 0}, {"title": "Sensitivity-Aware Finetuning for Accuracy Recovery on Deep Learning Hardware", "title_link": "https://arxiv.org/abs/2306.03076", "id": "YiLyF6uAeasJ", "cited_by_count": 0}, {"title": "From decoding to meta-generation: Inference-time algorithms for large language models", "title_link": "https://arxiv.org/abs/2406.16838", "id": "seevFgZNHtUJ", "cited_by_count": 2}, {"title": "Maestro: Uncovering Low-Rank Structures via Trainable Decomposition", "title_link": "https://arxiv.org/abs/2308.14929", "id": "OQ2JJPWMN3QJ", "cited_by_count": 2}, {"title": "High-efficiency Compressor Trees for Latest AMD FPGAs", "title_link": "https://dl.acm.org/doi/abs/10.1145/3645097", "id": "zOtKKcFor1IJ", "cited_by_count": 0}, {"title": ": Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding", "title_link": "https://arxiv.org/abs/2402.15991", "id": "O3rXMDBh-x0J", "cited_by_count": 0}, {"title": "Power-Efficient Machine Learning-Based Hardware Architectures for Biomedical Applications", "title_link": "https://search.proquest.com/openview/8f3221666e8cc75be537c6243324852e/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "LQ0qKJbj-MUJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Natural Language Conditioned Planning of Complex Robotics Tasks", "title_link": "https://library.oapen.org/bitstream/handle/20.500.12657/87610/9781040027042.pdf?sequence=1#page=158", "id": "SjbLCWDt6MoJ", "cited_by_count": 0}, {"title": "Towards Efficient Coarse-grained Dialogue Response Selection", "title_link": "https://dl.acm.org/doi/abs/10.1145/3597609", "id": "0MwYSP8omqoJ", "cited_by_count": 0}, {"title": "Training binary neural networks without floating point precision", "title_link": "https://arxiv.org/abs/2310.19815", "id": "jn82PKKjemMJ", "cited_by_count": 0}, {"title": "[PDF][PDF] Large Language Models for Creation, Enrichment and Evaluation of Taxonomic Graphs", "title_link": "https://semantic-web-journal.net/system/files/swj3751.pdf", "id": "2iEJXUnG66kJ", "cited_by_count": 0}, {"title": "[BUCH][B] Holistic Algorithm and System Co-Optimization for Trustworthy and Platform-Aware Deep Learning", "title_link": "https://search.proquest.com/openview/e15ae40c8ca147d0864841909e8650d9/1?pq-origsite=gscholar&cbl=18750&diss=y", "id": "VoVP8UlNC6UJ", "cited_by_count": 0}, {"title": "Practical processing and acceleration of graph neural networks", "title_link": "https://www.repository.cam.ac.uk/items/27470a7b-911d-46d4-8920-a6e68ff00016", "id": "P3tfHzHGtMYJ", "cited_by_count": 0}, {"title": "Low-Latency BERT Inference for heterogeneous multi-processor edge devices", "title_link": "https://escholarship.mcgill.ca/concern/theses/gf06g783t", "id": "1eoPkZ0rAMoJ", "cited_by_count": 0}, {"title": "Environment-aware knowledge distillation for improved resource-constrained edge speech recognition.", "title_link": "https://espace.inrs.ca/id/eprint/15682/", "id": "3Vds-2mjXl0J", "cited_by_count": 0}, {"title": "Agile and Efficient Inference of Quantized Neural Networks", "title_link": "https://www.research-collection.ethz.ch/handle/20.500.11850/675547", "id": "rPWn_VOIrp8J", "cited_by_count": 0}, {"title": "[PDF][PDF] \u0391\u03c1\u03b9\u03c3\u03c4\u03bf\u03c4\u03ad\u03bb\u03b5\u03b9\u03bf \u03a0\u03b1\u03bd\u03b5\u03c0\u03b9\u03c3\u03c4\u03ae \u03b9\u03bf \u0398\u03b5\u03c3\u03c3\u03b1\u03bb\u03bf\u03bd\u03af\u03ba\u03b7\u03c2", "title_link": "https://ikee.lib.auth.gr/record/354668/files/manuscript_18042024.pdf", "id": "I2G4NMsdqvQJ", "cited_by_count": 0}, {"title": "[PDF][PDF] \u9ad8\u6548\u80fd\u4e8c\u503c\u5316\u6b0a\u91cd\u795e\u7d93\u7db2\u8def\u4e4b\u8a2d\u8a08\u8207\u5be6\u73fe", "title_link": "https://tdr.lib.ntu.edu.tw/retrieve/b57a7ec3-aa70-4e87-aee4-13102ba1c95c/ntu-112-2.pdf", "id": "OJWsCPXyK9UJ", "cited_by_count": 0}, {"title": "Optimalizace LLM agent\u016f pro anal\u00fdzu tabulkov\u00fdch dat: Integrace LoRA pro zv\u00fd\u0161en\u00ed kvality", "title_link": "https://dspace.cvut.cz/handle/10467/115388", "id": "vfiBh4jwSoAJ", "cited_by_count": 0}, {"title": "Directly training spiking neural networks for cyber-physical systems: from supervised to reinforcement learning", "title_link": "http://amsdottorato.unibo.it/11309/", "id": "JN5yzCT3JuwJ", "cited_by_count": 0}, {"title": "Enabling Semantic Reasoning in Robots through Natural Language Processing", "title_link": "https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/3157083", "id": "Lp-ZiqrMecUJ", "cited_by_count": 0}]]