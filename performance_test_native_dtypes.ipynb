{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer\n",
    "# dont assign to variable, because it will be larger than memory\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10ab7350a0344f08bdd4432472754c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29842d91912b4f5281d8a8882e170269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# alternatively load tokenizer and model into one variable\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.save_pretrained(\"llama-2-7b-full-precision-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Available cuda device: NVIDIA L40S _CudaDeviceProperties(name='NVIDIA L40S', major=8, minor=9, total_memory=45494MB, multi_processor_count=142)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cpu\"\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"Available cuda device:\", torch.cuda.get_device_name(), torch.cuda.get_device_properties(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the datatypes of model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['torch.float32'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_dtypes(model):\n",
    "    \"\"\"\n",
    "    Return a dictionary of unique dtypes and with the values of the corresponding layers that have this dtype.\n",
    "    \"\"\"\n",
    "    uniq_dtypes = {}\n",
    "    for layer_name in model.state_dict():\n",
    "        curr_dtype = str(model.state_dict()[layer_name].dtype)\n",
    "\n",
    "        if curr_dtype not in uniq_dtypes.keys():\n",
    "            uniq_dtypes[curr_dtype] = [layer_name]\n",
    "        else:\n",
    "            uniq_dtypes[curr_dtype].append(layer_name)\n",
    "\n",
    "    return uniq_dtypes\n",
    "\n",
    "dtype_dict = check_dtypes(model)    \n",
    "dtype_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_footprint(model):\n",
    "    \"\"\"\n",
    "    Return Memory footpring in GB including buffers that do not use gradients\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_footprint = model.get_memory_footprint() / 1e+9\n",
    "    model.eval()\n",
    "    eval_footprint = model.get_memory_footprint() / 1e+9\n",
    "    return {\"train\": train_footprint, \"eval\": eval_footprint}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 26.953670912, 'eval': 26.953670912}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_footprint(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Model Weights to ternary {-1, 0, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitmat import convert_hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m convert_hf_model(model)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabsmean-ternary-bitmat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model = convert_hf_model(model)\n",
    "model.save_pretrained('llama-2-7b-absmean-ternary-bitmat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitmat import Auto158ModelForCausalLM\n",
    "model = Auto158ModelForCausalLM.from_pretrained(\"quantized/llama-2-7b-absmean-ternary-bitmat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# dont assign to variable, because it will be larger than memory\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Inferring the task automatically requires to check the hub with a model_id defined as a `str`. Llama158ForCausalLM(\n  (model): Llama158Model(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x Llama158DecoderLayer(\n        (self_attn): Llama158SdpaAttention(\n          (q_proj): BitLinear(\n            (norm): RMSLayerNorm()\n          )\n          (k_proj): BitLinear(\n            (norm): RMSLayerNorm()\n          )\n          (v_proj): BitLinear(\n            (norm): RMSLayerNorm()\n          )\n          (o_proj): BitLinear(\n            (norm): RMSLayerNorm()\n          )\n          (rotary_emb): Llama158RotaryEmbedding()\n        )\n        (mlp): Llama158MLP(\n          (gate_proj): BitLinear(\n            (norm): RMSLayerNorm()\n          )\n          (up_proj): BitLinear(\n            (norm): RMSLayerNorm()\n          )\n          (down_proj): BitLinear(\n            (norm): RMSLayerNorm()\n          )\n          (act_fn): SiLU()\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n) is not a valid model_id.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 2\u001b[0m quant_pipe \u001b[38;5;241m=\u001b[39m pipeline(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, model\u001b[38;5;241m=\u001b[39mmodel)\n",
      "File \u001b[0;32m/home/dominic/miniconda3/envs/quant-mistral/lib/python3.12/site-packages/transformers/pipelines/__init__.py:824\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 824\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    825\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInferring the task automatically requires to check the hub with a model_id defined as a `str`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    826\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid model_id.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    827\u001b[0m         )\n\u001b[1;32m    828\u001b[0m     task \u001b[38;5;241m=\u001b[39m get_task(model, token)\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# Retrieve the task\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Inferring the task automatically requires to check the hub with a model_id defined as a `str`. Llama158ForCausalLM(\n  (model): Llama158Model(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x Llama158DecoderLayer(\n        (self_attn): Llama158SdpaAttention(\n          (q_proj): BitLinear(\n            (norm): RMSLayerNorm()\n          )\n          (k_proj): BitLinear(\n            (norm): RMSLayerNorm()\n          )\n          (v_proj): BitLinear(\n            (norm): RMSLayerNorm()\n          )\n          (o_proj): BitLinear(\n            (norm): RMSLayerNorm()\n          )\n          (rotary_emb): Llama158RotaryEmbedding()\n        )\n        (mlp): Llama158MLP(\n          (gate_proj): BitLinear(\n            (norm): RMSLayerNorm()\n          )\n          (up_proj): BitLinear(\n            (norm): RMSLayerNorm()\n          )\n          (down_proj): BitLinear(\n            (norm): RMSLayerNorm()\n          )\n          (act_fn): SiLU()\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n) is not a valid model_id."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "quant_pipe = pipeline(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 14.139367424, 'eval': 2.806358464}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model size on CPU\n",
    "memory_footprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpu = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 14.139367424, 'eval': 2.806358464}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model size on GPU\n",
    "memory_footprint(model_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the quantized Model dtypes and quantization weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"torch.float32\": [\n",
      "        \"model.embed_tokens.weight\",\n",
      "        \"model.layers.0.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.0.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.0.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.0.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.0.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.0.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.0.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.1.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.1.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.1.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.1.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.1.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.1.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.1.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.2.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.2.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.2.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.2.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.2.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.2.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.2.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.3.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.3.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.3.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.3.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.3.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.3.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.3.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.4.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.4.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.4.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.4.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.4.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.4.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.4.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.5.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.5.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.5.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.5.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.5.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.5.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.5.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.6.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.6.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.6.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.6.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.6.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.6.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.6.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.7.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.7.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.7.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.7.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.7.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.7.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.7.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.8.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.8.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.8.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.8.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.8.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.8.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.8.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.9.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.9.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.9.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.9.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.9.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.9.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.9.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.10.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.10.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.10.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.10.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.10.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.10.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.10.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.11.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.11.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.11.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.11.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.11.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.11.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.11.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.12.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.12.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.12.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.12.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.12.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.12.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.12.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.13.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.13.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.13.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.13.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.13.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.13.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.13.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.14.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.14.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.14.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.14.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.14.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.14.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.14.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.15.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.15.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.15.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.15.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.15.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.15.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.15.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.16.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.16.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.16.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.16.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.16.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.16.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.16.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.17.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.17.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.17.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.17.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.17.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.17.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.17.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.18.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.18.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.18.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.18.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.18.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.18.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.18.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.19.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.19.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.19.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.19.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.19.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.19.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.19.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.20.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.20.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.20.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.20.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.20.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.20.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.20.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.21.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.21.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.21.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.21.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.21.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.21.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.21.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.22.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.22.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.22.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.22.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.22.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.22.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.22.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.23.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.23.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.23.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.23.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.23.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.23.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.23.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.24.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.24.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.24.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.24.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.24.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.24.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.24.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.25.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.25.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.25.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.25.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.25.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.25.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.25.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.26.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.26.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.26.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.26.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.26.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.26.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.26.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.27.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.27.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.27.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.27.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.27.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.27.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.27.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.28.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.28.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.28.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.28.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.28.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.28.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.28.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.29.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.29.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.29.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.29.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.29.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.29.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.29.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.30.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.30.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.30.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.30.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.30.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.30.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.30.mlp.down_proj.norm.weight\",\n",
      "        \"model.layers.31.self_attn.q_proj.norm.weight\",\n",
      "        \"model.layers.31.self_attn.k_proj.norm.weight\",\n",
      "        \"model.layers.31.self_attn.v_proj.norm.weight\",\n",
      "        \"model.layers.31.self_attn.o_proj.norm.weight\",\n",
      "        \"model.layers.31.mlp.gate_proj.norm.weight\",\n",
      "        \"model.layers.31.mlp.up_proj.norm.weight\",\n",
      "        \"model.layers.31.mlp.down_proj.norm.weight\",\n",
      "        \"lm_head.weight\"\n",
      "    ],\n",
      "    \"torch.float16\": [\n",
      "        \"model.layers.0.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.0.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.0.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.0.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.0.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.0.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.0.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.1.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.1.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.1.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.1.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.1.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.1.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.1.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.2.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.2.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.2.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.2.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.2.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.2.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.2.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.3.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.3.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.3.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.3.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.3.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.3.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.3.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.4.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.4.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.4.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.4.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.4.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.4.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.4.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.5.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.5.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.5.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.5.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.5.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.5.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.5.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.6.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.6.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.6.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.6.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.6.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.6.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.6.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.7.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.7.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.7.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.7.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.7.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.7.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.7.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.8.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.8.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.8.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.8.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.8.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.8.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.8.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.9.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.9.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.9.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.9.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.9.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.9.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.9.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.10.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.10.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.10.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.10.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.10.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.10.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.10.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.11.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.11.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.11.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.11.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.11.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.11.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.11.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.12.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.12.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.12.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.12.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.12.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.12.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.12.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.13.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.13.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.13.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.13.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.13.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.13.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.13.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.14.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.14.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.14.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.14.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.14.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.14.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.14.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.15.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.15.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.15.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.15.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.15.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.15.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.15.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.16.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.16.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.16.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.16.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.16.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.16.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.16.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.17.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.17.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.17.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.17.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.17.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.17.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.17.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.18.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.18.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.18.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.18.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.18.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.18.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.18.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.19.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.19.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.19.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.19.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.19.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.19.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.19.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.20.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.20.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.20.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.20.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.20.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.20.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.20.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.21.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.21.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.21.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.21.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.21.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.21.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.21.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.22.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.22.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.22.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.22.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.22.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.22.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.22.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.23.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.23.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.23.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.23.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.23.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.23.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.23.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.24.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.24.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.24.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.24.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.24.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.24.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.24.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.25.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.25.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.25.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.25.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.25.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.25.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.25.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.26.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.26.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.26.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.26.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.26.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.26.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.26.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.27.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.27.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.27.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.27.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.27.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.27.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.27.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.28.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.28.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.28.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.28.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.28.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.28.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.28.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.29.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.29.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.29.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.29.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.29.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.29.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.29.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.30.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.30.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.30.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.30.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.30.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.30.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.30.mlp.down_proj.scale_w\",\n",
      "        \"model.layers.31.self_attn.q_proj.scale_w\",\n",
      "        \"model.layers.31.self_attn.k_proj.scale_w\",\n",
      "        \"model.layers.31.self_attn.v_proj.scale_w\",\n",
      "        \"model.layers.31.self_attn.o_proj.scale_w\",\n",
      "        \"model.layers.31.mlp.gate_proj.scale_w\",\n",
      "        \"model.layers.31.mlp.up_proj.scale_w\",\n",
      "        \"model.layers.31.mlp.down_proj.scale_w\"\n",
      "    ],\n",
      "    \"torch.int8\": [\n",
      "        \"model.layers.0.self_attn.q_proj.weight\",\n",
      "        \"model.layers.0.self_attn.k_proj.weight\",\n",
      "        \"model.layers.0.self_attn.v_proj.weight\",\n",
      "        \"model.layers.0.self_attn.o_proj.weight\",\n",
      "        \"model.layers.0.mlp.gate_proj.weight\",\n",
      "        \"model.layers.0.mlp.up_proj.weight\",\n",
      "        \"model.layers.0.mlp.down_proj.weight\",\n",
      "        \"model.layers.1.self_attn.q_proj.weight\",\n",
      "        \"model.layers.1.self_attn.k_proj.weight\",\n",
      "        \"model.layers.1.self_attn.v_proj.weight\",\n",
      "        \"model.layers.1.self_attn.o_proj.weight\",\n",
      "        \"model.layers.1.mlp.gate_proj.weight\",\n",
      "        \"model.layers.1.mlp.up_proj.weight\",\n",
      "        \"model.layers.1.mlp.down_proj.weight\",\n",
      "        \"model.layers.2.self_attn.q_proj.weight\",\n",
      "        \"model.layers.2.self_attn.k_proj.weight\",\n",
      "        \"model.layers.2.self_attn.v_proj.weight\",\n",
      "        \"model.layers.2.self_attn.o_proj.weight\",\n",
      "        \"model.layers.2.mlp.gate_proj.weight\",\n",
      "        \"model.layers.2.mlp.up_proj.weight\",\n",
      "        \"model.layers.2.mlp.down_proj.weight\",\n",
      "        \"model.layers.3.self_attn.q_proj.weight\",\n",
      "        \"model.layers.3.self_attn.k_proj.weight\",\n",
      "        \"model.layers.3.self_attn.v_proj.weight\",\n",
      "        \"model.layers.3.self_attn.o_proj.weight\",\n",
      "        \"model.layers.3.mlp.gate_proj.weight\",\n",
      "        \"model.layers.3.mlp.up_proj.weight\",\n",
      "        \"model.layers.3.mlp.down_proj.weight\",\n",
      "        \"model.layers.4.self_attn.q_proj.weight\",\n",
      "        \"model.layers.4.self_attn.k_proj.weight\",\n",
      "        \"model.layers.4.self_attn.v_proj.weight\",\n",
      "        \"model.layers.4.self_attn.o_proj.weight\",\n",
      "        \"model.layers.4.mlp.gate_proj.weight\",\n",
      "        \"model.layers.4.mlp.up_proj.weight\",\n",
      "        \"model.layers.4.mlp.down_proj.weight\",\n",
      "        \"model.layers.5.self_attn.q_proj.weight\",\n",
      "        \"model.layers.5.self_attn.k_proj.weight\",\n",
      "        \"model.layers.5.self_attn.v_proj.weight\",\n",
      "        \"model.layers.5.self_attn.o_proj.weight\",\n",
      "        \"model.layers.5.mlp.gate_proj.weight\",\n",
      "        \"model.layers.5.mlp.up_proj.weight\",\n",
      "        \"model.layers.5.mlp.down_proj.weight\",\n",
      "        \"model.layers.6.self_attn.q_proj.weight\",\n",
      "        \"model.layers.6.self_attn.k_proj.weight\",\n",
      "        \"model.layers.6.self_attn.v_proj.weight\",\n",
      "        \"model.layers.6.self_attn.o_proj.weight\",\n",
      "        \"model.layers.6.mlp.gate_proj.weight\",\n",
      "        \"model.layers.6.mlp.up_proj.weight\",\n",
      "        \"model.layers.6.mlp.down_proj.weight\",\n",
      "        \"model.layers.7.self_attn.q_proj.weight\",\n",
      "        \"model.layers.7.self_attn.k_proj.weight\",\n",
      "        \"model.layers.7.self_attn.v_proj.weight\",\n",
      "        \"model.layers.7.self_attn.o_proj.weight\",\n",
      "        \"model.layers.7.mlp.gate_proj.weight\",\n",
      "        \"model.layers.7.mlp.up_proj.weight\",\n",
      "        \"model.layers.7.mlp.down_proj.weight\",\n",
      "        \"model.layers.8.self_attn.q_proj.weight\",\n",
      "        \"model.layers.8.self_attn.k_proj.weight\",\n",
      "        \"model.layers.8.self_attn.v_proj.weight\",\n",
      "        \"model.layers.8.self_attn.o_proj.weight\",\n",
      "        \"model.layers.8.mlp.gate_proj.weight\",\n",
      "        \"model.layers.8.mlp.up_proj.weight\",\n",
      "        \"model.layers.8.mlp.down_proj.weight\",\n",
      "        \"model.layers.9.self_attn.q_proj.weight\",\n",
      "        \"model.layers.9.self_attn.k_proj.weight\",\n",
      "        \"model.layers.9.self_attn.v_proj.weight\",\n",
      "        \"model.layers.9.self_attn.o_proj.weight\",\n",
      "        \"model.layers.9.mlp.gate_proj.weight\",\n",
      "        \"model.layers.9.mlp.up_proj.weight\",\n",
      "        \"model.layers.9.mlp.down_proj.weight\",\n",
      "        \"model.layers.10.self_attn.q_proj.weight\",\n",
      "        \"model.layers.10.self_attn.k_proj.weight\",\n",
      "        \"model.layers.10.self_attn.v_proj.weight\",\n",
      "        \"model.layers.10.self_attn.o_proj.weight\",\n",
      "        \"model.layers.10.mlp.gate_proj.weight\",\n",
      "        \"model.layers.10.mlp.up_proj.weight\",\n",
      "        \"model.layers.10.mlp.down_proj.weight\",\n",
      "        \"model.layers.11.self_attn.q_proj.weight\",\n",
      "        \"model.layers.11.self_attn.k_proj.weight\",\n",
      "        \"model.layers.11.self_attn.v_proj.weight\",\n",
      "        \"model.layers.11.self_attn.o_proj.weight\",\n",
      "        \"model.layers.11.mlp.gate_proj.weight\",\n",
      "        \"model.layers.11.mlp.up_proj.weight\",\n",
      "        \"model.layers.11.mlp.down_proj.weight\",\n",
      "        \"model.layers.12.self_attn.q_proj.weight\",\n",
      "        \"model.layers.12.self_attn.k_proj.weight\",\n",
      "        \"model.layers.12.self_attn.v_proj.weight\",\n",
      "        \"model.layers.12.self_attn.o_proj.weight\",\n",
      "        \"model.layers.12.mlp.gate_proj.weight\",\n",
      "        \"model.layers.12.mlp.up_proj.weight\",\n",
      "        \"model.layers.12.mlp.down_proj.weight\",\n",
      "        \"model.layers.13.self_attn.q_proj.weight\",\n",
      "        \"model.layers.13.self_attn.k_proj.weight\",\n",
      "        \"model.layers.13.self_attn.v_proj.weight\",\n",
      "        \"model.layers.13.self_attn.o_proj.weight\",\n",
      "        \"model.layers.13.mlp.gate_proj.weight\",\n",
      "        \"model.layers.13.mlp.up_proj.weight\",\n",
      "        \"model.layers.13.mlp.down_proj.weight\",\n",
      "        \"model.layers.14.self_attn.q_proj.weight\",\n",
      "        \"model.layers.14.self_attn.k_proj.weight\",\n",
      "        \"model.layers.14.self_attn.v_proj.weight\",\n",
      "        \"model.layers.14.self_attn.o_proj.weight\",\n",
      "        \"model.layers.14.mlp.gate_proj.weight\",\n",
      "        \"model.layers.14.mlp.up_proj.weight\",\n",
      "        \"model.layers.14.mlp.down_proj.weight\",\n",
      "        \"model.layers.15.self_attn.q_proj.weight\",\n",
      "        \"model.layers.15.self_attn.k_proj.weight\",\n",
      "        \"model.layers.15.self_attn.v_proj.weight\",\n",
      "        \"model.layers.15.self_attn.o_proj.weight\",\n",
      "        \"model.layers.15.mlp.gate_proj.weight\",\n",
      "        \"model.layers.15.mlp.up_proj.weight\",\n",
      "        \"model.layers.15.mlp.down_proj.weight\",\n",
      "        \"model.layers.16.self_attn.q_proj.weight\",\n",
      "        \"model.layers.16.self_attn.k_proj.weight\",\n",
      "        \"model.layers.16.self_attn.v_proj.weight\",\n",
      "        \"model.layers.16.self_attn.o_proj.weight\",\n",
      "        \"model.layers.16.mlp.gate_proj.weight\",\n",
      "        \"model.layers.16.mlp.up_proj.weight\",\n",
      "        \"model.layers.16.mlp.down_proj.weight\",\n",
      "        \"model.layers.17.self_attn.q_proj.weight\",\n",
      "        \"model.layers.17.self_attn.k_proj.weight\",\n",
      "        \"model.layers.17.self_attn.v_proj.weight\",\n",
      "        \"model.layers.17.self_attn.o_proj.weight\",\n",
      "        \"model.layers.17.mlp.gate_proj.weight\",\n",
      "        \"model.layers.17.mlp.up_proj.weight\",\n",
      "        \"model.layers.17.mlp.down_proj.weight\",\n",
      "        \"model.layers.18.self_attn.q_proj.weight\",\n",
      "        \"model.layers.18.self_attn.k_proj.weight\",\n",
      "        \"model.layers.18.self_attn.v_proj.weight\",\n",
      "        \"model.layers.18.self_attn.o_proj.weight\",\n",
      "        \"model.layers.18.mlp.gate_proj.weight\",\n",
      "        \"model.layers.18.mlp.up_proj.weight\",\n",
      "        \"model.layers.18.mlp.down_proj.weight\",\n",
      "        \"model.layers.19.self_attn.q_proj.weight\",\n",
      "        \"model.layers.19.self_attn.k_proj.weight\",\n",
      "        \"model.layers.19.self_attn.v_proj.weight\",\n",
      "        \"model.layers.19.self_attn.o_proj.weight\",\n",
      "        \"model.layers.19.mlp.gate_proj.weight\",\n",
      "        \"model.layers.19.mlp.up_proj.weight\",\n",
      "        \"model.layers.19.mlp.down_proj.weight\",\n",
      "        \"model.layers.20.self_attn.q_proj.weight\",\n",
      "        \"model.layers.20.self_attn.k_proj.weight\",\n",
      "        \"model.layers.20.self_attn.v_proj.weight\",\n",
      "        \"model.layers.20.self_attn.o_proj.weight\",\n",
      "        \"model.layers.20.mlp.gate_proj.weight\",\n",
      "        \"model.layers.20.mlp.up_proj.weight\",\n",
      "        \"model.layers.20.mlp.down_proj.weight\",\n",
      "        \"model.layers.21.self_attn.q_proj.weight\",\n",
      "        \"model.layers.21.self_attn.k_proj.weight\",\n",
      "        \"model.layers.21.self_attn.v_proj.weight\",\n",
      "        \"model.layers.21.self_attn.o_proj.weight\",\n",
      "        \"model.layers.21.mlp.gate_proj.weight\",\n",
      "        \"model.layers.21.mlp.up_proj.weight\",\n",
      "        \"model.layers.21.mlp.down_proj.weight\",\n",
      "        \"model.layers.22.self_attn.q_proj.weight\",\n",
      "        \"model.layers.22.self_attn.k_proj.weight\",\n",
      "        \"model.layers.22.self_attn.v_proj.weight\",\n",
      "        \"model.layers.22.self_attn.o_proj.weight\",\n",
      "        \"model.layers.22.mlp.gate_proj.weight\",\n",
      "        \"model.layers.22.mlp.up_proj.weight\",\n",
      "        \"model.layers.22.mlp.down_proj.weight\",\n",
      "        \"model.layers.23.self_attn.q_proj.weight\",\n",
      "        \"model.layers.23.self_attn.k_proj.weight\",\n",
      "        \"model.layers.23.self_attn.v_proj.weight\",\n",
      "        \"model.layers.23.self_attn.o_proj.weight\",\n",
      "        \"model.layers.23.mlp.gate_proj.weight\",\n",
      "        \"model.layers.23.mlp.up_proj.weight\",\n",
      "        \"model.layers.23.mlp.down_proj.weight\",\n",
      "        \"model.layers.24.self_attn.q_proj.weight\",\n",
      "        \"model.layers.24.self_attn.k_proj.weight\",\n",
      "        \"model.layers.24.self_attn.v_proj.weight\",\n",
      "        \"model.layers.24.self_attn.o_proj.weight\",\n",
      "        \"model.layers.24.mlp.gate_proj.weight\",\n",
      "        \"model.layers.24.mlp.up_proj.weight\",\n",
      "        \"model.layers.24.mlp.down_proj.weight\",\n",
      "        \"model.layers.25.self_attn.q_proj.weight\",\n",
      "        \"model.layers.25.self_attn.k_proj.weight\",\n",
      "        \"model.layers.25.self_attn.v_proj.weight\",\n",
      "        \"model.layers.25.self_attn.o_proj.weight\",\n",
      "        \"model.layers.25.mlp.gate_proj.weight\",\n",
      "        \"model.layers.25.mlp.up_proj.weight\",\n",
      "        \"model.layers.25.mlp.down_proj.weight\",\n",
      "        \"model.layers.26.self_attn.q_proj.weight\",\n",
      "        \"model.layers.26.self_attn.k_proj.weight\",\n",
      "        \"model.layers.26.self_attn.v_proj.weight\",\n",
      "        \"model.layers.26.self_attn.o_proj.weight\",\n",
      "        \"model.layers.26.mlp.gate_proj.weight\",\n",
      "        \"model.layers.26.mlp.up_proj.weight\",\n",
      "        \"model.layers.26.mlp.down_proj.weight\",\n",
      "        \"model.layers.27.self_attn.q_proj.weight\",\n",
      "        \"model.layers.27.self_attn.k_proj.weight\",\n",
      "        \"model.layers.27.self_attn.v_proj.weight\",\n",
      "        \"model.layers.27.self_attn.o_proj.weight\",\n",
      "        \"model.layers.27.mlp.gate_proj.weight\",\n",
      "        \"model.layers.27.mlp.up_proj.weight\",\n",
      "        \"model.layers.27.mlp.down_proj.weight\",\n",
      "        \"model.layers.28.self_attn.q_proj.weight\",\n",
      "        \"model.layers.28.self_attn.k_proj.weight\",\n",
      "        \"model.layers.28.self_attn.v_proj.weight\",\n",
      "        \"model.layers.28.self_attn.o_proj.weight\",\n",
      "        \"model.layers.28.mlp.gate_proj.weight\",\n",
      "        \"model.layers.28.mlp.up_proj.weight\",\n",
      "        \"model.layers.28.mlp.down_proj.weight\",\n",
      "        \"model.layers.29.self_attn.q_proj.weight\",\n",
      "        \"model.layers.29.self_attn.k_proj.weight\",\n",
      "        \"model.layers.29.self_attn.v_proj.weight\",\n",
      "        \"model.layers.29.self_attn.o_proj.weight\",\n",
      "        \"model.layers.29.mlp.gate_proj.weight\",\n",
      "        \"model.layers.29.mlp.up_proj.weight\",\n",
      "        \"model.layers.29.mlp.down_proj.weight\",\n",
      "        \"model.layers.30.self_attn.q_proj.weight\",\n",
      "        \"model.layers.30.self_attn.k_proj.weight\",\n",
      "        \"model.layers.30.self_attn.v_proj.weight\",\n",
      "        \"model.layers.30.self_attn.o_proj.weight\",\n",
      "        \"model.layers.30.mlp.gate_proj.weight\",\n",
      "        \"model.layers.30.mlp.up_proj.weight\",\n",
      "        \"model.layers.30.mlp.down_proj.weight\",\n",
      "        \"model.layers.31.self_attn.q_proj.weight\",\n",
      "        \"model.layers.31.self_attn.k_proj.weight\",\n",
      "        \"model.layers.31.self_attn.v_proj.weight\",\n",
      "        \"model.layers.31.self_attn.o_proj.weight\",\n",
      "        \"model.layers.31.mlp.gate_proj.weight\",\n",
      "        \"model.layers.31.mlp.up_proj.weight\",\n",
      "        \"model.layers.31.mlp.down_proj.weight\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "qunatized_model_dtypes = check_dtypes(model_gpu)\n",
    "print(json.dumps(qunatized_model_dtypes, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num quantized layers: '224' with '224' quantization weight scales. There are '1.0' weight scales per quantized layer.\n"
     ]
    }
   ],
   "source": [
    "int8_layers = len(qunatized_model_dtypes['torch.int8'])\n",
    "float16_layers = len(qunatized_model_dtypes['torch.float16'])\n",
    "ratio = float16_layers / int8_layers\n",
    "print(f\"Num quantized layers: \\'{int8_layers}\\' with \\'{float16_layers}\\' quantization weight scales. There are '{ratio}' weight scales per quantized layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 16.111116288, 'eval': 3.897303488}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_footprint(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the inference time of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency(model, tokenizer, ttft=False):\n",
    "    device = \"cuda:0\"\n",
    "    \n",
    "    # input\n",
    "    input_text = \"A test is a\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Warm-up\n",
    "    for _ in range(10):\n",
    "        _ = model.generate(input_ids, max_new_tokens=1)\n",
    "\n",
    "    if device.startswith(\"cuda\"):\n",
    "    # Measure time-to-first-token\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        if ttft:\n",
    "            start_event.record()\n",
    "            _ = model.generate(input_ids, max_new_tokens=1)\n",
    "            end_event.record()\n",
    "        else:\n",
    "            start_event.record()\n",
    "            tokens = model.generate(input_ids)\n",
    "            end_event.record()\n",
    "            print(tokens[0])\n",
    "            print(len(tokens[0]))\n",
    "\n",
    "            # Wait for GPU operations to complete\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        latency = start_event.elapsed_time(end_event)  # Latency in milliseconds\n",
    "\n",
    "    elif device.startswith(\"cpu\"):\n",
    "        import time\n",
    "        if ttft:\n",
    "            start_time = time.perf_counter()\n",
    "            _ = model.generate(input_ids, max_new_tokens=1)\n",
    "            end_time = time.perf_counter()\n",
    "        else:\n",
    "            start_time = time.perf_counter()\n",
    "            tokens = model.generate(input_ids)\n",
    "            end_time = time.perf_counter()\n",
    "            print(tokens[0])\n",
    "            print(len(tokens[0]))\n",
    "\n",
    "        latency = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "\n",
    "    else:\n",
    "        print(\"Error: wrong device\", device)\n",
    "\n",
    "    if ttft:\n",
    "        print(f\"Time-to-first-token latency: {latency:.2f} ms\")\n",
    "    else:\n",
    "        latency = latency / len(tokens[0])\n",
    "        print(f\"Latency per Token: {latency:.2f} ms\")\n",
    "    return latency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/dominic/miniconda3/envs/quant-mistral/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,   319,  1243,   338,   263,   809, 10579,  4051, 23209, 13056,\n",
      "         4051,  5015, 16033, 24626, 13305,  3237,  4956, 23393, 16325, 22322],\n",
      "       device='cuda:0')\n",
      "20\n",
      "Latency per Token: 268.40 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-to-first-token latency: 65.86 ms\n"
     ]
    }
   ],
   "source": [
    "# meassure latency for the quantized model\n",
    "from bitmat import Auto158ModelForCausalLM\n",
    "model = Auto158ModelForCausalLM.from_pretrained(\"quantized/llama-2-7b-absmean-ternary-bitmat\")\n",
    "model = model.to(\"cuda:0\")\n",
    "quant_latency_per_token = measure_latency(model, tokenizer, ttft=False)\n",
    "quant_latency_ttft = measure_latency(model, tokenizer, ttft=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latency_per_token': 268.4, 'latency_ttft': 65.86}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_latencies = {\"latency_per_token\": 268.40, \"latency_ttft\": 65.86}\n",
    "quant_latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad762648a4fa4238b3c6d160bf7675a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-to-first-token latency: 46.63 ms\n",
      "tensor([   1,  319, 1243,  ...,  263, 1243,  310], device='cuda:0')\n",
      "4096\n",
      "Latency per Token: 52.90 ms\n"
     ]
    }
   ],
   "source": [
    "# meassure latency for the baseline model\n",
    "#del model \n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"cuda:0\")\n",
    "\n",
    "base_latency_ttft = measure_latency(model, tokenizer, ttft=True)\n",
    "base_latency_per_token = measure_latency(model, tokenizer, ttft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latency_per_token': 52.9, 'latency_ttft': 46.63}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_latencies = {\"latency_per_token\": 52.90, \"latency_ttft\": 46.63}\n",
    "base_latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
